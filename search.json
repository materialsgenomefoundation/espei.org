[
  {
    "objectID": "tutorials/cu-mg-example/cu-mg-example.html",
    "href": "tutorials/cu-mg-example/cu-mg-example.html",
    "title": "Cu-Mg Example",
    "section": "",
    "text": "The Cu-Mg binary system is an interesting and simple binary subsystem for light metal alloys. It has been modeled in the literature by (Coughanowr et al. 1991), (Zuo and Chang 1993), and (Zhou et al. 2007) and was featured as a case study in Computational Thermodynamics: The Calphad Method (Lukas, Fries, and Sundman 2007).\nHere we will combine results from firstprinciples density functional theory calculations and experimental data for individual phases to generate a initial set of Calphad model parameters and phase diagram. Then that database will be used as a starting point for a Markov Chain Monte Carlo (MCMC) Bayesian optimization of the parameters to fit thermochemical data and phase diagram data simultaneously.",
    "crumbs": [
      "Tutorials",
      "Cu-Mg Example"
    ]
  },
  {
    "objectID": "tutorials/cu-mg-example/cu-mg-example.html#input-data",
    "href": "tutorials/cu-mg-example/cu-mg-example.html#input-data",
    "title": "Cu-Mg Example",
    "section": "Input data",
    "text": "Input data\nAll datasets and input files for this tutorial can be found at the GitHub repository for this website in the Cu-Mg example directory. The input files will be very breifly explained in this tutorial so that you are able to know their use. More detailed description of the files is found on the relevant schema reference documentation pages.",
    "crumbs": [
      "Tutorials",
      "Cu-Mg Example"
    ]
  },
  {
    "objectID": "tutorials/cu-mg-example/cu-mg-example.html#phases-and-calphad-models",
    "href": "tutorials/cu-mg-example/cu-mg-example.html#phases-and-calphad-models",
    "title": "Cu-Mg Example",
    "section": "Phases and Calphad models",
    "text": "Phases and Calphad models\nThe Cu-Mg system contains five stable phases: liquid, disordered fcc and hcp, the C15 Laves phase and the CuMg2 phase. All of these phases will be modeled as solution phases, except for CuMg2, which will be represented as a stoichiometric compound. The phase names and corresponding sublattice models are as follows:\n    LIQUID:    (CU, MG)1\n    CUMG2:     (CU)1 (MG)2\n    FCC_A1:    (CU, MG)1\n    HCP_A3:    (CU, MG)1\n    LAVES_C15: (CU, MG)2 (CU, MG)1\nThese phase names and sublattice models are described in the JSON file phase_models.json file (see the Phase Models Schema for more details):\n\n\nphase_models.json\n\n{\n  \"components\": [\"CU\", \"MG\"],\n  \"phases\": {\n         \"LIQUID\" : {\n            \"sublattice_model\": [[\"CU\", \"MG\"]],\n            \"sublattice_site_ratios\": [1]\n         },\n         \"CUMG2\": {\n            \"sublattice_model\": [[\"CU\"], [\"MG\"]],\n            \"sublattice_site_ratios\": [1, 2]\n         },\n         \"FCC_A1\": {\n            \"sublattice_model\": [[\"CU\", \"MG\"]],\n            \"sublattice_site_ratios\": [1]\n         },\n         \"HCP_A3\": {\n            \"sublattice_site_ratios\": [1],\n            \"sublattice_model\": [[\"CU\", \"MG\"]]\n         },\n         \"LAVES_C15\": {\n            \"sublattice_site_ratios\": [2, 1],\n            \"sublattice_model\": [[\"CU\", \"MG\"], [\"CU\", \"MG\"]]\n         }\n    }\n}",
    "crumbs": [
      "Tutorials",
      "Cu-Mg Example"
    ]
  },
  {
    "objectID": "tutorials/cu-mg-example/cu-mg-example.html#running-espei",
    "href": "tutorials/cu-mg-example/cu-mg-example.html#running-espei",
    "title": "Cu-Mg Example",
    "section": "Running ESPEI",
    "text": "Running ESPEI\nESPEI has two types of fitting -- parameter generation and MCMC optimization. The parameter generation step uses experimental and first-principles data of the derivatives of the Gibbs free energy to parameterize the Gibbs energies of each individual phase. The MCMC optimization step fits the generated parameters to experimental phase equilibria data. These two fitting procedures can be used together to fully assess a given system. For clarity, we will preform these steps separately to fit Cu-Mg. The next two sections are devoted to describing ESPEI’s parameter generation and optimization.\nThe datasets provided here are already well formed, but, you should get in the habit of checking datasets before running ESPEI. ESPEI has a tool to help find and report problems in your datasets. This is automatically run when you load the datasets, but will fail on the first error. Running the following commmand (assuming from here on that you are in the cu-mg-example tutorial folder from GitHub):\nespei --check-datasets input-data\nThe benefit of the this approach is that all of the datasets will be checked and reported at once. If there are any failures, a list of them will be reported with the two main types of errors being JSONError, for which you should read the JSON section of the dataset schema, or DatasetError, which are related to the validity of your datasets scientifically (maching conditions and values shape, etc.). The DatasetError messages are designed to be clear, so please open a discussion on GitHub if there is any confusion.\n\nGenerating Calphad model parameters\nBy using the phase_models.json phase description for the fit settings and passing all of the input data in the input-data folder, we can first use ESPEI to generate a phase diagram based on single-phase experimental and DFT data. Currently all of the input datasets must be formation properties, and it can be seen that the formation enthalpies are defined from DFT and experiments for the Laves and CuMg2 phases. Mixing enthalpies are defined for the for the fcc, hcp, and Laves phases from DFT and for liquid from experimental measurements.\nThe following command will generate a database named Cu-Mg-generated.tdb with parameters selected and fit by ESPEI:\nespei --input run_param_gen.yaml\nwhere run_param_gen.yaml is an ESPEI input file with the following contents\n\n\nrun_param_gen.yaml\n\nsystem:\n  phase_models: phase_models.json    # path to the phases file\n  datasets: input-data               # path to the directory containing input data\n  tags:\n    dft:\n      excluded_model_contributions: ['idmix', 'mag']\n    estimated-entropy:\n      excluded_model_contributions: ['idmix', 'mag']\n      weight: 0.1\noutput:\n  output_db: Cu-Mg-generated.tdb     # what to name the output TDB file\n  verbosity: 2                       # levels of verbosity. Choose 0, 1 or 2 for Warnings, Info, or Debug\ngenerate_parameters:\n  ref_state: SGTE91\n  excess_model: linear\n  aicc_penalty_factor:\n    LIQUID:\n      HM: 1.4  # goal is to generate two parameters\n      SM: 1.4\n\nThe calculation should be relatively quick, on the order of a minute of runtime. With the above command, only mininmal output (warnings) will be reported. You can increase the verbosity to report info messages by setting the output.verbosity key to 1 or debug messages with 2.\nWith the following code, we can look at the generated phase diagram and compare it to our data.\n\n# import everything we need in one spot\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pycalphad import Database, binplot, variables as v\nfrom espei.datasets import load_datasets, recursive_glob\nfrom espei.plot import dataplot\n\n# load the experimental and DFT datasets\ndatasets = load_datasets(recursive_glob(\"input-data\"))\n\n# Plot initial phase diagram with generated parameters\n\n# load database we generated\ndbf = Database(\"Cu-Mg-generated.tdb\")\ncomps = [\"CU\", \"MG\", \"VA\"]\nphases = list(dbf.phases.keys())\nconds = {v.P: 101325, v.T: (500, 1500, 10), v.X(\"MG\"): (0, 1, 0.02)}\n\n# plot the phase diagram and data\nax = binplot(dbf, comps, phases, conds)\ndataplot(comps, phases, conds, datasets, ax=ax)\nfig = ax.figure\nfig.show()\n\n\n\n\n\n\n\n\nWe can see that the phase diagram is already very reasonable compared to the experimental points. The liquidus temperatures and the solubilities of the fcc and Laves phases are the key differences between the equilibrium data and our first-principles phase diagram. The next section will discuss using ESPEI to optimize the parameters selected and calculated based on all the data.\n\n\nRefining model parameters with MCMC\nWith the data in the input data directory, ESPEI generated 22 parameters to fit. For systems with more components, solution phases, and input data, may more parameters could be required to describe the thermodynamics of the specific system well. Because they describe Gibbs free energies, parameters in Calphad models are highly correlated in both single-phase descriptions and for describing equilibria between phases. For large systems, global numerical optimization of many parameters simultaneously is computationally intractable.\nTo combat the problem of optimizing many paramters, ESPEI uses MCMC, a stochastic optimzation method.\nNow we will use our zero phase fraction equilibria data to optimize our first-principles database with MCMC. The following command will take the database we created in the single-phase parameter selection and perform a MCMC optimization, creating a cu-mg_mcmc.tdb:\nespei --input run_mcmc.yaml\nwhere run_mcmc.yaml is an ESPEI input file with the following structure\n\n\nrun_mcmc.yaml\n\nsystem:\n  phase_models: phase_models.json      # path to the phases file\n  datasets: input-data  # path to the directory containing input data\n  tags:\n    dft:\n      excluded_model_contributions: ['idmix', 'mag']\n    estimated-entropy:\n      excluded_model_contributions: ['idmix', 'mag']\noutput:\n  output_db: Cu-Mg-mcmc.tdb               # what to name the output TDB file\n  verbosity: 2                            # levels of verbosity. Choose 0, 1 or 2 for Warnings, Info, or Debug\n  tracefile: trace.npy\n  probfile:  lnprob.npy\n  logfile:   mcmc-log.txt\nmcmc:\n  iterations: 2000\n  save_interval: 1\n  scheduler: null  # don't parallelize, just run on one core\n  input_db: Cu-Mg-generated.tdb\n  chains_per_parameter: 4\n  prior:\n    name: normal\n    loc_relative: 1.0\n    scale_relative: 1.0\n\nESPEI defaults to run 1000 iterations and depends on calculating equilibrium in pycalphad several times for each iteration and the optimization is compute-bound. Fortunately, MCMC optimzations are embarrasingly parallel and ESPEI allows for parallelization using dask.\nFinally, we can use the newly optimized database to plot the phase diagram\n\n# load the database from after the MCMC run\ndbf = Database(\"Cu-Mg-mcmc.tdb\")\ncomps = [\"CU\", \"MG\", \"VA\"]\nphases = list(dbf.phases.keys())\nconds = {v.P: 101325, v.T: (500, 1500, 10), v.X(\"MG\"): (0, 1, 0.02)}\n\n# plot the phase diagram and data\nax = binplot(dbf, comps, phases, conds)\ndataplot(comps, phases, conds, datasets, ax=ax)\nfig = ax.figure\nfig.show()",
    "crumbs": [
      "Tutorials",
      "Cu-Mg Example"
    ]
  },
  {
    "objectID": "tutorials/cu-mg-example/cu-mg-example.html#analyzing-espei-results",
    "href": "tutorials/cu-mg-example/cu-mg-example.html#analyzing-espei-results",
    "title": "Cu-Mg Example",
    "section": "Analyzing ESPEI results",
    "text": "Analyzing ESPEI results\nAfter finishing a MCMC run, you will want to analyze your results.\nAll of the MCMC results will be maintained in two output files, which are serialized NumPy arrays. The file names are set in the run_mcmc.yaml file. The filenames are set by output.tracefile and output.probfile (documentation) and the defaults are trace.npy and lnprob.npy, respectively.\nThe tracefile contains all of the parameters that were proposed over all chains and iterations (the trace). The probfile contains all of calculated log probabilities for all chains and iterations (as negative numbers, by convention).\nThere are several aspects of your data that you may wish to analyze. The next sections will explore some of the options.\n\nfrom espei.analysis import truncate_arrays\n# load our trace and lnprob files to use in later analysis steps\ntrace = np.load(\"trace.npy\")\nlnprob = np.load(\"lnprob.npy\")\ntrace, lnprob = truncate_arrays(trace, lnprob)\n\n\nProbability convergence\nFirst we’ll plot how the probability changes for all of the chains as a function of iterations. This gives a qualitative view of convergence. There are several quantitative metrics that we won’t explore here, such as autocorrelation. Qualitatively, this run does not appear converged after 115 iterations.\n\niterations = np.arange(1, trace.shape[1]+1)\nfig, ax = plt.subplots()\nax.plot(iterations, lnprob.T)\nax.set_title(\"log-probability convergence\")\nax.set_xlabel(\"iterations\")\nax.set_ylabel(\"lnprob\")\nax.set_yscale(\"symlog\")  # log-probabilties are often negative, symlog gives log scale for negative numbers\nfig.show()\n\n\n\n\n\n\n\n\nNext we’ll discard a number of iterations to account for the MCMC optimizer reaching the typical set.\n\nburn_in_iterations = 1000\nfig, ax = plt.subplots()\nax.plot(iterations[burn_in_iterations:], lnprob[:,burn_in_iterations:].T)\nax.set_title(\"log-probability convergence\")\nax.set_xlabel(\"iterations\")\nax.set_ylabel(\"lnprob\")\nfig.show()\n\n\n\n\n\n\n\n\nAfter discarding the first 1000 iterations as burn-in, we can see the chains are mixing well and exploring near the mode of the distribution.\n\n\nVisualizing the trace of each parameter\nWe would like to see how each parameter changed during the iterations. For brevity in the number of plots we’ll plot all the chains for each parameter on the same plot. Here we are looking to see how the parameters explore the space and converge to a solution.\n\nnum_chains = trace.shape[0]\nnum_parameters = 3 # only plot the first three parameters, for all of them use `trace.shape[2]`\nfor parameter in range(num_parameters):\n    fig, ax = plt.subplots()\n    ax.set_xlabel('Iterations')\n    ax.set_ylabel('Parameter value')\n    for chain in range(num_chains):\n        ax.plot(trace[chain, :, parameter])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorner plots\nIn a corner plot, the distributions for each parameter are plotted along the diagonal and covariances between them under the diagonal. A more circular covariance means that parameters are not correlated to each other, while elongated shapes indicate that the two parameters are correlated. Strongly correlated parameters are expected for some parameters in Calphad models within phases or for phases in equilibrium, because increasing one parameter while decreasing another would give a similar error.\n\nimport corner\nfig = plt.figure(figsize=(16,16)) # this is a little condensed for the web view\n# flatten the along the first dimension containing all the chains in parallel\ncorner.corner(trace[:, burn_in_iterations:, :].reshape(-1, trace.shape[-1]), fig=fig)\nfig.show()\n\n\n\n\n\n\n\n\nUltimately, there are many features to explore and we have only covered a few basics. Since all of the results are stored as arrays, you are free to analyze using whatever methods are relevant.",
    "crumbs": [
      "Tutorials",
      "Cu-Mg Example"
    ]
  },
  {
    "objectID": "tutorials/cu-mg-example/cu-mg-example.html#summary",
    "href": "tutorials/cu-mg-example/cu-mg-example.html#summary",
    "title": "Cu-Mg Example",
    "section": "Summary",
    "text": "Summary\nESPEI allows thermodynamic databases to be easily reoptimized with little user interaction, so more data can be added later and the database reoptimized at the cost of only computer time. In fact, the existing database from estimates can be used as a starting point, rather than one directly from first-principles, and the database can simply be modified to match any new data.",
    "crumbs": [
      "Tutorials",
      "Cu-Mg Example"
    ]
  },
  {
    "objectID": "tutorials/cu-mg-example/cu-mg-example.html#acknowledgements",
    "href": "tutorials/cu-mg-example/cu-mg-example.html#acknowledgements",
    "title": "Cu-Mg Example",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nCredit for initially preparing the datasets goes to Aleksei Egorov.",
    "crumbs": [
      "Tutorials",
      "Cu-Mg Example"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ESPEI Documentation",
    "section": "",
    "text": "ESPEI, or Extensible Self-optimizing Phase Equilibria Infrastructure, is a tool for creating Calphad databases and evaluating the uncertainty of Calphad models. The purpose of ESPEI is to be both a user tool for fitting state-of-the-art Calphad-type models and to be a research platform for developing methods for fitting and uncertainty quantification. ESPEI uses pycalphad for the thermodynamic backend and supports fitting adjustable parameters for any pycalphad model.\nESPEI is developed in the open on GitHub. The project is led by Brandon Bocklund, who is currently a postdoctoral researcher at Lawrence Livermore National Laboratory. Brandon developed ESPEI while completing his Ph.D. under Zi-Kui Liu at The Pennsylvania State University. See the project's History for more details.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#parameter-generation",
    "href": "index.html#parameter-generation",
    "title": "ESPEI Documentation",
    "section": "Parameter generation",
    "text": "Parameter generation\nESPEI can be used to generate model parameters for Calphad models of the Gibbs energy that follow the temperature-dependent power series expansion of the Gibbs energy within the compound energy formalism (CEF) for endmembers and for binary and ternary Redlich-Kister interaction parameters with Muggianu extrapolation. This parameter generation step augments the Calphad modeler by providing tools for data-driven model selection, rather than relying on a modeler's intuition alone. Model generation is based on a linear regression of enthalpy, entropy, and heat capacity data (see non-equilibrium thermochemical data), using the corrected Akiake Information Criterion (AICc) to prevent overfitting.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#optimization-and-uncertainty-quantification",
    "href": "index.html#optimization-and-uncertainty-quantification",
    "title": "ESPEI Documentation",
    "section": "Optimization and uncertainty quantification",
    "text": "Optimization and uncertainty quantification\nESPEI can optimize and quantify the uncertainty of Calphad model parameters to thermochemical and phase boundary data. Optimization and uncertainty quantification is performed using a Bayesian ensemble Markov Chain Monte Carlo (MCMC) method. Any Calphad database can be used, including databases generated by ESPEI or starting from an existing Calphad database.\nESPEI supports all models supported by pycalphad. User-developed models that are compatible with pycalphad can be used without making any modifications to ESPEI's code. Performing Bayesian parameter estimation for arbitrary multicomponent thermodynamic data is supported.\n\n\n\n\n\nThe Cu-Mg phase diagram plotted from a database created with and optimized by ESPEI. See the Cu-Mg Example to try it yourself!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "developer/changelog.html",
    "href": "developer/changelog.html",
    "title": "Changelog",
    "section": "",
    "text": "Migrate to PyCalphad Workspace (@bocklund - issue 256)\n\nPyCalphad's IsolatedPhase feature improves accuracy and elimintes the need for a heuristic to perform constrained equilibria when computing the likelihood for phase diagram data.\nESPEI users will likely see a small 2-4x performance regression in MCMC as some of the heavily optimized code paths in ESPEI were removed in favor of the versions implemented in Workspace. We are looking in to further optimizations and utilizing Jansson derivatives to compute likelihood gradients that will enable us to use more performant optimization and MCMC methods.\n\n\n\n\n\n\nUpdate minimum required PyCalphad version to 0.11 (@bocklund - issue 259)\nAdd NumPy 2.0 support (@bocklund - issue 255)\n\n\n\n\n\nRemove code scheduled for deprecation in ESPEI 0.9: deprecations (@bocklund - issue 257)\n\nespei.plot: plot_parameters is replaced with plot_interaction and plot_endmember. eqdataplot and multiplot are replaced with using dataplot combined with the relevant PyCalphad phase diagram plotting code.\nESPEI YAML input: scheduler: \"None\" is replaced by scheduler: null\n\n\n\n\n\n\n\n\n\nAdd a generic framework for property model parameter generation (@bocklund - issue 251)\nImprove performance for generating candidate models with many features (@bocklund - issue 254)\nMCMC: Replace one of the initial MCMC chains with the initial parameter values. This should improve convergence and prevent significant regression compared to input parameters. (@toastedcrumpets - issue 249)\nSupport checking only a single dataset from command line (@zhyrek - issue 244)\n\n\n\n\n\nFix dataplot() for ternary cases with that use __HYPERPLANE__ (@bocklund - issue 252)\nFix Residual protocol for MCMC for newer Python versions (3.11+) (@bocklund - issue 253)\nCorrectly pass parameters through calculate_activity_error() (@bocklund - issue 246)\n\n\n\n\n\nUpdate pycalphad minimum version to 0.10.4 to support new property modeling framework (@bocklund)\nMigrate to pydantic v2 (@bocklund - issue 247, issue 248)\n[Developer] Add support for Python 3.12 in test framework (@bocklund)\n\n\n\n\n\n\n\n\nSupport varying pressure and P-X plots in dataplot() and ravel_zpf_values() (@bocklund - issue 240)\nMCMC: Refactor residual/likelihood functions to better support custom user data types and likelihood functions (@bocklund - issue 236)\n\n\n\n\n\nDrop Python 3.7 (following NEP-29) (@bocklund - issue 237)\nRemove custom graph of optimizer history (@bocklund - issue 235)\n\n\n\n\n\n\n\n\nGeneralize entering the __HYPERPLANE__ phase in ZPF vertices to control the compositions for the target hyperplane (@bocklund - issue 221)\nUpdate for CompositionSet-based solver in pycalphad 0.10.1 (@bocklund - issue 232)\nFix generating parameters for symmetry of 4 sublattice FCC models (@bocklund - issue 229)\nFix a bug where a trace of all zeros would raise an error.\n\n\n\n\n\n\n\n\nAdd support for Python 3.10 (@bocklund - issue 227)\nAdd support for changes in pycalphad 0.10 that use SymEngine as the symbolic backend (@bocklund - issue 212)\nPerformance improvements in parameter selection, which can lead to more than an order of magnitude improvement for phases with large sublattice models (@bocklund - issue 225)\n\n\n\n\n\n\n\n\nSignificant performance improvement in startup time in MCMC for ZPF data and decrease in runtime memory usage (@bocklund - issue 210)\nImprove sampling of energy surface for finding constrained driving forces in ZPF data, at least one energy surface sample is now guaranteed (@bocklund - issue 211)\nFix bug in MCMC when fitting ZPF data for non-pure element species with recent versions of pycalphad (@wahab2604 - issue 208)\nMCMC: Add support for fitting subsets of the database components given in phase models (@bocklund - issue 218)\nUpdate dependencies and resolve deprecations (@bocklund - issue 222, issue 223)\n\n\n\n\n\n\n\n\nMigrate to the new pycalphad internal APIs for the minimizer and calculate utilities (@bocklund - issue 201 and issue 206)\nImprove AICc formulation to have more consistent behavior when the number of data points is small (@bocklund - issue 205)\nEnable specifying custom pycalphad Model classes in MCMC simulations via the phase models data file (@bocklund - issue 202)\n\n\n\n\n\n\n\n\nMigrate to pycalphad's new minimizer (@bocklund - issue 196)\nFix a memory issue in large sublattice models by using the new minimizer to implement constrained driving forces (@bocklund - issue 196)\nFix a regression where plot_interaction and plot_endmember re-used reference labels and markers (@bocklund - issue 187)\nParameter selection now adds and fits only phases which in the phase models and are active (@bocklund - issue 188)\nFix a bug where plot_interaction and plot_endmember would raise an error when axes were not passed explicitly (@bocklund - issue 191)\nFix a bug where reference keys were assumed to be present in dataplot, plot_interaction and plot_endmember (@bocklund - issue 191)\nDocumentation: Rewrite phase diagram datasets section, switch paper references to RST citations, reorganize sections into how-to/reference material appropriately (@bocklund - issue 192)\nDocumentation: Switch from sphinx_rtd_theme to furo theme (@bocklund - issue 193)\n\n\n\n\n\npycalphad versions lower than 0.9.0 are no longer supported.\n\n\n\n\n\n\n\n\nFix a bug where excluded model contributions could be double counted (@bocklund - issue 181)\nSupport internal API changes for pycalphad 0.8.5 (@bocklund - issue 183)\nFix a regression for ZPF error calculations introduced in issue 181 where prescribed phase compositions of stoichiometric phases that used to work no longer work because the phase composition of a stoichiometric phase may be unsatisfiable (@bocklund - issue 185).\nFix a bug in ZPF error calculations where stoichiometric phases could give incorrect energies for exact equilibrium when prescribed mass balance conditions could not be satisfied. The fix now computes the driving force exactly in all cases for stoichiometric compounds. (@bocklund - issue 185)\n\n\n\n\n\n\n\n\nFix weighting in model selection (@bocklund - issue 176)\n\n\n\n\n\nplot_parameters is deprecated in favor of plot_interaction and plot_endmember (@bocklund - issue 177)\n\n\n\n\n\nThis is a minor release that fixes a performance regression and retires unused utility code.\n\nFixes a performance regression in _sample_solution_constitution that could cause getting ZPF data for MCMC to be extremely slow. (@bocklund - issue 174)\nThe flexible_open_string and add_bibtex_to_bib_database utilities were removed. Both were unusued in ESPEI. ESPEI no longer depends on bibtexparser. (@bocklund - issue 171)\n\n\n\n\nThis is a major release with bug fixes and a backward compatible public API, but breaking changes in the behavior of parameter selection and MCMC parameter estimation. Some internal functions were deprecated.\n\n\n\nRevamped internal logging. For users, ESPEI now has namespaced logging and filters out all non-ESPEI logs (e.g. dask and matplotlib). This change also fixed a bug where changing the verbosity in Jupyter was not taking effect. (@bocklund - issue 165)\nFixed a bug where scalar weights of non-equilibrium thermochemical datasets were not being broadcasted correctly and raised errors. (@bocklund - issue 154)\nFixed a bug where non-equilibrium thermochemical datasets using broadcasted temperatures and compositions were broadcasted against the values incorrectly. (@bocklund - issue 154)\nAllow disabling datasets semantically using disabled: true in JSON datasets. (@bocklund - issue 153)\nUsers can now pass custom SER reference data to override SER phases, mass, H298, and S298 for existing elements or new elements. Includes better warnings for common errors when the SER data is incompatible with the phases being fit. (@bocklund - issue 158)\nFixed a bug in computing activity error in MCMC where species were not correctly generated from the pure comopnents. (@bocklund - issue 152)\n\n\n\n\n\nDriving forces in ZPF error are now computed from local minimum solutions rather than global minimum solutions. This change significantly improves the convergence for any phases with stable or metastable miscibility gaps. It also prevents users from prescribing phase composition conditions that cannot be satisfied. See the linked GitHub issue for a detailed description of the rationale and implementation of this change. (@bocklund - issue 151)\nRemoved automatically added ideal exclusions, which was deprecated in ESPEI 0.7. Non-equilibrium thermochemical data should use the excluded_model_contributions key to exclude idmix, mag or other model contributions. (@bocklund - issue 168)\nRemove deprecated mcmc.py (@bocklund - issue 164)\n\n\n\n\n\nSetting mcmc.scheduler to the string \"None\" to get a serial scheduler is deprecated. Users should use null in YAML/JSON or None in Python.\nDeprecated multiplot and eqdataplot in favor of having users compose binplot and dataplot. pycalphad's binplot is much faster than multiplot. The extra functional call added is worth removing the maintenance burden and allows users to understand more explictly the difference between plotting data and plotting the calculated phase diagram. The documentation was updated to reflect this change and no longer uses multiplot. (@bocklund - issue 162)\n\n\n\n\n\nThis is a minor bugfix release that updates the SGTE reference state data for carbon and more strictly specifies dependences. No changes to the code were made since 0.7.11.\n\n\n\nThis is a minor bugfix release with backwards compatible changes.\n\nFix numpy v1.20 deprecations (@bocklund - issue 147)\nAdd dataplot tie-line flag (@bocklund - issue 145)\nAdd corner package to dependencies so the recipes now work without installing extra packages\n\n\n\n\nThis is a minor bugfix release that addresses a potential inconsistency with hyphen/underscore usage in dask configuration files (@bocklund - issue 136).\n\n\n\nThis is a minor maintenance release that automatically disables work stealing (users are no longer required to configure this themselves) (@bocklund - issue 134).\n\n\n\nThis is a bug fix release with backwards compatible changes.\n\nFix a bug triggered by pycalphad 0.8.4 where the new parameter extraction behavior could break the MCMC sampler (@bocklund - issue 132)\nFix a bug where some feature matrices had incorrect shape, stemming from using SymPy.Matrix to symbolically manipulate the data (@bocklund - issue 130)\nMigrate to tinydb v4+ (@bocklund - issue 126)\n\n\n\n\nThis is a minor feature and bug fix release with backwards compatible changes.\n\nPreliminary support for thermochemical error for arbitrary properties (@bocklund - issue 124)\nUpdate the preferred method for disabling tracefile, probfile, logfile, and no scheduler in YAML to use null instead of \"None\" (@bocklund - issue 125)\nFix a bug in truncate_arrays and optimal_parameters to allow some zeros (@bocklund - issue 122)\nEnable custom unary reference states for parameter generation with entry_points plugin system (@bocklund - issue 121)\n\n\n\n\nThis is a minor bug fix release.\n\nFixes a bug introduced in 0.7.5 for calculating likelihood for phase boundary data under equilibrium failures (@bocklund - issue 120)\nSince Python 2 was dropped, six has been removed as a dependency (@bocklund - issue 119)\n\n\n\n\nThis release includes performance optimizations, bug fixes and new features for MCMC simulations.\n\nThis version of ESPEI now requires pycalphad 0.8.2 or later for the features below.\nFitting subsystems of a large database is explicitly supported and tested for all implemented MCMC data types. Fixes a bug in ZPF error and activity error where having phases in the database that are inactive in the subsystem would raise errors (@bocklund - issue 118).\nComputing thermochemical error and phase boundary (ZPF) error are now optimized to reduce overhead time in dependencies (@bocklund - issue 117)\nA new feature for calculating approximate driving force for phase boundary data is implemented, which can give performance improvements of 3x-10x, depending on the system (@bocklund - issue 115)\n\n\n\n\nThis release includes small fixes for parameter generation.\n\nExcluded model contributions are fixed for models with different sublattice site ratios and for data that are not endmembers (@bocklund - issue 113)\n\n\n\n\nThis change includes several new features and performance improvements.\n\nDrop Python 2 support (Python 2 is no longer supported on January 1, 2020).\nUpdate dask and distributed support to versions &gt;=2. (@bocklund)\nUsers can tweak the AICc penalty factor for each phase to nudge parameter selection towards adding more or fewer parameters based on user modeling intuition. (@bocklund)\nAllow for tracefile and probfile to be set to None. (@jwsiegel2510)\nWeighting individual datasets in single phase fitting is now implemented via scikit-learn. (@bocklund)\nPerformance improvements by reducing overhead. (@bocklund)\nIncreased solver accuracy by using pycalphad's exact Hessian solver. (@bocklund)\nSupport writing SER reference state information to the ELEMENT keyword in TDBs based on the SGTE unary 5 database. (@bocklund)\nMCMC now calculates the likelihood of the initial parameter set so the starting point can be reasonably compared. (@bocklund)\nFixed a bug where mis-aligned configurations and site occupancies in single phase datasets passed the dataset checker (@bocklund)\n\n\n\n\nThis is a small bugfix release that fixes the inability to provide the EmceeOptimizer a restart_trace.\n\n\n\nThis is a significant update reflecting many internal improvements, new features, and bug fixes. Users using the YAML input or the run_espei Python API should see entirely backwards compatible changes with ESPEI 0.6.2.\npycalphad 0.8, which introduced many key features for these changes is now required. This should almost completely eliminate the time to build phases due to the symengine backend (phases will likely build in less time than to call the MCMC objective function). Users can expect a slight performance improvement for MCMC fitting.\n\n\n\nPriors can now be specified and are documented online.\nWeights for different datasets are added and are supported by a \"weight\" key at the top level of any dataset.\nWeights for different types of data are added. These are controlled via the input YAML and are documented there.\nA new internal API is introduced for generic fitting of parameters to datasets in the OptimizerBase class. The MCMC optimizer in emcee was migrated to this API (the mcmc_fit function is now deprecated, but still works until the next major version of ESPEI). A simple SciPy-based optimizer was implemented using this API.\nParameter selection can now be passed initial databases with parameters (e.g. for adding magnetic or other parameters manually).\npycalphad's reference state support can now be used to properly reference out different types of model contributions (ideal mixing, magnetic, etc.). This is especially useful for DFT thermochemical data which does not include model contributions from ideal mixing or magnetic heat capacity. Useful for experimental data which does include ideal mixing (previously ESPEI assumed all data\nDatasets and input YAML files now have a tag system where tags that are specified in the input YAML can override any keys/values in the JSON datasets at runtime. This is useful for tagging data with different weights/model contribution exclusions (e.g. DFT tags may get lower weights and can be set to exclude model contributions). If no tags are applied, removing ideal mixing from all thermochemical data is applied automatically for backwards compatibility. This backwards compatibility feature will be removed in the next major version of ESPEI (all model contributions will be included by default and exclusions must be specified manually).\n\n\n\n\n\nBug fixed where asymmetric ternary parameters were not properly replaced in SymPy\nFixed error where ZPF error was considering the chemical potentials of stoichiometric phases in the target hyperplane (they are meaningless)\nReport the actual file paths when dask's work-stealing is set to false.\nErrors in the ZPF error function are no longer swallowed with -np.inf error. Any errors should be reported as bugs.\nFix bug where subsets of symbols to fit are not built properly for thermochemical data\n\n\n\n\n\nDocumentation recipe added for plot_parameters\n[Developer] ZPF and thermochemical datasets now have an function to get all the data up front in a dictionary that can be used in the functions for separation of concerns and calculation efficiency by not recalculating the same thing every iteration.\n[Developer] a function to generate the a context dict to pass to lnprob now exists. It gets the datasets automatically using the above.\n[Developer] transition to pycalphad's new build_callables function, taking care of the v.N state variable.\n[Developer] Load all YAML safely, silencing warnings.\n\n\n\n\n\nThis backwards-compatible release includes several bug fixes and improvements.\n\nUpdated branding to include the new ESPEI logo. See the logo in the docs/_static directory.\nAdd support for fitting excess heat capacity.\nBug fix for broken potassium unary.\nDocumentation improvements for recipes\npycalphad 0.7.1 fixes for dask, sympy, and gmpy2 should mean that ESPEI should not require package upgrade or downgrades. Please report any installations issues in ESPEI's Gitter Channel.\n[Developers] ESPEI's eq_callables_dict is now pycalphad.codegen.callables.build_callables.\n[Developers] matplotlib plotting tests are removed because nose is no longer supported.\n\n\n\n\nThis a major release with several important features and bug fixes.\n\nEnable use of ridge regression alpha for parameter selection via the parameter_generation.ridge_alpha input parameter.\nAdd ternary parameter selection. Works by default, just add data.\nSet memory limit to zero to avoid dask killing workers near the dask memory limits.\nRemove ideal mixing from plotting models so that plot_parameters gives the correct entropy values.\nAdd recipes documentation that contains some Python code for common utility operations.\nAdd documentation for running custom distributed schedulers in ESPEI\n\n\n\n\nThis is a update including breaking changes to the input files and several minor improvements.\n\nUpdate input file schema and Python API to be more consistent so that the trace always refers to the collection of chains and chain refers to individual chains. Additionally removed some redundancy in the parameters nested under the mcmc heading, e.g. mcmc_steps is now iterations and mcmc_save_interval is now save_interval in the input file and Python API. See YAML Schema documentation for all of the updates.\nThe default save interval is now 1, which is more reasonable for most MCMC systems with significant numbers of phase equilibria.\nBug fixes for plotting and some better plotting defaults for plotting input data\nDataset parsing and cleaning improvements.\nDocumentation improvements (see the PDF!)\n\n\n\n\nThis is a major bugfix release for MCMC multi-phase fitting runs for single phase data.\n\nFixed a major issue where single phase thermochemical data was always compared to Gibbs energy, giving incorrect errors in MCMC runs.\nSingle phase errors in ESPEI incorrectly compared values with ideal mixing contributions to data, which is excess only.\nFixed a bug where single phase thermochemical data with that are dependent on composition and pressure and/or temperature were not fit correctly.\nAdded utilities for analyzing ESPEI results and add them to the Cu-Mg example docs.\n\n\n\n\nThis is a minor bugfix release.\n\nParameter generation for phases with vacancies would produce incorrect parameters because the vacancy site fractions were not being correctly removed from the contributions due to their treatment as Species objects in pycalphad &gt;=0.7.\n\n\n\n\n\nParameter selection now uses the corrected AIC, which further prevents overparameterization where there is sparse training data.\nActivity and single phase thermochemical data can now be included in MCMC fitting runs. Including single phase data can help anchor metastable phases to DFT data when they are not on the stable phase diagram. See the Gathering input data documentation for information on how to input activity data.\nDataset checking has been improved. Now there are checks to make sure sublattice interactions are properly sorted and mole fractions sum to less than 1.0 in ZPF data.\nSupport for fitting phases with arbitrary pycalphad Models in MCMC, including (charged and neutral) species and ionic liquids. There are several consequences of this:\n\nESPEI requires support on pycalphad &gt;=0.7\nESPEI now uses pycalphad Model objects directly. Using the JIT compiled Models has shown up to a 50% performance improvement in MCMC runs.\nUsing JIT compiled Model objects required the use of cloudpickle everywhere. Due to challenges in overriding pickle for upstream packages, we now rely solely on dask for scheduler tasks, including mpi via dask-mpi. Note that users must turn off work-stealing in their ~/.dask/config.yaml file.\n\n[Developers] Each method for calculating error in MCMC has been moved into a module for that method in an error_functions subpackage. One top level function from each module should be imported into the mcmc.py and used in lnprob. Developers should then just customize lnprob.\n[Developers] Significant internal docs improvements: all non-trivial functions have complete docstrings.\n\n\n\n\n\nEnable plotting of isothermal sections with data using dataplot (and multiplot, etc.)\nTielines are now plotted in dataplot for isothermal sections and T-x phase diagrams\nAdd a useful ravel_conditions method to unpack conditions from datasets\n\n\n\n\n\nMCMC is now deterministic by default (can be toggled off with the mcmc.deterministic setting).\nAdded support for having no scheduler (running with no parallelism) with the mcmc.scheduler option set to None. This may be useful for debugging.\nLogging improvements\n\nExtraneous warnings that may be confusing for users and dirty the log are silenced.\nA warning is added for when there are no datasets found.\nFixed a bug where logging was silenced with the dask scheduler\n\nAdd optimal_parameters utility function as a helper to get optimal parameter sets for analysis\nSeveral improvements to plotting\n\nUsers can now plot phase diagram data alone with dataplot, useful for checking datasets visually. This changes the API for dataplot to no longer infer the conditions from an equilibrium Dataset (from pycalphad). That functionality is preserved in eqdataplot.\nExperimental data points are now plotted with unique symbols depending on the reference key in the dataset. This is for both phase diagram and single phase parameter plots.\nOptions to control plotting parameters (e.g. symbol size) and take user supplied Axes and Figures in the plotting functions. The symbol size is now smaller by default.\n\nDocumentation improvements for API and separation of theory from the Cu-Mg example\nFixes a bug where elements with single character names would not find the correct reference state (which are typically named GHSERCC for the example of C).\n[Developer] All MCMC code is moved from the paramselect module to the mcmc module to separate these tasks\n[Developer] Support for arbitrary user reference states (so long as the reference state is in the refdata module and follows the same format as SGTE91)\n\n\n\n\n\nPropagate the new entry point to setup.py\n\n\n\n\n\nFix for module name/function conflict in entry point\n\n\n\n\n\nESPEI is much easier to run interactively in Python and in Jupyter Notebooks\nReference data is now included in ESPEI instead of in pycalphad\nSeveral reference data fixes including support for single character elements ('V', 'B', 'C', ...)\nSupport for using multiprocessing to parallelize MCMC runs, used by default (@olivia-higgins)\nImproved documentation for installing and developing ESPEI\n\n\n\n\n\nAdd input-schema.yaml file to installer\n\n\n\n\n\nAdd LICENSE to manifest\n\n\n\n\n\nESPEI input is now described by a file. This change is breaking. Old command line arguments are not supported. See Writing input files for a full description of all the inputs.\nNew input options are supported, including modifying the number of chains and standard deviation from the mean\nESPEI is now available on conda-forge\nTinyDB 2 support is dropped in favor of TinyDB 3 for conda-forge deployment\nAllow for restarting previous mcmc calculations with a trace file\nAdd Cu-Mg example to documentation\n\n\n\n\nFixes to the 0.2 release plotting interface\n\nmultiplot is renamed from multi_plot, as in docs.\nFixed an issue where phases in datasets, but not in equilibrium were not plotted by dataplot and raised an error.\n\n\n\n\n\nNew multiplot interface for convenient plotting of phase diagrams + data. dataplot function underlies key data plotting features and can be used with eqplot. See their API docs for examples. Will break existing code using multiplot.\nMPI support for local/HPC runs. Only single node runs are explicitly supported currently. Use --scheduler='MPIPool' command line option. Requires mpi4py.\nDefault debug reporting of acceptance ratios\nOption (and default) to output the log probability array matching the trace. Use --probfile option to control.\nOptimal parameters are now chosen based on lowest error in chain.\nBug fixes including\n\n\npy2/3 compatibility\nUnicode datasets\nhandling of singular matrix errors from pycalphad's equilibrium\nreporting of failed conditions\n\n\n\n\n\n\n\nSignificant error checking of JSON inputs.\nAdd new --check-datasets option to check the datasets at path. It should be run before you run ESPEI fittings. All errors must be resolved before you run.\nMove the espei script module from fit.py to run_espei.py.\nBetter docs building with mocking\nGoogle docstrings are now NumPy docstrings\n\n\n\n\n\nDocumentation improvements for usage and API docs\nFail fast on JSON errors\n\n\n\n\n\nFix bad version pinning in setup.py\nExplicitly support Python 2.7\n\n\n\n\n\nFix dask incompatibility due to new API usage\n\n\n\n\n\nFix a bug that caused logging to raise if bokeh isn't installed\n\n\n\n\nESPEI is now a package! New features include\n\nFork https://github.com/richardotis/pycalphad-fitting\nUse emcee for MCMC fitting rather than pymc\nSupport single-phase only fitting\nMore control options for running ESPEI from the command line\nBetter support for incremental saving of the chain\nControl over output with logging over printing\nSignificant code cleanup\nBetter usage documentation",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section",
    "href": "developer/changelog.html#section",
    "title": "Changelog",
    "section": "",
    "text": "Migrate to PyCalphad Workspace (@bocklund - issue 256)\n\nPyCalphad's IsolatedPhase feature improves accuracy and elimintes the need for a heuristic to perform constrained equilibria when computing the likelihood for phase diagram data.\nESPEI users will likely see a small 2-4x performance regression in MCMC as some of the heavily optimized code paths in ESPEI were removed in favor of the versions implemented in Workspace. We are looking in to further optimizations and utilizing Jansson derivatives to compute likelihood gradients that will enable us to use more performant optimization and MCMC methods.\n\n\n\n\n\n\nUpdate minimum required PyCalphad version to 0.11 (@bocklund - issue 259)\nAdd NumPy 2.0 support (@bocklund - issue 255)\n\n\n\n\n\nRemove code scheduled for deprecation in ESPEI 0.9: deprecations (@bocklund - issue 257)\n\nespei.plot: plot_parameters is replaced with plot_interaction and plot_endmember. eqdataplot and multiplot are replaced with using dataplot combined with the relevant PyCalphad phase diagram plotting code.\nESPEI YAML input: scheduler: \"None\" is replaced by scheduler: null",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-1",
    "href": "developer/changelog.html#section-1",
    "title": "Changelog",
    "section": "",
    "text": "Add a generic framework for property model parameter generation (@bocklund - issue 251)\nImprove performance for generating candidate models with many features (@bocklund - issue 254)\nMCMC: Replace one of the initial MCMC chains with the initial parameter values. This should improve convergence and prevent significant regression compared to input parameters. (@toastedcrumpets - issue 249)\nSupport checking only a single dataset from command line (@zhyrek - issue 244)\n\n\n\n\n\nFix dataplot() for ternary cases with that use __HYPERPLANE__ (@bocklund - issue 252)\nFix Residual protocol for MCMC for newer Python versions (3.11+) (@bocklund - issue 253)\nCorrectly pass parameters through calculate_activity_error() (@bocklund - issue 246)\n\n\n\n\n\nUpdate pycalphad minimum version to 0.10.4 to support new property modeling framework (@bocklund)\nMigrate to pydantic v2 (@bocklund - issue 247, issue 248)\n[Developer] Add support for Python 3.12 in test framework (@bocklund)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-2",
    "href": "developer/changelog.html#section-2",
    "title": "Changelog",
    "section": "",
    "text": "Support varying pressure and P-X plots in dataplot() and ravel_zpf_values() (@bocklund - issue 240)\nMCMC: Refactor residual/likelihood functions to better support custom user data types and likelihood functions (@bocklund - issue 236)\n\n\n\n\n\nDrop Python 3.7 (following NEP-29) (@bocklund - issue 237)\nRemove custom graph of optimizer history (@bocklund - issue 235)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-3",
    "href": "developer/changelog.html#section-3",
    "title": "Changelog",
    "section": "",
    "text": "Generalize entering the __HYPERPLANE__ phase in ZPF vertices to control the compositions for the target hyperplane (@bocklund - issue 221)\nUpdate for CompositionSet-based solver in pycalphad 0.10.1 (@bocklund - issue 232)\nFix generating parameters for symmetry of 4 sublattice FCC models (@bocklund - issue 229)\nFix a bug where a trace of all zeros would raise an error.",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-4",
    "href": "developer/changelog.html#section-4",
    "title": "Changelog",
    "section": "",
    "text": "Add support for Python 3.10 (@bocklund - issue 227)\nAdd support for changes in pycalphad 0.10 that use SymEngine as the symbolic backend (@bocklund - issue 212)\nPerformance improvements in parameter selection, which can lead to more than an order of magnitude improvement for phases with large sublattice models (@bocklund - issue 225)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-5",
    "href": "developer/changelog.html#section-5",
    "title": "Changelog",
    "section": "",
    "text": "Significant performance improvement in startup time in MCMC for ZPF data and decrease in runtime memory usage (@bocklund - issue 210)\nImprove sampling of energy surface for finding constrained driving forces in ZPF data, at least one energy surface sample is now guaranteed (@bocklund - issue 211)\nFix bug in MCMC when fitting ZPF data for non-pure element species with recent versions of pycalphad (@wahab2604 - issue 208)\nMCMC: Add support for fitting subsets of the database components given in phase models (@bocklund - issue 218)\nUpdate dependencies and resolve deprecations (@bocklund - issue 222, issue 223)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-6",
    "href": "developer/changelog.html#section-6",
    "title": "Changelog",
    "section": "",
    "text": "Migrate to the new pycalphad internal APIs for the minimizer and calculate utilities (@bocklund - issue 201 and issue 206)\nImprove AICc formulation to have more consistent behavior when the number of data points is small (@bocklund - issue 205)\nEnable specifying custom pycalphad Model classes in MCMC simulations via the phase models data file (@bocklund - issue 202)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-7",
    "href": "developer/changelog.html#section-7",
    "title": "Changelog",
    "section": "",
    "text": "Migrate to pycalphad's new minimizer (@bocklund - issue 196)\nFix a memory issue in large sublattice models by using the new minimizer to implement constrained driving forces (@bocklund - issue 196)\nFix a regression where plot_interaction and plot_endmember re-used reference labels and markers (@bocklund - issue 187)\nParameter selection now adds and fits only phases which in the phase models and are active (@bocklund - issue 188)\nFix a bug where plot_interaction and plot_endmember would raise an error when axes were not passed explicitly (@bocklund - issue 191)\nFix a bug where reference keys were assumed to be present in dataplot, plot_interaction and plot_endmember (@bocklund - issue 191)\nDocumentation: Rewrite phase diagram datasets section, switch paper references to RST citations, reorganize sections into how-to/reference material appropriately (@bocklund - issue 192)\nDocumentation: Switch from sphinx_rtd_theme to furo theme (@bocklund - issue 193)\n\n\n\n\n\npycalphad versions lower than 0.9.0 are no longer supported.",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-8",
    "href": "developer/changelog.html#section-8",
    "title": "Changelog",
    "section": "",
    "text": "Fix a bug where excluded model contributions could be double counted (@bocklund - issue 181)\nSupport internal API changes for pycalphad 0.8.5 (@bocklund - issue 183)\nFix a regression for ZPF error calculations introduced in issue 181 where prescribed phase compositions of stoichiometric phases that used to work no longer work because the phase composition of a stoichiometric phase may be unsatisfiable (@bocklund - issue 185).\nFix a bug in ZPF error calculations where stoichiometric phases could give incorrect energies for exact equilibrium when prescribed mass balance conditions could not be satisfied. The fix now computes the driving force exactly in all cases for stoichiometric compounds. (@bocklund - issue 185)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-9",
    "href": "developer/changelog.html#section-9",
    "title": "Changelog",
    "section": "",
    "text": "Fix weighting in model selection (@bocklund - issue 176)\n\n\n\n\n\nplot_parameters is deprecated in favor of plot_interaction and plot_endmember (@bocklund - issue 177)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-10",
    "href": "developer/changelog.html#section-10",
    "title": "Changelog",
    "section": "",
    "text": "This is a minor release that fixes a performance regression and retires unused utility code.\n\nFixes a performance regression in _sample_solution_constitution that could cause getting ZPF data for MCMC to be extremely slow. (@bocklund - issue 174)\nThe flexible_open_string and add_bibtex_to_bib_database utilities were removed. Both were unusued in ESPEI. ESPEI no longer depends on bibtexparser. (@bocklund - issue 171)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-11",
    "href": "developer/changelog.html#section-11",
    "title": "Changelog",
    "section": "",
    "text": "This is a major release with bug fixes and a backward compatible public API, but breaking changes in the behavior of parameter selection and MCMC parameter estimation. Some internal functions were deprecated.\n\n\n\nRevamped internal logging. For users, ESPEI now has namespaced logging and filters out all non-ESPEI logs (e.g. dask and matplotlib). This change also fixed a bug where changing the verbosity in Jupyter was not taking effect. (@bocklund - issue 165)\nFixed a bug where scalar weights of non-equilibrium thermochemical datasets were not being broadcasted correctly and raised errors. (@bocklund - issue 154)\nFixed a bug where non-equilibrium thermochemical datasets using broadcasted temperatures and compositions were broadcasted against the values incorrectly. (@bocklund - issue 154)\nAllow disabling datasets semantically using disabled: true in JSON datasets. (@bocklund - issue 153)\nUsers can now pass custom SER reference data to override SER phases, mass, H298, and S298 for existing elements or new elements. Includes better warnings for common errors when the SER data is incompatible with the phases being fit. (@bocklund - issue 158)\nFixed a bug in computing activity error in MCMC where species were not correctly generated from the pure comopnents. (@bocklund - issue 152)\n\n\n\n\n\nDriving forces in ZPF error are now computed from local minimum solutions rather than global minimum solutions. This change significantly improves the convergence for any phases with stable or metastable miscibility gaps. It also prevents users from prescribing phase composition conditions that cannot be satisfied. See the linked GitHub issue for a detailed description of the rationale and implementation of this change. (@bocklund - issue 151)\nRemoved automatically added ideal exclusions, which was deprecated in ESPEI 0.7. Non-equilibrium thermochemical data should use the excluded_model_contributions key to exclude idmix, mag or other model contributions. (@bocklund - issue 168)\nRemove deprecated mcmc.py (@bocklund - issue 164)\n\n\n\n\n\nSetting mcmc.scheduler to the string \"None\" to get a serial scheduler is deprecated. Users should use null in YAML/JSON or None in Python.\nDeprecated multiplot and eqdataplot in favor of having users compose binplot and dataplot. pycalphad's binplot is much faster than multiplot. The extra functional call added is worth removing the maintenance burden and allows users to understand more explictly the difference between plotting data and plotting the calculated phase diagram. The documentation was updated to reflect this change and no longer uses multiplot. (@bocklund - issue 162)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-12",
    "href": "developer/changelog.html#section-12",
    "title": "Changelog",
    "section": "",
    "text": "This is a minor bugfix release that updates the SGTE reference state data for carbon and more strictly specifies dependences. No changes to the code were made since 0.7.11.",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-13",
    "href": "developer/changelog.html#section-13",
    "title": "Changelog",
    "section": "",
    "text": "This is a minor bugfix release with backwards compatible changes.\n\nFix numpy v1.20 deprecations (@bocklund - issue 147)\nAdd dataplot tie-line flag (@bocklund - issue 145)\nAdd corner package to dependencies so the recipes now work without installing extra packages",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-14",
    "href": "developer/changelog.html#section-14",
    "title": "Changelog",
    "section": "",
    "text": "This is a minor bugfix release that addresses a potential inconsistency with hyphen/underscore usage in dask configuration files (@bocklund - issue 136).",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-15",
    "href": "developer/changelog.html#section-15",
    "title": "Changelog",
    "section": "",
    "text": "This is a minor maintenance release that automatically disables work stealing (users are no longer required to configure this themselves) (@bocklund - issue 134).",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-16",
    "href": "developer/changelog.html#section-16",
    "title": "Changelog",
    "section": "",
    "text": "This is a bug fix release with backwards compatible changes.\n\nFix a bug triggered by pycalphad 0.8.4 where the new parameter extraction behavior could break the MCMC sampler (@bocklund - issue 132)\nFix a bug where some feature matrices had incorrect shape, stemming from using SymPy.Matrix to symbolically manipulate the data (@bocklund - issue 130)\nMigrate to tinydb v4+ (@bocklund - issue 126)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-17",
    "href": "developer/changelog.html#section-17",
    "title": "Changelog",
    "section": "",
    "text": "This is a minor feature and bug fix release with backwards compatible changes.\n\nPreliminary support for thermochemical error for arbitrary properties (@bocklund - issue 124)\nUpdate the preferred method for disabling tracefile, probfile, logfile, and no scheduler in YAML to use null instead of \"None\" (@bocklund - issue 125)\nFix a bug in truncate_arrays and optimal_parameters to allow some zeros (@bocklund - issue 122)\nEnable custom unary reference states for parameter generation with entry_points plugin system (@bocklund - issue 121)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-18",
    "href": "developer/changelog.html#section-18",
    "title": "Changelog",
    "section": "",
    "text": "This is a minor bug fix release.\n\nFixes a bug introduced in 0.7.5 for calculating likelihood for phase boundary data under equilibrium failures (@bocklund - issue 120)\nSince Python 2 was dropped, six has been removed as a dependency (@bocklund - issue 119)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-19",
    "href": "developer/changelog.html#section-19",
    "title": "Changelog",
    "section": "",
    "text": "This release includes performance optimizations, bug fixes and new features for MCMC simulations.\n\nThis version of ESPEI now requires pycalphad 0.8.2 or later for the features below.\nFitting subsystems of a large database is explicitly supported and tested for all implemented MCMC data types. Fixes a bug in ZPF error and activity error where having phases in the database that are inactive in the subsystem would raise errors (@bocklund - issue 118).\nComputing thermochemical error and phase boundary (ZPF) error are now optimized to reduce overhead time in dependencies (@bocklund - issue 117)\nA new feature for calculating approximate driving force for phase boundary data is implemented, which can give performance improvements of 3x-10x, depending on the system (@bocklund - issue 115)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-20",
    "href": "developer/changelog.html#section-20",
    "title": "Changelog",
    "section": "",
    "text": "This release includes small fixes for parameter generation.\n\nExcluded model contributions are fixed for models with different sublattice site ratios and for data that are not endmembers (@bocklund - issue 113)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-21",
    "href": "developer/changelog.html#section-21",
    "title": "Changelog",
    "section": "",
    "text": "This change includes several new features and performance improvements.\n\nDrop Python 2 support (Python 2 is no longer supported on January 1, 2020).\nUpdate dask and distributed support to versions &gt;=2. (@bocklund)\nUsers can tweak the AICc penalty factor for each phase to nudge parameter selection towards adding more or fewer parameters based on user modeling intuition. (@bocklund)\nAllow for tracefile and probfile to be set to None. (@jwsiegel2510)\nWeighting individual datasets in single phase fitting is now implemented via scikit-learn. (@bocklund)\nPerformance improvements by reducing overhead. (@bocklund)\nIncreased solver accuracy by using pycalphad's exact Hessian solver. (@bocklund)\nSupport writing SER reference state information to the ELEMENT keyword in TDBs based on the SGTE unary 5 database. (@bocklund)\nMCMC now calculates the likelihood of the initial parameter set so the starting point can be reasonably compared. (@bocklund)\nFixed a bug where mis-aligned configurations and site occupancies in single phase datasets passed the dataset checker (@bocklund)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-22",
    "href": "developer/changelog.html#section-22",
    "title": "Changelog",
    "section": "",
    "text": "This is a small bugfix release that fixes the inability to provide the EmceeOptimizer a restart_trace.",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-23",
    "href": "developer/changelog.html#section-23",
    "title": "Changelog",
    "section": "",
    "text": "This is a significant update reflecting many internal improvements, new features, and bug fixes. Users using the YAML input or the run_espei Python API should see entirely backwards compatible changes with ESPEI 0.6.2.\npycalphad 0.8, which introduced many key features for these changes is now required. This should almost completely eliminate the time to build phases due to the symengine backend (phases will likely build in less time than to call the MCMC objective function). Users can expect a slight performance improvement for MCMC fitting.\n\n\n\nPriors can now be specified and are documented online.\nWeights for different datasets are added and are supported by a \"weight\" key at the top level of any dataset.\nWeights for different types of data are added. These are controlled via the input YAML and are documented there.\nA new internal API is introduced for generic fitting of parameters to datasets in the OptimizerBase class. The MCMC optimizer in emcee was migrated to this API (the mcmc_fit function is now deprecated, but still works until the next major version of ESPEI). A simple SciPy-based optimizer was implemented using this API.\nParameter selection can now be passed initial databases with parameters (e.g. for adding magnetic or other parameters manually).\npycalphad's reference state support can now be used to properly reference out different types of model contributions (ideal mixing, magnetic, etc.). This is especially useful for DFT thermochemical data which does not include model contributions from ideal mixing or magnetic heat capacity. Useful for experimental data which does include ideal mixing (previously ESPEI assumed all data\nDatasets and input YAML files now have a tag system where tags that are specified in the input YAML can override any keys/values in the JSON datasets at runtime. This is useful for tagging data with different weights/model contribution exclusions (e.g. DFT tags may get lower weights and can be set to exclude model contributions). If no tags are applied, removing ideal mixing from all thermochemical data is applied automatically for backwards compatibility. This backwards compatibility feature will be removed in the next major version of ESPEI (all model contributions will be included by default and exclusions must be specified manually).\n\n\n\n\n\nBug fixed where asymmetric ternary parameters were not properly replaced in SymPy\nFixed error where ZPF error was considering the chemical potentials of stoichiometric phases in the target hyperplane (they are meaningless)\nReport the actual file paths when dask's work-stealing is set to false.\nErrors in the ZPF error function are no longer swallowed with -np.inf error. Any errors should be reported as bugs.\nFix bug where subsets of symbols to fit are not built properly for thermochemical data\n\n\n\n\n\nDocumentation recipe added for plot_parameters\n[Developer] ZPF and thermochemical datasets now have an function to get all the data up front in a dictionary that can be used in the functions for separation of concerns and calculation efficiency by not recalculating the same thing every iteration.\n[Developer] a function to generate the a context dict to pass to lnprob now exists. It gets the datasets automatically using the above.\n[Developer] transition to pycalphad's new build_callables function, taking care of the v.N state variable.\n[Developer] Load all YAML safely, silencing warnings.",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-24",
    "href": "developer/changelog.html#section-24",
    "title": "Changelog",
    "section": "",
    "text": "This backwards-compatible release includes several bug fixes and improvements.\n\nUpdated branding to include the new ESPEI logo. See the logo in the docs/_static directory.\nAdd support for fitting excess heat capacity.\nBug fix for broken potassium unary.\nDocumentation improvements for recipes\npycalphad 0.7.1 fixes for dask, sympy, and gmpy2 should mean that ESPEI should not require package upgrade or downgrades. Please report any installations issues in ESPEI's Gitter Channel.\n[Developers] ESPEI's eq_callables_dict is now pycalphad.codegen.callables.build_callables.\n[Developers] matplotlib plotting tests are removed because nose is no longer supported.",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-25",
    "href": "developer/changelog.html#section-25",
    "title": "Changelog",
    "section": "",
    "text": "This a major release with several important features and bug fixes.\n\nEnable use of ridge regression alpha for parameter selection via the parameter_generation.ridge_alpha input parameter.\nAdd ternary parameter selection. Works by default, just add data.\nSet memory limit to zero to avoid dask killing workers near the dask memory limits.\nRemove ideal mixing from plotting models so that plot_parameters gives the correct entropy values.\nAdd recipes documentation that contains some Python code for common utility operations.\nAdd documentation for running custom distributed schedulers in ESPEI",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-26",
    "href": "developer/changelog.html#section-26",
    "title": "Changelog",
    "section": "",
    "text": "This is a update including breaking changes to the input files and several minor improvements.\n\nUpdate input file schema and Python API to be more consistent so that the trace always refers to the collection of chains and chain refers to individual chains. Additionally removed some redundancy in the parameters nested under the mcmc heading, e.g. mcmc_steps is now iterations and mcmc_save_interval is now save_interval in the input file and Python API. See YAML Schema documentation for all of the updates.\nThe default save interval is now 1, which is more reasonable for most MCMC systems with significant numbers of phase equilibria.\nBug fixes for plotting and some better plotting defaults for plotting input data\nDataset parsing and cleaning improvements.\nDocumentation improvements (see the PDF!)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-27",
    "href": "developer/changelog.html#section-27",
    "title": "Changelog",
    "section": "",
    "text": "This is a major bugfix release for MCMC multi-phase fitting runs for single phase data.\n\nFixed a major issue where single phase thermochemical data was always compared to Gibbs energy, giving incorrect errors in MCMC runs.\nSingle phase errors in ESPEI incorrectly compared values with ideal mixing contributions to data, which is excess only.\nFixed a bug where single phase thermochemical data with that are dependent on composition and pressure and/or temperature were not fit correctly.\nAdded utilities for analyzing ESPEI results and add them to the Cu-Mg example docs.",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-28",
    "href": "developer/changelog.html#section-28",
    "title": "Changelog",
    "section": "",
    "text": "This is a minor bugfix release.\n\nParameter generation for phases with vacancies would produce incorrect parameters because the vacancy site fractions were not being correctly removed from the contributions due to their treatment as Species objects in pycalphad &gt;=0.7.",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-29",
    "href": "developer/changelog.html#section-29",
    "title": "Changelog",
    "section": "",
    "text": "Parameter selection now uses the corrected AIC, which further prevents overparameterization where there is sparse training data.\nActivity and single phase thermochemical data can now be included in MCMC fitting runs. Including single phase data can help anchor metastable phases to DFT data when they are not on the stable phase diagram. See the Gathering input data documentation for information on how to input activity data.\nDataset checking has been improved. Now there are checks to make sure sublattice interactions are properly sorted and mole fractions sum to less than 1.0 in ZPF data.\nSupport for fitting phases with arbitrary pycalphad Models in MCMC, including (charged and neutral) species and ionic liquids. There are several consequences of this:\n\nESPEI requires support on pycalphad &gt;=0.7\nESPEI now uses pycalphad Model objects directly. Using the JIT compiled Models has shown up to a 50% performance improvement in MCMC runs.\nUsing JIT compiled Model objects required the use of cloudpickle everywhere. Due to challenges in overriding pickle for upstream packages, we now rely solely on dask for scheduler tasks, including mpi via dask-mpi. Note that users must turn off work-stealing in their ~/.dask/config.yaml file.\n\n[Developers] Each method for calculating error in MCMC has been moved into a module for that method in an error_functions subpackage. One top level function from each module should be imported into the mcmc.py and used in lnprob. Developers should then just customize lnprob.\n[Developers] Significant internal docs improvements: all non-trivial functions have complete docstrings.",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-30",
    "href": "developer/changelog.html#section-30",
    "title": "Changelog",
    "section": "",
    "text": "Enable plotting of isothermal sections with data using dataplot (and multiplot, etc.)\nTielines are now plotted in dataplot for isothermal sections and T-x phase diagrams\nAdd a useful ravel_conditions method to unpack conditions from datasets",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-31",
    "href": "developer/changelog.html#section-31",
    "title": "Changelog",
    "section": "",
    "text": "MCMC is now deterministic by default (can be toggled off with the mcmc.deterministic setting).\nAdded support for having no scheduler (running with no parallelism) with the mcmc.scheduler option set to None. This may be useful for debugging.\nLogging improvements\n\nExtraneous warnings that may be confusing for users and dirty the log are silenced.\nA warning is added for when there are no datasets found.\nFixed a bug where logging was silenced with the dask scheduler\n\nAdd optimal_parameters utility function as a helper to get optimal parameter sets for analysis\nSeveral improvements to plotting\n\nUsers can now plot phase diagram data alone with dataplot, useful for checking datasets visually. This changes the API for dataplot to no longer infer the conditions from an equilibrium Dataset (from pycalphad). That functionality is preserved in eqdataplot.\nExperimental data points are now plotted with unique symbols depending on the reference key in the dataset. This is for both phase diagram and single phase parameter plots.\nOptions to control plotting parameters (e.g. symbol size) and take user supplied Axes and Figures in the plotting functions. The symbol size is now smaller by default.\n\nDocumentation improvements for API and separation of theory from the Cu-Mg example\nFixes a bug where elements with single character names would not find the correct reference state (which are typically named GHSERCC for the example of C).\n[Developer] All MCMC code is moved from the paramselect module to the mcmc module to separate these tasks\n[Developer] Support for arbitrary user reference states (so long as the reference state is in the refdata module and follows the same format as SGTE91)",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#post2-2017-10-31",
    "href": "developer/changelog.html#post2-2017-10-31",
    "title": "Changelog",
    "section": "",
    "text": "Propagate the new entry point to setup.py",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#post1-2017-10-31",
    "href": "developer/changelog.html#post1-2017-10-31",
    "title": "Changelog",
    "section": "",
    "text": "Fix for module name/function conflict in entry point",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-32",
    "href": "developer/changelog.html#section-32",
    "title": "Changelog",
    "section": "",
    "text": "ESPEI is much easier to run interactively in Python and in Jupyter Notebooks\nReference data is now included in ESPEI instead of in pycalphad\nSeveral reference data fixes including support for single character elements ('V', 'B', 'C', ...)\nSupport for using multiprocessing to parallelize MCMC runs, used by default (@olivia-higgins)\nImproved documentation for installing and developing ESPEI",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#post2-2017-09-20",
    "href": "developer/changelog.html#post2-2017-09-20",
    "title": "Changelog",
    "section": "",
    "text": "Add input-schema.yaml file to installer",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#post1-2017-09-20",
    "href": "developer/changelog.html#post1-2017-09-20",
    "title": "Changelog",
    "section": "",
    "text": "Add LICENSE to manifest",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-33",
    "href": "developer/changelog.html#section-33",
    "title": "Changelog",
    "section": "",
    "text": "ESPEI input is now described by a file. This change is breaking. Old command line arguments are not supported. See Writing input files for a full description of all the inputs.\nNew input options are supported, including modifying the number of chains and standard deviation from the mean\nESPEI is now available on conda-forge\nTinyDB 2 support is dropped in favor of TinyDB 3 for conda-forge deployment\nAllow for restarting previous mcmc calculations with a trace file\nAdd Cu-Mg example to documentation",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-34",
    "href": "developer/changelog.html#section-34",
    "title": "Changelog",
    "section": "",
    "text": "Fixes to the 0.2 release plotting interface\n\nmultiplot is renamed from multi_plot, as in docs.\nFixed an issue where phases in datasets, but not in equilibrium were not plotted by dataplot and raised an error.",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-35",
    "href": "developer/changelog.html#section-35",
    "title": "Changelog",
    "section": "",
    "text": "New multiplot interface for convenient plotting of phase diagrams + data. dataplot function underlies key data plotting features and can be used with eqplot. See their API docs for examples. Will break existing code using multiplot.\nMPI support for local/HPC runs. Only single node runs are explicitly supported currently. Use --scheduler='MPIPool' command line option. Requires mpi4py.\nDefault debug reporting of acceptance ratios\nOption (and default) to output the log probability array matching the trace. Use --probfile option to control.\nOptimal parameters are now chosen based on lowest error in chain.\nBug fixes including\n\n\npy2/3 compatibility\nUnicode datasets\nhandling of singular matrix errors from pycalphad's equilibrium\nreporting of failed conditions",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-36",
    "href": "developer/changelog.html#section-36",
    "title": "Changelog",
    "section": "",
    "text": "Significant error checking of JSON inputs.\nAdd new --check-datasets option to check the datasets at path. It should be run before you run ESPEI fittings. All errors must be resolved before you run.\nMove the espei script module from fit.py to run_espei.py.\nBetter docs building with mocking\nGoogle docstrings are now NumPy docstrings",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-37",
    "href": "developer/changelog.html#section-37",
    "title": "Changelog",
    "section": "",
    "text": "Documentation improvements for usage and API docs\nFail fast on JSON errors",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-38",
    "href": "developer/changelog.html#section-38",
    "title": "Changelog",
    "section": "",
    "text": "Fix bad version pinning in setup.py\nExplicitly support Python 2.7",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-39",
    "href": "developer/changelog.html#section-39",
    "title": "Changelog",
    "section": "",
    "text": "Fix dask incompatibility due to new API usage",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-40",
    "href": "developer/changelog.html#section-40",
    "title": "Changelog",
    "section": "",
    "text": "Fix a bug that caused logging to raise if bokeh isn't installed",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "developer/changelog.html#section-41",
    "href": "developer/changelog.html#section-41",
    "title": "Changelog",
    "section": "",
    "text": "ESPEI is now a package! New features include\n\nFork https://github.com/richardotis/pycalphad-fitting\nUse emcee for MCMC fitting rather than pymc\nSupport single-phase only fitting\nMore control options for running ESPEI from the command line\nBetter support for incremental saving of the chain\nControl over output with logging over printing\nSignificant code cleanup\nBetter usage documentation",
    "crumbs": [
      "Developer",
      "Changelog"
    ]
  },
  {
    "objectID": "how-to/recipes/mcmc-parameter-traces/index.html",
    "href": "how-to/recipes/mcmc-parameter-traces/index.html",
    "title": "MCMC Parameter Trace Plots",
    "section": "",
    "text": "Looking at how each parameter chain evolves across the chains can show if any particular chains are diverging from the rest, if there are multiple modes being explored, or how wide the distribution of parameters are relative to each other.\nEach line corresponds to one chain in the ensemble.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom espei.analysis import truncate_arrays\n\ntrace = np.load('trace.npy')\nlnprob = np.load('lnprob.npy')\ntrace, lnprob = truncate_arrays(trace, lnprob)\n\nnum_chains = trace.shape[0]\nnum_parameters = trace.shape[2]\niterations = np.arange(1,trace.shape[1]+1)\nfig, axs = plt.subplots(nrows=num_parameters, gridspec_kw=dict(hspace=0.4), figsize=(4, 3*num_parameters))\nfor parameter in range(num_parameters):\n    ax = axs[parameter]\n    ax.set_xlabel('Iterations')\n    ax.set_ylabel('Parameter value')\n    ax.plot(iterations, trace[..., parameter].T)\nfig.show()\n\n\n\n\n\n\n\n\nand if we plot again after 500 iterations of burn-in:\n\nburn_in_iterations = 500\nfig, axs = plt.subplots(nrows=num_parameters, gridspec_kw=dict(hspace=0.4), figsize=(4, 3*num_parameters))\nfor parameter in range(num_parameters):\n    ax = axs[parameter]\n    ax.set_xlabel('Iterations')\n    ax.set_ylabel('Parameter value')\n    ax.plot(iterations[burn_in_iterations:], trace[:, burn_in_iterations:, parameter].T)\nfig.show()\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "How-To",
      "Recipes",
      "MCMC Parameter Trace Plots"
    ]
  },
  {
    "objectID": "how-to/recipes/plot-thermochemical-data/index.html",
    "href": "how-to/recipes/plot-thermochemical-data/index.html",
    "title": "Plot Thermochemical Data",
    "section": "",
    "text": "Parameter selection in ESPEI fits Calphad parameters to thermochemical data. MCMC can adjust these parameters. In both cases, it may be useful to compare the energies of specific interactions to the model predictions. The espei.plot.plot_interaction code will plot the predicted interaction from the database against the available data, if any.\n\nPHASE_NAME = 'LIQUID'\n# CONFIGURATION must be a tuple of the configuration to be plotted.\n# This can only plot one endmember or interaction at a time.\n# Note that the outside tuples are the whole configuration\n# and the insides are for each individual sublattice.\n# Single sublattices *MUST* have the comma after the\n# object in order to be a tuple, not just parantheses.\n# some examples:\n# (\"CU\", \"MG\")  # endmember\n# ((\"CU\", \"MG\"),)  # ((\"CU\", \"MG\")) is invalid because it will become (\"CU\", \"MG\")\n# (\"MG\", (\"CU\", \"MG\"))\nCONFIGURATION = ((\"CU\", \"MG\"),)\n\n# Plot the parameter\nimport matplotlib.pyplot as plt\nfrom pycalphad import Database\nfrom espei.datasets import load_datasets, recursive_glob\nfrom espei.plot import plot_interaction\n\ndbf = Database(\"Cu-Mg-generated.tdb\")\ncomps = sorted(dbf.elements)\ndatasets = load_datasets(recursive_glob(\"input-data\"))\nplot_interaction(dbf, comps, PHASE_NAME, CONFIGURATION, \"HM_MIX\", datasets=datasets)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "How-To",
      "Recipes",
      "Plot Thermochemical Data"
    ]
  },
  {
    "objectID": "how-to/recipes/plot-phase-diagram-data/index.html",
    "href": "how-to/recipes/plot-phase-diagram-data/index.html",
    "title": "Plot phase diagram data",
    "section": "",
    "text": "When compiling ESPEI datasets of phase equilibria data, it can be useful to plot the data to check that it matches visually with what you are expecting. This script plots data from a binary phase diagram.\n\nimport matplotlib.pyplot as plt\nfrom pycalphad import variables as v\nfrom espei.datasets import recursive_glob, load_datasets\nfrom espei.plot import dataplot\n\n# load datasets from directory\nds = load_datasets(recursive_glob(\"input-data\"))\n\ncomponents = [\"CU\", \"MG\", \"VA\"]\nindependent_component = components[1]\nphases = [\"BCC_A2\", \"CUMG2\", \"FCC_A1\", \"LAVES_C15\", \"LIQUID\"]\n# Conditions don't need to have valid values.\n# Only the keys matter because they are used to search for relevant data.\nconds = {v.P: 101325, v.T: (1,1,1), v.X(independent_component): (1, 1, 1)}\n\nfig, ax = plt.subplots()\ndataplot(components, phases, conds, ds, ax=ax)\nax.set_xlim(0, 1)\nax.set_xlabel(f\"X({independent_component.capitalize()})\")\nax.set_ylabel(f\"Temperature (K)\")\nax.set_title(\"Cu-Mg Data\")\nfig.show()",
    "crumbs": [
      "How-To",
      "Recipes",
      "Plot phase diagram data"
    ]
  },
  {
    "objectID": "how-to/recipes/plot-phase-diagram-data/index.html#plot-data-only",
    "href": "how-to/recipes/plot-phase-diagram-data/index.html#plot-data-only",
    "title": "Plot phase diagram data",
    "section": "",
    "text": "When compiling ESPEI datasets of phase equilibria data, it can be useful to plot the data to check that it matches visually with what you are expecting. This script plots data from a binary phase diagram.\n\nimport matplotlib.pyplot as plt\nfrom pycalphad import variables as v\nfrom espei.datasets import recursive_glob, load_datasets\nfrom espei.plot import dataplot\n\n# load datasets from directory\nds = load_datasets(recursive_glob(\"input-data\"))\n\ncomponents = [\"CU\", \"MG\", \"VA\"]\nindependent_component = components[1]\nphases = [\"BCC_A2\", \"CUMG2\", \"FCC_A1\", \"LAVES_C15\", \"LIQUID\"]\n# Conditions don't need to have valid values.\n# Only the keys matter because they are used to search for relevant data.\nconds = {v.P: 101325, v.T: (1,1,1), v.X(independent_component): (1, 1, 1)}\n\nfig, ax = plt.subplots()\ndataplot(components, phases, conds, ds, ax=ax)\nax.set_xlim(0, 1)\nax.set_xlabel(f\"X({independent_component.capitalize()})\")\nax.set_ylabel(f\"Temperature (K)\")\nax.set_title(\"Cu-Mg Data\")\nfig.show()",
    "crumbs": [
      "How-To",
      "Recipes",
      "Plot phase diagram data"
    ]
  },
  {
    "objectID": "how-to/recipes/plot-phase-diagram-data/index.html#plot-phase-diagram-with-data",
    "href": "how-to/recipes/plot-phase-diagram-data/index.html#plot-phase-diagram-with-data",
    "title": "Plot phase diagram data",
    "section": "Plot phase diagram with data",
    "text": "Plot phase diagram with data\nThis example uses binplot from PyCalphad and dataplot from ESPEI to plot a phase diagram with the data used to fit it on the same axes.\n\nimport matplotlib.pyplot as plt\nfrom pycalphad import Database, binplot, variables as v\nfrom espei.datasets import load_datasets, recursive_glob\nfrom espei.plot import dataplot\n\n# load datasets from directory\ndatasets = load_datasets(recursive_glob(\"input-data\"))\n\n# set up the pycalphad phase diagram calculation\ndbf = Database(\"Cr-Ni_mcmc.tdb\")\ncomps = [\"CR\", \"NI\", \"VA\"]\nphases = list(dbf.phases.keys())\nconds = {v.P: 101325, v.T: (700, 2200, 10), v.X(\"CR\"): (0, 1, 0.01)}\n\n# plot the phase diagram and data\nfig, ax = plt.subplots()\nbinplot(dbf, comps, phases, conds, plot_kwargs=dict(ax=ax))\ndataplot(comps, phases, conds, datasets, ax=ax)\nax.set_title(\"Cr-Ni MCMC\")\nax.set_ylim(conds[v.T][0], conds[v.T][1])\nax.set_xlabel(\"X(Cr)\")\nax.set_ylabel(\"Temperature (K)\")\nfig.show()",
    "crumbs": [
      "How-To",
      "Recipes",
      "Plot phase diagram data"
    ]
  },
  {
    "objectID": "how-to/recipes/mcmc-corner-plots/index.html",
    "href": "how-to/recipes/mcmc-corner-plots/index.html",
    "title": "MCMC Corner Plots",
    "section": "",
    "text": "In a corner plot, the distributions for each parameter are plotted along the diagonal and covariances between them under the diagonal. A more circular covariance means that parameters are not correlated to each other, while elongated shapes indicate that the two parameters are correlated.\nStrongly correlated parameters are expected for some parameters in Calphad models within phases or for phases in equilibrium, because increasing one parameter while decreasing another would give a similar likelihood.\nIf you started an MCMC run from scratch, you almost certainly want to discard a number of initial parameters to account for burn-in. The figure produced is relatively small figsize=(8,8) so that it can fit more easily on a page. Larger figures without passing the fig argument to corner.corner will be more legible (try not passing fig to corner.corner).\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport corner\n\ntrace = np.load('trace.npy')\n\n# the following lines are optional, but useful if your traces are not full\n# (i.e. your MCMC runs didn't run all their steps)\n# from espei.analysis import truncate_arrays\n# lnprob = np.load('lnprob.npy')\n# trace, lnprob = truncate_arrays(trace, lnprob)\n\nburn_in_iterations = 500  # number of samples of burn-in to discard\nprint(\"(# chains, # iterations, # parameters)\")\nprint(f\"Trace shape: {trace.shape}\")\nprint(f\"Trace shape after burn-in: {trace[:, burn_in_iterations:, :].shape}\")\n\n# flatten the along the first dimension containing all the chains in parallel\nfig = plt.figure(figsize=(8,8))  # figures for corner cannot have axes\ncorner.corner(trace[:, burn_in_iterations:, :].reshape(-1, trace.shape[-1]), fig=fig)\nfig.show()\n\n(# chains, # iterations, # parameters)\nTrace shape: (40, 1000, 10)\nTrace shape after burn-in: (40, 500, 10)\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "How-To",
      "Recipes",
      "MCMC Corner Plots"
    ]
  },
  {
    "objectID": "how-to/custom_unary_data.html",
    "href": "how-to/custom_unary_data.html",
    "title": "Custom unary reference states",
    "section": "",
    "text": "Unary reference state data provides three pieces of information about pure elements to ESPEI:\nBy default, ESPEI uses the SGTE91 reference state using the functions defined in (Dinsdale 1991).\nESPEI can be provided custom unary reference states for parameter generation. Some common use cases for custom reference states are:\nUsing a custom reference state is very easy once it is created or installed. Instead of generating parameters with:\nA custom reference state called MyCustomReferenceState could be used by:\nUsing the template package below, you can create a Python package of custom reference data to install for yourself, and optionally to distribute to others.",
    "crumbs": [
      "How-To",
      "Custom unary reference states"
    ]
  },
  {
    "objectID": "how-to/custom_unary_data.html#quickstart-skeleton-package",
    "href": "how-to/custom_unary_data.html#quickstart-skeleton-package",
    "title": "Custom unary reference states",
    "section": "Quickstart: Skeleton package",
    "text": "Quickstart: Skeleton package\nIf you are not comfortable developing a Python package using the details below, that's okay! We have provided a skeleton package that can be downloaded and installed to give you a working example.\n\nRunning the example\nFollowing these steps will give you a working unary reference state for Al and Ag named CustomRefstate2020. Assuming you have git and a working Python environment, starting from the command line:\n\nClone the skeleton repository: git clone https://github.com/PhasesResearchLab/ESPEI-unary-refstate-skeleton\nEnter the downloaded repository: cd ESPEI-unary-refstate-skeleton\nInstall the package using pip: pip install -e .\n\nThis will install the packaged, named espei_refstate_customrefstate2020, and provide a reference state named CustomRefstate2020.\nWe can use that by passing using ref_state: CustomRefstate2020 in the generate_parameters heading in ESPEI’s YAML input. If you have ESPEI installed already, you can test that this works by:\n\nEnter the espei-example directory: cd espei-example\nRun the YAML input file using ESPEI (note: it's safe to ignore a warning that no datsets were found - we aren't fitting any parameters to data here): espei --in gen_Ag-Al.yaml\n\nIf it was successful, you should have ran the YAML file:\nsystem:\n  phase_models: Ag-Al.json\n  datasets: input-datasets\ngenerate_parameters:\n  excess_model: linear\n  ref_state: CustomRefstate2020\nand generated a database, out.tdb, containing our custom GHSERAG function (among others):\nFUNCTION GHSERAG 298.15 118.202013*T - 7209.512; 1234.93 Y 190.266404*T -\n   15095.252; 3000.0 N !\nand lattice stabilities for phases defined in the reference state and the system Ag-Al.json, such as GHCPAG.\nFinally, since this reference state is probably not useful for developing any databases, uninstall the package by running pip uninstall espei_refstate_customrefstate2020 and removing the directory espei_refstate_customrefstate2020.egg-info from the root directory if one exists.\n\n\nUsing the skeleton to create your own database\nIf you want to use the skeleton to create your own reference state to provide ESPEI, you can follow the steps below. To keep the steps concrete, we'll create a reference state for Cu called Bocklund2019 following the unary description published for Cu in (Bocklund et al. 2019). within the segmented regression approach by [Roslyakova2016](Roslyakova et al. 2016).\nAssuming that you are fresh (without the skeleton downloaded yet):\n\nClone the skeleton repository: git clone https://github.com/PhasesResearchLab/ESPEI-unary-refstate-skeleton\nEnter the downloaded repository: cd ESPEI-unary-refstate-skeleton\nUpdate the NAME = 'CustomRefstate2020' parameter in setup.py to NAME = 'Bocklund2019'\nIn the refstate.py module, create the Bockund2019Stable, Bockund2019, and (optionally) Bocklund2019SER dictionaries (see creating refstate dicts for more details)\n\nDelete the CustomRefstate2020Stable and CustomRefstate2020 variables\nAdd the stable phase Gibbs energy for Cu to the Bockund2019Stable variable. Note that OrderedDict is defined in the collections module in the Python standard library.\nBocklund2019Stable = OrderedDict([\n    ('CU',\n    Piecewise((-0.0010514335*T**2 + 8.7685671186*T*log(exp(155.1404/T) - 1.0) + 16.1968683846*T*log(exp(290.9421/T) - 1.0) - 11038.0904080745, And(T &gt;= 0.01, T &lt; 103.57591)), (-2.15621953171362e-6*T**3 + 0.000288560900942072*T**2 - 0.13879113947248*T*log(T) + 8.7685671186*T*log(exp(155.1404/T) - 1.0) + 16.1968683846*T*log(exp(290.9421/T) - 1.0) + 0.574637617323048*T - 11042.8822142647, And(T &gt;= 103.57591, T &lt; 210.33309)), (-0.002432585*T**2 + 0.4335558862135*T*log(T) + 8.7685671186*T*log(exp(155.1404/T) - 1.0) + 16.1968683846*T*log(exp(290.9421/T) - 1.0) - 2.20049706600083*T - 11002.7543747764, And(T &gt;= 210.33309, T &lt; 1357.77)), (-31.38*T*log(T) + 183.555483717662*T - 12730.2995781851 + 7.42232714807953e+28/T**9, And(T &gt;= 1357.77, T &lt; 3200.0)), (0, True))),\n])\nAdd the lattice stability for all elements, including the stable phase, to the Bocklund2019 variable\nBocklund2019 = OrderedDict([\n    (('CU', 'HCP_A3'), Piecewise((-3.38438862938597e-7*T**3 - 0.00121182291077191*T**2 + 8.7685671186*T*log(exp(155.1404/T) - 1.0) + 16.1968683846*T*log(exp(290.9421/T) - 1.0) - 0.321147237334052*T - 10441.4393392344, And(T &gt;= 0.01, T &lt; 298.15)), (1.29223e-7*T**3 - 0.00265684*T**2 - 24.112392*T*log(T) + 130.685235*T - 7170.458 + 52478/T, And(T &gt;= 298.15, T &lt; 1357.77)), (-31.38*T*log(T) + 184.003828*T - 12942.0252504739 + 3.64167e+29/T**9, And(T &gt;= 1357.77, T &lt; 3200.0)), (0, True))),\n    (('CU', 'FCC_A1'), Piecewise((Symbol('GHSERCU'), And(T &lt; 10000.0, T &gt;= 1.0)), (0, True))),\n    (('CU', 'LIQUID'), Piecewise((-3.40056501515466e-7*T**3 - 0.00121066539331185*T**2 + 8.7685671186*T*log(exp(155.1404/T) - 1.0) + 16.1968683846*T*log(exp(290.9421/T) - 1.0) - 10.033338832193*T + 2379.36422209194, And(T &gt;= 0.01, T &lt; 298.15)), (-5.8489e-21*T**7 + 1.29223e-7*T**3 - 0.00265684*T**2 - 24.112392*T*log(T) + 120.973331*T + 5650.32106235287 + 52478/T, And(T &gt;= 298.15, T &lt; 1357.77)), (-31.38*T*log(T) + 173.881484*T + 409.498458129716, And(T &gt;= 1357.77, T &lt; 3200.0)), (0, True))),\n])\n(Optional) Add the SER data. If you don't add this data, the SGTE91 data will be used as a fallback\nBocklund2019SER = OrderedDict([\n   ('CU', {'phase': 'FCC_A1', 'mass': 63.546, 'H298': 5004.1, 'S298': 33.15}),\n])\n\nInstall the package as editable using pip: pip install -e .\nYou can now use your reference state in ESPEI, and even change the definitions on the fly.",
    "crumbs": [
      "How-To",
      "Custom unary reference states"
    ]
  },
  {
    "objectID": "how-to/custom_unary_data.html#creating-the-reference-state-dictionaries",
    "href": "how-to/custom_unary_data.html#creating-the-reference-state-dictionaries",
    "title": "Custom unary reference states",
    "section": "Creating the reference state dictionaries",
    "text": "Creating the reference state dictionaries\nTo define the reference Gibbs energy, lattice stabilities, and SER data you must define three ordered dictionaries:\n\n&lt;NAME&gt;SER, mapping element names to dictionaries of SER data\n&lt;NAME&gt;Stable, mapping element names to Gibbs energy expressions for the stable phase\n&lt;NAME&gt;, mapping pairs of (element name, phase name) to lattice stability expressions\n\nThe Gibbs energy expressions must be defined as valid symbolic expressions using SymPy Symbol objects and pycalphad StateVariable objects (e.g. v.T, v.P), which can be (but are not required to be) piecewise in temperature. Any SymPy functions can be used (exp, log, Piecewise, ...). Any valid Python syntax or functions can be used, including those not available in commercial software (for example, direct exponentiation with non-integer powers). Any expression supported by pycalphad Model objects can be used, but note that the TDB files that ESPEI writes using these expressions may not be compatible with commercial software.\nIt's important to note that the users probably want to add a (0, True) expression/condition pair to the end of any Piecewise expressions used. Since pycalphad does not automatically extrapolate the piecewise expressions outside of thier valid ranges, this condition will allow the solver to be numerically stable, returning zero instead of NaN.\nFor &lt;NAME&gt; lattice stability data, all GHSER symbols will have a two letter element name, regardless of how many letters the element name has. This is to prevent abbreviation name clashes in commercial software. For example, GHSERC could represent the Gibbs energy for carbon (C), but also be a valid abbreviation for calcium (CA). Using GHSERCC automatically fixes this issue, but be aware to use Symbol(\"GHSERCC\") in the case of single letter phase names.",
    "crumbs": [
      "How-To",
      "Custom unary reference states"
    ]
  },
  {
    "objectID": "how-to/custom_unary_data.html#detailed-information",
    "href": "how-to/custom_unary_data.html#detailed-information",
    "title": "Custom unary reference states",
    "section": "Detailed Information",
    "text": "Detailed Information\n\nSetting up setup.py\nIf you want to go dig deeper into how the skeleton works, ESPEI uses the entry_points feature of setuptools to treat additional reference states as plugins.\nA package providing a reference state to ESPEI should provide a module that has three dictionaries: &lt;NAME&gt;Stable, &lt;NAME&gt;, and (optional) &lt;NAME&gt;SER, according to the creating refstate dicts section above. The module can have any name, &lt;MODULE&gt;, (the skeleton uses refstate.py). ESPEI looks for the entry_point called espei.reference_states following the example from the setuptools documentation. Concretely, the entry_point should be described by:\n# setup.py\n\nfrom setuptools import setup\n\nsetup(# ...\n    entry_points={'espei.reference_states': '&lt;NAME&gt; = &lt;MODULE&gt;'}\n)\nwhere &lt;NAME&gt; and &lt;MODULE&gt; are replaced by the corresponding name of the reference state and the name of the module with the reference states defined.\nInterested readers may also find the entry_points specification here.\n\n\nDebugging\nIf you want to test whether your modules are found, you can run the following Python code to show what reference states were found\nimport espei\nprint(espei.refdata.INSERTED_USER_REFERENCE_STATES)\nIf you do this after installing the unchanged skeleton package package from this repository, you should find CustomRefstate2020 is printed and the dictionaries espei.refdata.CustomRefstate2020Stable and espei.refdata.CustomRefstate2020 should be defined in the espei.refdata module. For more details on the implementation, see the espei.refdata.find_and_insert_user_refstate function.",
    "crumbs": [
      "How-To",
      "Custom unary reference states"
    ]
  },
  {
    "objectID": "reference/phase_models_schema.html",
    "href": "reference/phase_models_schema.html",
    "title": "Phase Models Schema",
    "section": "",
    "text": "Formatting JSON Files for ESPEI\n\n\n\n\n\nESPEI has a single input style in JSON format that is used for all data entry. For those unfamiliar with JSON, it is fairly similar to Python dictionaries with some rigid requirements\n\n\nAll string quotes must be double quotes. Use \"key\" instead of 'key'.\nNumbers should not have leading zeros. 00.123 should be 0.123 and 012.34 must be 12.34.\nLists and nested key-value pairs cannot have trailing commas. {\"nums\": [1,2,3,],} is invalid and should be {\"nums\": [1,2,3]}.\n\n\nThese errors can be challenging to track down, particularly if you are only reading the JSON error messages in Python. A visual editor is encouraged for debugging JSON files such as JSONLint. A quick reference to the format can be found at Learn JSON in Y minutes.\nESPEI has support for checking all of your input datasets for errors, which you should always use before you attempt to run ESPEI. This error checking will report all of the errors at once and all errors should be fixed. Errors in the datasets will prevent fitting. To check the datasets at path my-input-data/ you can run espei --check-datasets my-input-data.\n\n\n\nThe JSON file for describing Calphad phases is conceptually similar to a setup file in Thermo-Calc's PARROT module. At the top of the file there is the refdata key that describes which reference state you would like to choose. Currently the reference states are strings referring to dictionaries in pycalphad.refdata only \"SGTE91\" is implemented.\nEach phase is described with the phase name as they key in the dictionary of phases. The details of that phase is a dictionary of values for that key. There are 4 possible entries to describe a phase: sublattice_model, sublattice_site_ratios, equivalent_sublattices, and aliases. sublattice_model is a list of lists, where each internal list contains all of the components in that sublattice. The BCC_B2 sublattice model is [[\"AL\", \"NI\", \"VA\"], [\"AL\", \"NI\", \"VA\"], [\"VA\"]], thus there are three sublattices where the first two have Al, Ni, and vacancies. sublattice_site_ratios should be of the same length as the sublattice model (e.g. 3 for BCC_B2). The sublattice site ratios can be fractional or integers and do not have to sum to unity.\nThe optional equivalent_sublattices key is a list of lists that describe which sublattices are symmetrically equivalent. Each sub-list in equivalent_sublattices describes the indices (zero-indexed) of sublattices that are equivalent. For BCC_B2 the equivalent sublattices are [[0, 1]], meaning that the sublattice at index 0 and index 1 are equivalent. There can be multiple different sets (multiple sub-lists) of equivalent sublattices and there can be many equivalent sublattices within a sublattice (see FCC_L12). If no equivalent_sublattice key exists, it is assumed that there are none.a\nFinally, the aliases key is used to refer to other phases that this sublattice model can describe when symmetry is accounted for. Aliases are used here to describe the BCC_A2 and FCC_A1, which are the disordered phases of BCC_B2 and FCC_L12, respectively. Notice that the aliased phases are not otherwise described in the input file. Multiple phases can exist with aliases to the same phase, e.g. FCC_L12 and FCC_L10 can both have FCC_A1 as an alias.\n{\n  \"refdata\": \"SGTE91\",\n  \"components\": [\"AL\", \"NI\", \"VA\"],\n  \"phases\": {\n      \"LIQUID\" : {\n      \"sublattice_model\": [[\"AL\", \"NI\"]],\n      \"sublattice_site_ratios\": [1]\n      },\n      \"BCC_B2\": {\n      \"aliases\": [\"BCC_A2\"],\n      \"sublattice_model\": [[\"AL\", \"NI\", \"VA\"], [\"AL\", \"NI\", \"VA\"], [\"VA\"]],\n      \"sublattice_site_ratios\": [0.5, 0.5, 1],\n      \"equivalent_sublattices\": [[0, 1]]\n      },\n      \"FCC_L12\": {\n      \"aliases\": [\"FCC_A1\"],\n      \"sublattice_model\": [[\"AL\", \"NI\"], [\"AL\", \"NI\"], [\"AL\", \"NI\"], [\"AL\", \"NI\"], [\"VA\"]],\n      \"sublattice_site_ratios\": [0.25, 0.25, 0.25, 0.25, 1],\n      \"equivalent_sublattices\": [[0, 1, 2, 3]]\n      },\n      \"AL3NI1\": {\n      \"sublattice_site_ratios\": [0.75, 0.25],\n      \"sublattice_model\": [[\"AL\"], [\"NI\"]]\n      },\n      \"AL3NI2\": {\n      \"sublattice_site_ratios\": [3, 2, 1],\n      \"sublattice_model\": [[\"AL\"], [\"AL\", \"NI\"], [\"NI\", \"VA\"]]\n      },\n      \"AL3NI5\": {\n      \"sublattice_site_ratios\": [0.375, 0.625],\n      \"sublattice_model\": [[\"AL\"], [\"NI\"]]\n      }\n    }\n}\n\n\n\n Back to top",
    "crumbs": [
      "Reference",
      "Phase Models Schema"
    ]
  },
  {
    "objectID": "reference/api/datasets.recursive_glob.html",
    "href": "reference/api/datasets.recursive_glob.html",
    "title": "datasets.recursive_glob",
    "section": "",
    "text": "datasets.recursive_glob(start, pattern='*.json')\nRecursively glob for the given pattern from the start directory.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart\nstr\nPath of the directory to walk while for file globbing\nrequired\n\n\npattern\nstr\nFilename pattern to match in the glob.\n'*.json'\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\n[str]\nList of matched filenames",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.datasets",
      "datasets.recursive_glob"
    ]
  },
  {
    "objectID": "reference/api/datasets.recursive_glob.html#parameters",
    "href": "reference/api/datasets.recursive_glob.html#parameters",
    "title": "datasets.recursive_glob",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nstart\nstr\nPath of the directory to walk while for file globbing\nrequired\n\n\npattern\nstr\nFilename pattern to match in the glob.\n'*.json'",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.datasets",
      "datasets.recursive_glob"
    ]
  },
  {
    "objectID": "reference/api/datasets.recursive_glob.html#returns",
    "href": "reference/api/datasets.recursive_glob.html#returns",
    "title": "datasets.recursive_glob",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\n[str]\nList of matched filenames",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.datasets",
      "datasets.recursive_glob"
    ]
  },
  {
    "objectID": "reference/api/index.html",
    "href": "reference/api/index.html",
    "title": "API Reference",
    "section": "",
    "text": "generate_parameters\nGenerate parameters from given phase models and datasets\n\n\nrun_espei\nWrapper around the ESPEI fitting procedure, taking only a settings dictionary.\n\n\n\n\n\n\n\n\n\ndatasets.load_datasets\nCreate a PickelableTinyDB with the data from a list of filenames.\n\n\ndatasets.recursive_glob\nRecursively glob for the given pattern from the start directory.\n\n\n\n\n\n\n\n\n\nanalysis.truncate_arrays\nReturn slides of ESPEI output arrays with any empty remaining iterations (zeros) removed.\n\n\n\n\n\n\nResidual functions\n\n\n\nerror_functions.residual_base.residual_function_registry\n\n\n\nerror_functions.residual_base.ResidualFunction\nProtocol class for computing the error (residual) between data and a model\n\n\nerror_functions.activity_error.ActivityResidual\n\n\n\nerror_functions.zpf_error.ZPFResidual\n\n\n\nerror_functions.equilibrium_thermochemical_error.EquilibriumPropertyResidual\n\n\n\nerror_functions.non_equilibrium_thermochemical_error.FixedConfigurationPropertyResidual\n\n\n\n\n\n\n\nFitting descriptions for custom model parameter generation\n\n\n\nparameter_selection.fitting_descriptions.ModelFittingDescription\n\n\n\n\n\n\n\nFitting steps for custom model parameter generation\n\n\n\nparameter_selection.fitting_steps\n\n\n\n\n\n\n\n\n\n\nutils.database_symbols_to_fit\nReturn names of the symbols to fit that match the regular expression\n\n\nutils.optimal_parameters\nReturn the optimal parameters in the trace based on the highest likelihood.\n\n\n\n\n\n\n\n\n\nplot.dataplot\nPlot datapoints corresponding to the components, phases, and conditions.\n\n\nplot.plot_interaction\nReturn one set of plotted Axes with data compared to calculated parameters\n\n\nplot.plot_endmember\nReturn one set of plotted Axes with data compared to calculated parameters",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api/index.html#espei",
    "href": "reference/api/index.html#espei",
    "title": "API Reference",
    "section": "",
    "text": "generate_parameters\nGenerate parameters from given phase models and datasets\n\n\nrun_espei\nWrapper around the ESPEI fitting procedure, taking only a settings dictionary.",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api/index.html#espei.datasets",
    "href": "reference/api/index.html#espei.datasets",
    "title": "API Reference",
    "section": "",
    "text": "datasets.load_datasets\nCreate a PickelableTinyDB with the data from a list of filenames.\n\n\ndatasets.recursive_glob\nRecursively glob for the given pattern from the start directory.",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api/index.html#espei.analsys",
    "href": "reference/api/index.html#espei.analsys",
    "title": "API Reference",
    "section": "",
    "text": "analysis.truncate_arrays\nReturn slides of ESPEI output arrays with any empty remaining iterations (zeros) removed.",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api/index.html#espei.error_functions",
    "href": "reference/api/index.html#espei.error_functions",
    "title": "API Reference",
    "section": "",
    "text": "Residual functions\n\n\n\nerror_functions.residual_base.residual_function_registry\n\n\n\nerror_functions.residual_base.ResidualFunction\nProtocol class for computing the error (residual) between data and a model\n\n\nerror_functions.activity_error.ActivityResidual\n\n\n\nerror_functions.zpf_error.ZPFResidual\n\n\n\nerror_functions.equilibrium_thermochemical_error.EquilibriumPropertyResidual\n\n\n\nerror_functions.non_equilibrium_thermochemical_error.FixedConfigurationPropertyResidual",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api/index.html#espei.parameter_selection.fitting_descriptions",
    "href": "reference/api/index.html#espei.parameter_selection.fitting_descriptions",
    "title": "API Reference",
    "section": "",
    "text": "Fitting descriptions for custom model parameter generation\n\n\n\nparameter_selection.fitting_descriptions.ModelFittingDescription",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api/index.html#espei.parameter_selection.fitting_steps",
    "href": "reference/api/index.html#espei.parameter_selection.fitting_steps",
    "title": "API Reference",
    "section": "",
    "text": "Fitting steps for custom model parameter generation\n\n\n\nparameter_selection.fitting_steps",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api/index.html#espei.utils",
    "href": "reference/api/index.html#espei.utils",
    "title": "API Reference",
    "section": "",
    "text": "utils.database_symbols_to_fit\nReturn names of the symbols to fit that match the regular expression\n\n\nutils.optimal_parameters\nReturn the optimal parameters in the trace based on the highest likelihood.",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api/index.html#espei.plot",
    "href": "reference/api/index.html#espei.plot",
    "title": "API Reference",
    "section": "",
    "text": "plot.dataplot\nPlot datapoints corresponding to the components, phases, and conditions.\n\n\nplot.plot_interaction\nReturn one set of plotted Axes with data compared to calculated parameters\n\n\nplot.plot_endmember\nReturn one set of plotted Axes with data compared to calculated parameters",
    "crumbs": [
      "Reference",
      "API Reference"
    ]
  },
  {
    "objectID": "reference/api/error_functions.residual_base.residual_function_registry.html",
    "href": "reference/api/error_functions.residual_base.residual_function_registry.html",
    "title": "error_functions.residual_base.residual_function_registry",
    "section": "",
    "text": "error_functions.residual_base.residual_function_registry\nerror_functions.residual_base.residual_function_registry\n\n\n\n\n Back to top",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.error_functions",
      "error_functions.residual_base.residual_function_registry"
    ]
  },
  {
    "objectID": "reference/api/utils.database_symbols_to_fit.html",
    "href": "reference/api/utils.database_symbols_to_fit.html",
    "title": "utils.database_symbols_to_fit",
    "section": "",
    "text": "utils.database_symbols_to_fit(dbf, symbol_regex='^V[V]?([0-9]+)$')\nReturn names of the symbols to fit that match the regular expression\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndbf\nDatabase\npycalphad Database\nrequired\n\n\nsymbol_regex\nstr\nRegular expression of the fitting symbols. Defaults to V or VV followed by one or more numbers.\n'^V[V]?([0-9]+)$'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict\nContext dictionary for different methods of calculation the error.",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.utils",
      "utils.database_symbols_to_fit"
    ]
  },
  {
    "objectID": "reference/api/utils.database_symbols_to_fit.html#parameters",
    "href": "reference/api/utils.database_symbols_to_fit.html#parameters",
    "title": "utils.database_symbols_to_fit",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndbf\nDatabase\npycalphad Database\nrequired\n\n\nsymbol_regex\nstr\nRegular expression of the fitting symbols. Defaults to V or VV followed by one or more numbers.\n'^V[V]?([0-9]+)$'",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.utils",
      "utils.database_symbols_to_fit"
    ]
  },
  {
    "objectID": "reference/api/utils.database_symbols_to_fit.html#returns",
    "href": "reference/api/utils.database_symbols_to_fit.html#returns",
    "title": "utils.database_symbols_to_fit",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\ndict\nContext dictionary for different methods of calculation the error.",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.utils",
      "utils.database_symbols_to_fit"
    ]
  },
  {
    "objectID": "reference/api/error_functions.residual_base.ResidualFunction.html",
    "href": "reference/api/error_functions.residual_base.ResidualFunction.html",
    "title": "error_functions.residual_base.ResidualFunction",
    "section": "",
    "text": "error_functions.residual_base.ResidualFunction(self, database, datasets, phase_models, symbols_to_fit=None, weight=None)\nProtocol class for computing the error (residual) between data and a model prediction given a set of parameters.\nClasses that implement this protocol typically will concerned with implementing a residual/likelihood function for a certain type of data. The protocol is intentitionally left to be simple to implement while enabling flexibility to implement additional methods and attributes for performance and debugging purposes.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndatabase\nDatabase\n\nrequired\n\n\ndatasets\nPickleableTinyDB\nThe candidate datasets for the a contribution. Usually the datasets are a superset (in components, phases, data types, etc.) of the actual data used to compute the residual for any particular contribution.\nrequired\n\n\nphase_models\nPhaseModels\nDefines the active set of components that should be fit and any user-provided overrides to the PyCalphad Model class for each phase.\nrequired\n\n\nsymbols_to_fit\nOptional[List[SymbolName]]\nUser-provided symbols to fit. By default, the symbols to fit should be set by espei.utils.database_symbols_to_fit.\nNone\n\n\nweight\nOptional[float]\nWhen computing the likelihood, this should be used to modify the probability distribution. Higher weights should correspond to narrower probability distributions, but it’s exact use will depend on the particular probability distribution.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nweight\nOptional[Union[float, Dict[str, float]]]\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_likelihood\nReturn log-likelihood for the set of parameters.\n\n\nget_residuals\nReturn the residual comparing the selected data to the set of parameters.\n\n\n\n\n\nerror_functions.residual_base.ResidualFunction.get_likelihood(parameters)\nReturn log-likelihood for the set of parameters.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameters\nArrayLike\n1D parameter vector. The size of the parameters array should match the number of fitting symbols used to build the models. This is not checked.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nValue of log-likelihood for the given set of parameters\n\n\n\n\n\n\n\nerror_functions.residual_base.ResidualFunction.get_residuals(parameters)\nReturn the residual comparing the selected data to the set of parameters.\nThe residual is zero if the database predictions under the given parameters agrees with the data exactly.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameters\nArrayLike\n1D parameter vector. The size of the parameters array should match the number of fitting symbols used to build the models. This is not checked.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTuple[List[float], List[float]]\nTuple of (residuals, weights), which must obey len(residuals) == len(weights)",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.error_functions",
      "error_functions.residual_base.ResidualFunction"
    ]
  },
  {
    "objectID": "reference/api/error_functions.residual_base.ResidualFunction.html#parameters",
    "href": "reference/api/error_functions.residual_base.ResidualFunction.html#parameters",
    "title": "error_functions.residual_base.ResidualFunction",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndatabase\nDatabase\n\nrequired\n\n\ndatasets\nPickleableTinyDB\nThe candidate datasets for the a contribution. Usually the datasets are a superset (in components, phases, data types, etc.) of the actual data used to compute the residual for any particular contribution.\nrequired\n\n\nphase_models\nPhaseModels\nDefines the active set of components that should be fit and any user-provided overrides to the PyCalphad Model class for each phase.\nrequired\n\n\nsymbols_to_fit\nOptional[List[SymbolName]]\nUser-provided symbols to fit. By default, the symbols to fit should be set by espei.utils.database_symbols_to_fit.\nNone\n\n\nweight\nOptional[float]\nWhen computing the likelihood, this should be used to modify the probability distribution. Higher weights should correspond to narrower probability distributions, but it’s exact use will depend on the particular probability distribution.\nNone",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.error_functions",
      "error_functions.residual_base.ResidualFunction"
    ]
  },
  {
    "objectID": "reference/api/error_functions.residual_base.ResidualFunction.html#attributes",
    "href": "reference/api/error_functions.residual_base.ResidualFunction.html#attributes",
    "title": "error_functions.residual_base.ResidualFunction",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nweight\nOptional[Union[float, Dict[str, float]]]",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.error_functions",
      "error_functions.residual_base.ResidualFunction"
    ]
  },
  {
    "objectID": "reference/api/error_functions.residual_base.ResidualFunction.html#methods",
    "href": "reference/api/error_functions.residual_base.ResidualFunction.html#methods",
    "title": "error_functions.residual_base.ResidualFunction",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_likelihood\nReturn log-likelihood for the set of parameters.\n\n\nget_residuals\nReturn the residual comparing the selected data to the set of parameters.\n\n\n\n\n\nerror_functions.residual_base.ResidualFunction.get_likelihood(parameters)\nReturn log-likelihood for the set of parameters.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameters\nArrayLike\n1D parameter vector. The size of the parameters array should match the number of fitting symbols used to build the models. This is not checked.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nValue of log-likelihood for the given set of parameters\n\n\n\n\n\n\n\nerror_functions.residual_base.ResidualFunction.get_residuals(parameters)\nReturn the residual comparing the selected data to the set of parameters.\nThe residual is zero if the database predictions under the given parameters agrees with the data exactly.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameters\nArrayLike\n1D parameter vector. The size of the parameters array should match the number of fitting symbols used to build the models. This is not checked.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTuple[List[float], List[float]]\nTuple of (residuals, weights), which must obey len(residuals) == len(weights)",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.error_functions",
      "error_functions.residual_base.ResidualFunction"
    ]
  },
  {
    "objectID": "reference/api/generate_parameters.html",
    "href": "reference/api/generate_parameters.html",
    "title": "generate_parameters",
    "section": "",
    "text": "generate_parameters(phase_models, datasets, ref_state, excess_model, ridge_alpha=None, aicc_penalty_factor=None, dbf=None, fitting_description=gibbs_energy_fitting_description)\nGenerate parameters from given phase models and datasets\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nphase_models\ndict\nDictionary of components and phases to fit.\nrequired\n\n\ndatasets\nPickleableTinyDB\ndatabase of single- and multi-phase to fit.\nrequired\n\n\nref_state\nstr\nString of the reference data to use, e.g. ‘SGTE91’ or ‘SR2016’\nrequired\n\n\nexcess_model\nstr\nString of the type of excess model to fit to, e.g. ‘linear’\nrequired\n\n\nridge_alpha\nfloat\nValue of the :math:\\alpha hyperparameter used in ridge regression. Defaults to None, which falls back to ordinary least squares regression. For now, the parameter is applied to all features.\nNone\n\n\naicc_penalty_factor\ndict\nMap of phase name to feature to a multiplication factor for the AICc’s parameter penalty.\nNone\n\n\ndbf\nDatabase\nInitial pycalphad Database that can have parameters that would not be fit by ESPEI\nNone\n\n\nfitting_description\nType[ModelFittingDescription]\nModelFittingDescription object describing the fitting steps and model\ngibbs_energy_fitting_description\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npycalphad.Database",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei",
      "generate_parameters"
    ]
  },
  {
    "objectID": "reference/api/generate_parameters.html#parameters",
    "href": "reference/api/generate_parameters.html#parameters",
    "title": "generate_parameters",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nphase_models\ndict\nDictionary of components and phases to fit.\nrequired\n\n\ndatasets\nPickleableTinyDB\ndatabase of single- and multi-phase to fit.\nrequired\n\n\nref_state\nstr\nString of the reference data to use, e.g. ‘SGTE91’ or ‘SR2016’\nrequired\n\n\nexcess_model\nstr\nString of the type of excess model to fit to, e.g. ‘linear’\nrequired\n\n\nridge_alpha\nfloat\nValue of the :math:\\alpha hyperparameter used in ridge regression. Defaults to None, which falls back to ordinary least squares regression. For now, the parameter is applied to all features.\nNone\n\n\naicc_penalty_factor\ndict\nMap of phase name to feature to a multiplication factor for the AICc’s parameter penalty.\nNone\n\n\ndbf\nDatabase\nInitial pycalphad Database that can have parameters that would not be fit by ESPEI\nNone\n\n\nfitting_description\nType[ModelFittingDescription]\nModelFittingDescription object describing the fitting steps and model\ngibbs_energy_fitting_description",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei",
      "generate_parameters"
    ]
  },
  {
    "objectID": "reference/api/generate_parameters.html#returns",
    "href": "reference/api/generate_parameters.html#returns",
    "title": "generate_parameters",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\npycalphad.Database",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei",
      "generate_parameters"
    ]
  },
  {
    "objectID": "reference/api/utils.optimal_parameters.html",
    "href": "reference/api/utils.optimal_parameters.html",
    "title": "utils.optimal_parameters",
    "section": "",
    "text": "utils.optimal_parameters(trace_array, lnprob_array, kth=0)\nReturn the optimal parameters in the trace based on the highest likelihood. If kth is specified, return the kth set of unique optimal parameters.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrace_array\nndarray\nArray of shape (number of chains, iterations, number of parameters)\nrequired\n\n\nlnprob_array\nndarray\nArray of shape (number of chains, iterations)\nrequired\n\n\nkth\nint\nZero-indexed optimum. 0 (the default) is the most optimal solution. 1 is the second most optimal, etc.. Only unique solutions will be returned.\n0\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nArray of optimal parameters\n\n\n\n\n\n\n\nIt is ok if the calculation did not finish and the arrays are padded with zeros. The number of chains and iterations in the trace and lnprob arrays must match.\n\n\n\n&gt;&gt;&gt; from espei.utils import optimal_parameters\n&gt;&gt;&gt; trace = np.array([[[1, 0], [2, 0], [3, 0], [0, 0]], [[0, 2], [0, 4], [0, 6], [0, 0]]])  # 3 iterations of 4 allocated\n&gt;&gt;&gt; lnprob = np.array([[-6, -4, -2, 0], [-3, -1, -2, 0]])\n&gt;&gt;&gt; bool(np.all(np.isclose(optimal_parameters(trace, lnprob), np.array([0, 4]))))\nTrue",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.utils",
      "utils.optimal_parameters"
    ]
  },
  {
    "objectID": "reference/api/utils.optimal_parameters.html#parameters",
    "href": "reference/api/utils.optimal_parameters.html#parameters",
    "title": "utils.optimal_parameters",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntrace_array\nndarray\nArray of shape (number of chains, iterations, number of parameters)\nrequired\n\n\nlnprob_array\nndarray\nArray of shape (number of chains, iterations)\nrequired\n\n\nkth\nint\nZero-indexed optimum. 0 (the default) is the most optimal solution. 1 is the second most optimal, etc.. Only unique solutions will be returned.\n0",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.utils",
      "utils.optimal_parameters"
    ]
  },
  {
    "objectID": "reference/api/utils.optimal_parameters.html#returns",
    "href": "reference/api/utils.optimal_parameters.html#returns",
    "title": "utils.optimal_parameters",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nArray of optimal parameters",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.utils",
      "utils.optimal_parameters"
    ]
  },
  {
    "objectID": "reference/api/utils.optimal_parameters.html#notes",
    "href": "reference/api/utils.optimal_parameters.html#notes",
    "title": "utils.optimal_parameters",
    "section": "",
    "text": "It is ok if the calculation did not finish and the arrays are padded with zeros. The number of chains and iterations in the trace and lnprob arrays must match.",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.utils",
      "utils.optimal_parameters"
    ]
  },
  {
    "objectID": "reference/api/utils.optimal_parameters.html#examples",
    "href": "reference/api/utils.optimal_parameters.html#examples",
    "title": "utils.optimal_parameters",
    "section": "",
    "text": "&gt;&gt;&gt; from espei.utils import optimal_parameters\n&gt;&gt;&gt; trace = np.array([[[1, 0], [2, 0], [3, 0], [0, 0]], [[0, 2], [0, 4], [0, 6], [0, 0]]])  # 3 iterations of 4 allocated\n&gt;&gt;&gt; lnprob = np.array([[-6, -4, -2, 0], [-3, -1, -2, 0]])\n&gt;&gt;&gt; bool(np.all(np.isclose(optimal_parameters(trace, lnprob), np.array([0, 4]))))\nTrue",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.utils",
      "utils.optimal_parameters"
    ]
  },
  {
    "objectID": "reference/api/error_functions.activity_error.ActivityResidual.html",
    "href": "reference/api/error_functions.activity_error.ActivityResidual.html",
    "title": "error_functions.activity_error.ActivityResidual",
    "section": "",
    "text": "error_functions.activity_error.ActivityResidual\nerror_functions.activity_error.ActivityResidual(self, database, datasets, phase_models, symbols_to_fit=None, weight=None)\n\n\n\n\n Back to top",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.error_functions",
      "error_functions.activity_error.ActivityResidual"
    ]
  },
  {
    "objectID": "reference/api/plot.plot_interaction.html",
    "href": "reference/api/plot.plot_interaction.html",
    "title": "plot.plot_interaction",
    "section": "",
    "text": "plot.plot_interaction(dbf, comps, phase_name, configuration, output, datasets=None, symmetry=None, ax=None, plot_kwargs=None, dataplot_kwargs=None)\nReturn one set of plotted Axes with data compared to calculated parameters\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndbf\nDatabase\npycalphad thermodynamic database containing the relevant parameters.\nrequired\n\n\ncomps\nSequence[str]\nNames of components to consider in the calculation.\nrequired\n\n\nphase_name\nstr\nName of the considered phase phase\nrequired\n\n\nconfiguration\nTuple[Tuple[str]]\nESPEI-style configuration\nrequired\n\n\noutput\nstr\nModel property to plot on the y-axis e.g. 'HM_MIX', or 'SM_MIX'. Must be a '_MIX' property.\nrequired\n\n\ndatasets\ntinydb.TinyDB\n\nNone\n\n\nsymmetry\nlist\nList of lists containing indices of symmetric sublattices e.g. [[0, 1], [2, 3]]\nNone\n\n\nax\nplt.Axes\nDefault axes used if not specified.\nNone\n\n\nplot_kwargs\nOptional[Dict[str, Any]]\nKeyword arguments to ax.plot for the predicted data.\nNone\n\n\ndataplot_kwargs\nOptional[Dict[str, Any]]\nKeyword arguments to ax.plot the observed data.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nplt.Axes",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.plot",
      "plot.plot_interaction"
    ]
  },
  {
    "objectID": "reference/api/plot.plot_interaction.html#parameters",
    "href": "reference/api/plot.plot_interaction.html#parameters",
    "title": "plot.plot_interaction",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndbf\nDatabase\npycalphad thermodynamic database containing the relevant parameters.\nrequired\n\n\ncomps\nSequence[str]\nNames of components to consider in the calculation.\nrequired\n\n\nphase_name\nstr\nName of the considered phase phase\nrequired\n\n\nconfiguration\nTuple[Tuple[str]]\nESPEI-style configuration\nrequired\n\n\noutput\nstr\nModel property to plot on the y-axis e.g. 'HM_MIX', or 'SM_MIX'. Must be a '_MIX' property.\nrequired\n\n\ndatasets\ntinydb.TinyDB\n\nNone\n\n\nsymmetry\nlist\nList of lists containing indices of symmetric sublattices e.g. [[0, 1], [2, 3]]\nNone\n\n\nax\nplt.Axes\nDefault axes used if not specified.\nNone\n\n\nplot_kwargs\nOptional[Dict[str, Any]]\nKeyword arguments to ax.plot for the predicted data.\nNone\n\n\ndataplot_kwargs\nOptional[Dict[str, Any]]\nKeyword arguments to ax.plot the observed data.\nNone",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.plot",
      "plot.plot_interaction"
    ]
  },
  {
    "objectID": "reference/api/plot.plot_interaction.html#returns",
    "href": "reference/api/plot.plot_interaction.html#returns",
    "title": "plot.plot_interaction",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nplt.Axes",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.plot",
      "plot.plot_interaction"
    ]
  },
  {
    "objectID": "reference/api/parameter_selection.fitting_steps.html",
    "href": "reference/api/parameter_selection.fitting_steps.html",
    "title": "parameter_selection.fitting_steps",
    "section": "",
    "text": "parameter_selection.fitting_steps\n\n\n\n\n\nName\nDescription\n\n\n\n\nAbstractLinearPropertyStep\nThis class is a base class for generic linear properties.\n\n\n\n\n\nparameter_selection.fitting_steps.AbstractLinearPropertyStep()\nThis class is a base class for generic linear properties.\n“Generic” meaning, essentially, that property_name == parameter_name == data_type and PyCalphad models set this property something like\nself.&lt;parameter_name&gt; = self.redlich_kister_sum(phase, param_search, &lt;param_name&gt;_query)\nNote that redlich_kister_sum specifically is an implementation detail and isn’t relevant for this class in particular. Any mixing model could work, although ESPEI currently only generates features by build_redlich_kister_candidate_models.\nFitting steps that want to use this base class should need to subclass this class, override the parameter_name and data_type_read (typically these match), and optionally override the features if a desired.\nFor models that are ‘nearly linear’, and the transform_data method can be overriden to try to linearize the data with respect to the model parameters.\nMost parameters are fit per mole of formula units, but there are some exceptions (e.g. VA parameters). The normalize_parameter_per_mole_formula attribute handles normalization.\n\n\n\n\n\nName\nDescription\n\n\n\n\ntransform_data\nHelper function to linearize data in terms of the model parameters.\n\n\n\n\n\nparameter_selection.fitting_steps.AbstractLinearPropertyStep.transform_data(d, model=None)\nHelper function to linearize data in terms of the model parameters.\nIf data is already linear w.r.t. the parameters, the default implementation of identity (return d can be preserved).",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.parameter_selection.fitting_steps",
      "parameter_selection.fitting_steps"
    ]
  },
  {
    "objectID": "reference/api/parameter_selection.fitting_steps.html#classes",
    "href": "reference/api/parameter_selection.fitting_steps.html#classes",
    "title": "parameter_selection.fitting_steps",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nAbstractLinearPropertyStep\nThis class is a base class for generic linear properties.\n\n\n\n\n\nparameter_selection.fitting_steps.AbstractLinearPropertyStep()\nThis class is a base class for generic linear properties.\n“Generic” meaning, essentially, that property_name == parameter_name == data_type and PyCalphad models set this property something like\nself.&lt;parameter_name&gt; = self.redlich_kister_sum(phase, param_search, &lt;param_name&gt;_query)\nNote that redlich_kister_sum specifically is an implementation detail and isn’t relevant for this class in particular. Any mixing model could work, although ESPEI currently only generates features by build_redlich_kister_candidate_models.\nFitting steps that want to use this base class should need to subclass this class, override the parameter_name and data_type_read (typically these match), and optionally override the features if a desired.\nFor models that are ‘nearly linear’, and the transform_data method can be overriden to try to linearize the data with respect to the model parameters.\nMost parameters are fit per mole of formula units, but there are some exceptions (e.g. VA parameters). The normalize_parameter_per_mole_formula attribute handles normalization.\n\n\n\n\n\nName\nDescription\n\n\n\n\ntransform_data\nHelper function to linearize data in terms of the model parameters.\n\n\n\n\n\nparameter_selection.fitting_steps.AbstractLinearPropertyStep.transform_data(d, model=None)\nHelper function to linearize data in terms of the model parameters.\nIf data is already linear w.r.t. the parameters, the default implementation of identity (return d can be preserved).",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.parameter_selection.fitting_steps",
      "parameter_selection.fitting_steps"
    ]
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quickstart",
    "section": "",
    "text": "ESPEI has two different fitting modes: parameter generation and Bayesian parameter estimation, which uses Markov Chain Monte Carlo (MCMC). You can run either of these modes or both of them sequentially.\nTo run either of the modes, you need to have a phase models file that describes the phases in the system using the standard Calphad approach within the compound energy formalism. You also need to describe the data that ESPEI should fit to. You will need single-phase and multi-phase data for a full run. Fit settings and all datasets are stored as JSON files and described in detail at the YAML input schema page. All of your input datasets should be validated by running espei --check-datasets my-input-datasets, where my-input-datasets is a folder of all your JSON files.\nThe main output result is going to be a database (defaults to out.tdb), an array of the steps in the MCMC trace (defaults to trace.npy), and the an array of the log-probabilities for each iteration and chain (defaults to lnprob.npy).\n\n\nIf you have only non-equilibrium thermochemical data, e.g. heat capacity, entropy and enthalpy data and mixing data from first-principles calculations, you may want to see the starting point for your MCMC calculation.\nCreate an input file called espei-in.yaml.\nsystem:\n  phase_models: my-phases.json\n  datasets: my-input-datasets\ngenerate_parameters:\n  excess_model: linear\n  ref_state: SGTE91\nThen ESPEI can be run by running\nespei --input espei-in.yaml\n\n\n\nIf you have a database already and want to do a Bayesian parameter estimation, you can specify a starting TDB file (named my-tdb.tdb) with\nsystem:\n  phase_models: my-phases.json\n  datasets: my-input-data\nmcmc:\n  iterations: 1000\n  input_db: my-tdb.tdb\nThe TDB file you input must have all of the degrees of freedom you want as FUNCTIONs with names beginning with VV.\n\n\n\nIf you've run an MCMC fitting already in ESPEI and have a trace file called my-previous-trace.npy , then you can resume the calculation with the following input file\nsystem:\n  phase_models: my-phases.json\n  datasets: my-input-data\nmcmc:\n  iterations: 1000\n  input_db: my-tdb.tdb\n  restart_trace: my-previous-trace.npy\n\n\n\nA minimal full run of ESPEI is done by the following\nsystem:\n  phase_models: my-phases.json\n  datasets: my-input-data\ngenerate_parameters:\n  excess_model: linear\n  ref_state: SGTE91\nmcmc:\n  iterations: 1000\n\n\n\nESPEI lets you control many aspects of your calculations with the input files shown above. See the YAML input schema for a full description of all possible inputs.\n\n\n\n\n\nA: Common mistakes are using single quotes instead of the double quotes required by JSON files. Another common source of errors is misaligned open/closing brackets.\nMany mistakes are found with ESPEI's check-datasets utility. Run espei check-datasets my-input-datasets on your directory my-input-datasets.\n\n\n\nA: By default, ESPEI will create trace.npy and lnprob.npy for the MCMC chain at the specified save interval and according to the save interval (defaults to ever iteration). These are created from arrays via numpy.save() and can thus be loaded with numpy.load(). Note that the arrays are preallocated with zeros. These filenames and settings can be changed using in the input file. You can then use these chains and corresponding log-probabilities to make corner plots, calculate autocorrelations, find optimal parameters for databases, etc.. Some examples are shown on the Recipes page. Finally, you can use espei.plot functions such as espei.plot.dataplot in concert with pycalphad to plot phase diagrams with your input equilibria data. The espei.plot.plot_endmember and espei.plot.plot_interaction functions can be used to compare single-phase data (e.g. formation and mixing data) with the properties calculated from your database.\n\n\n\nA: Yes! ESPEI has MPI support. See the page on alternative schedulers for more details.\n\n\n\nMCMC simulation requires determining the probability of the data given a set of parameters, \\(p(D|\\theta)\\). In MCMC, the log probability is often used to avoid floating point errors that arise from multiplying many small floating point numbers. For each type of data the error, often interpreted as the absolute difference between the expected and calculated value, is determined. For the types of data and how the error is calculated, refer to the ESPEI paper (Bocklund et al. 2019).\nThe error is assumed to be normally distributed around the experimental data point that the prediction of a set of parameters is being compared against. The log probability of each data type is calculated by the log probability density function of the error in this normal distribution with a mean of zero and the standard deviation as given by the data type and the adjustable weights (see data_weights in YAML input schema). The total log probability is the sum of all log probabilities.\nNote that any probability density function always returns a positive value between 0 and 1, so the log probability density function should return negative numbers and the log probability reported by ESPEI should be negative.\n\n\n\nA: A version number of '0+unknown' indicates that you do not have git installed. This can occur on Windows where git is not in the PATH (and the Python interpreter cannot see it). You can install git using conda install git on Windows.\n\n\n\nA: Yes, if you have a multicomponent Calphad database, but want to optimize or determine the uncertainty for a constituent unary, binary or ternary subsystem that you have data for, you can do that without any extra effort.\nYou may be interested in the input_mcmc_symbols input parameter to specify which parameter subset to optimize.\nNote that if you optimize parameters in a subsystem (e.g. Cu-Mg) that is used in a higher order description (e.g. Al-Cu-Mg), you may need to reoptimize the parameters for the higher order system as well.",
    "crumbs": [
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#parameter-generation-only",
    "href": "quickstart.html#parameter-generation-only",
    "title": "Quickstart",
    "section": "",
    "text": "If you have only non-equilibrium thermochemical data, e.g. heat capacity, entropy and enthalpy data and mixing data from first-principles calculations, you may want to see the starting point for your MCMC calculation.\nCreate an input file called espei-in.yaml.\nsystem:\n  phase_models: my-phases.json\n  datasets: my-input-datasets\ngenerate_parameters:\n  excess_model: linear\n  ref_state: SGTE91\nThen ESPEI can be run by running\nespei --input espei-in.yaml",
    "crumbs": [
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#bayesian-parameter-estimation-mcmc-only",
    "href": "quickstart.html#bayesian-parameter-estimation-mcmc-only",
    "title": "Quickstart",
    "section": "",
    "text": "If you have a database already and want to do a Bayesian parameter estimation, you can specify a starting TDB file (named my-tdb.tdb) with\nsystem:\n  phase_models: my-phases.json\n  datasets: my-input-data\nmcmc:\n  iterations: 1000\n  input_db: my-tdb.tdb\nThe TDB file you input must have all of the degrees of freedom you want as FUNCTIONs with names beginning with VV.",
    "crumbs": [
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#restart-from-previous-run-phase-only",
    "href": "quickstart.html#restart-from-previous-run-phase-only",
    "title": "Quickstart",
    "section": "",
    "text": "If you've run an MCMC fitting already in ESPEI and have a trace file called my-previous-trace.npy , then you can resume the calculation with the following input file\nsystem:\n  phase_models: my-phases.json\n  datasets: my-input-data\nmcmc:\n  iterations: 1000\n  input_db: my-tdb.tdb\n  restart_trace: my-previous-trace.npy",
    "crumbs": [
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#full-run",
    "href": "quickstart.html#full-run",
    "title": "Quickstart",
    "section": "",
    "text": "A minimal full run of ESPEI is done by the following\nsystem:\n  phase_models: my-phases.json\n  datasets: my-input-data\ngenerate_parameters:\n  excess_model: linear\n  ref_state: SGTE91\nmcmc:\n  iterations: 1000",
    "crumbs": [
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#input-customization",
    "href": "quickstart.html#input-customization",
    "title": "Quickstart",
    "section": "",
    "text": "ESPEI lets you control many aspects of your calculations with the input files shown above. See the YAML input schema for a full description of all possible inputs.",
    "crumbs": [
      "Quickstart"
    ]
  },
  {
    "objectID": "quickstart.html#faq",
    "href": "quickstart.html#faq",
    "title": "Quickstart",
    "section": "",
    "text": "A: Common mistakes are using single quotes instead of the double quotes required by JSON files. Another common source of errors is misaligned open/closing brackets.\nMany mistakes are found with ESPEI's check-datasets utility. Run espei check-datasets my-input-datasets on your directory my-input-datasets.\n\n\n\nA: By default, ESPEI will create trace.npy and lnprob.npy for the MCMC chain at the specified save interval and according to the save interval (defaults to ever iteration). These are created from arrays via numpy.save() and can thus be loaded with numpy.load(). Note that the arrays are preallocated with zeros. These filenames and settings can be changed using in the input file. You can then use these chains and corresponding log-probabilities to make corner plots, calculate autocorrelations, find optimal parameters for databases, etc.. Some examples are shown on the Recipes page. Finally, you can use espei.plot functions such as espei.plot.dataplot in concert with pycalphad to plot phase diagrams with your input equilibria data. The espei.plot.plot_endmember and espei.plot.plot_interaction functions can be used to compare single-phase data (e.g. formation and mixing data) with the properties calculated from your database.\n\n\n\nA: Yes! ESPEI has MPI support. See the page on alternative schedulers for more details.\n\n\n\nMCMC simulation requires determining the probability of the data given a set of parameters, \\(p(D|\\theta)\\). In MCMC, the log probability is often used to avoid floating point errors that arise from multiplying many small floating point numbers. For each type of data the error, often interpreted as the absolute difference between the expected and calculated value, is determined. For the types of data and how the error is calculated, refer to the ESPEI paper (Bocklund et al. 2019).\nThe error is assumed to be normally distributed around the experimental data point that the prediction of a set of parameters is being compared against. The log probability of each data type is calculated by the log probability density function of the error in this normal distribution with a mean of zero and the standard deviation as given by the data type and the adjustable weights (see data_weights in YAML input schema). The total log probability is the sum of all log probabilities.\nNote that any probability density function always returns a positive value between 0 and 1, so the log probability density function should return negative numbers and the log probability reported by ESPEI should be negative.\n\n\n\nA: A version number of '0+unknown' indicates that you do not have git installed. This can occur on Windows where git is not in the PATH (and the Python interpreter cannot see it). You can install git using conda install git on Windows.\n\n\n\nA: Yes, if you have a multicomponent Calphad database, but want to optimize or determine the uncertainty for a constituent unary, binary or ternary subsystem that you have data for, you can do that without any extra effort.\nYou may be interested in the input_mcmc_symbols input parameter to specify which parameter subset to optimize.\nNote that if you optimize parameters in a subsystem (e.g. Cu-Mg) that is used in a higher order description (e.g. Al-Cu-Mg), you may need to reoptimize the parameters for the higher order system as well.",
    "crumbs": [
      "Quickstart"
    ]
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "To install ESPEI from PyPI using pip:\npip install -U pip\npip install -U espei\nA recommended best practice is to install Python packages into a virtual environment. To create an environment and install ESPEI on Linux and macOS/OSX:\npython -m venv calphad-env\nsource calphad-env/bin/activate\npip install -U pip\npip install -U pycalphad\nOn Windows:\npython -m venv calphad-env\ncalphad-env\\Scripts\\activate\npip install -U pip\npip install -U pycalphad",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#pip-recommended",
    "href": "installation.html#pip-recommended",
    "title": "Installation",
    "section": "",
    "text": "To install ESPEI from PyPI using pip:\npip install -U pip\npip install -U espei\nA recommended best practice is to install Python packages into a virtual environment. To create an environment and install ESPEI on Linux and macOS/OSX:\npython -m venv calphad-env\nsource calphad-env/bin/activate\npip install -U pip\npip install -U pycalphad\nOn Windows:\npython -m venv calphad-env\ncalphad-env\\Scripts\\activate\npip install -U pip\npip install -U pycalphad",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#anaconda",
    "href": "installation.html#anaconda",
    "title": "Installation",
    "section": "Anaconda",
    "text": "Anaconda\nIf you prefer using Anaconda, ESPEI is distributed on conda-forge. If you do not have Anaconda installed, we recommend you download and install Miniconda3. ESPEI can be installed with the conda package manager by:\nconda install -c conda-forge espei",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#installing-development-versions",
    "href": "installation.html#installing-development-versions",
    "title": "Installation",
    "section": "Development versions",
    "text": "Development versions\nTo make changes to the ESPEI source code, the development version must be installed. If you'll need to make changes to pycalphad simultaneously, follow the instructions to install the development version of pycalphad first.\nWe strongly recommend that users interested in developing packages work in a virtual environment. The steps below will create an environment called calphad-dev, which can be entered using source calphad-dev/bin/activate on Linux or macOS/OSX or activate calphad-dev\\bin\\activate on Windows. The environment name is arbitrary - you can use whatever name you prefer.\nESPEI uses Git and GitHub for version control. Windows users: if you do not have a working version of Git, download it here first.\nTo install the latest development version of ESPEI using pip:\npython -m venv calphad-env\nsource calphad-env/bin/activate\ngit clone https://github.com/phasesresearchlab/espei.git\ncd espei\npip install -U pip\npip install -U --editable .[dev]\nWith the development version installed and your environment activated, you can run the automated tests by running\npytest\nIf the test suite passes, you are ready to start using the development version or making changes yourself! See the guide for contributing to ESPEI to learn more. If any tests fail, please report the failure to the ESPEI issue tracker on GitHub.\nTo upgrade your development version to the latest version, run git pull from the top level ESPEI directory (the directory containing the setup.py file).",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "reference/dataset_schema.html",
    "href": "reference/dataset_schema.html",
    "title": "ESPEI Dataset Schema",
    "section": "",
    "text": "Formatting JSON Files for ESPEI\n\n\n\n\n\nESPEI has a single input style in JSON format that is used for all data entry. For those unfamiliar with JSON, it is fairly similar to Python dictionaries with some rigid requirements\n\n\nAll string quotes must be double quotes. Use \"key\" instead of 'key'.\nNumbers should not have leading zeros. 00.123 should be 0.123 and 012.34 must be 12.34.\nLists and nested key-value pairs cannot have trailing commas. {\"nums\": [1,2,3,],} is invalid and should be {\"nums\": [1,2,3]}.\n\n\nThese errors can be challenging to track down, particularly if you are only reading the JSON error messages in Python. A visual editor is encouraged for debugging JSON files such as JSONLint. A quick reference to the format can be found at Learn JSON in Y minutes.\nESPEI has support for checking all of your input datasets for errors, which you should always use before you attempt to run ESPEI. This error checking will report all of the errors at once and all errors should be fixed. Errors in the datasets will prevent fitting. To check the datasets at path my-input-data/ you can run espei --check-datasets my-input-data.",
    "crumbs": [
      "Reference",
      "ESPEI Dataset Schema"
    ]
  },
  {
    "objectID": "reference/dataset_schema.html#non_equilibrium_thermochemical_data",
    "href": "reference/dataset_schema.html#non_equilibrium_thermochemical_data",
    "title": "ESPEI Dataset Schema",
    "section": "Non-equilibrium Thermochemical Data",
    "text": "Non-equilibrium Thermochemical Data\nNon-equilibrium thermochemical data is used where the internal degrees of freedom for a phase are known. This type of data is the only data that can be used for parameter generation, but it can also be used in Bayesian parameter estimation.\nTwo examples follow. The first dataset has some data for the formation heat capacity for BCC_B2.\n\nThe components and phases keys simply describe those found in this entry.\nUse the reference key for bookkeeping the source of the data.\nThe comment key and value can be used anywhere in the data to keep notes for your reference. It takes no effect.\nThe solver the internal degrees of freedom and and site ratios are described for the phase.\n\nsublattice_configurations is a list of different configurations, that should correspond to the sublattices for the phase descriptions. Non-mixing sublattices are represented as a string, while mixing sublattices are represented as a lists. Thus an endmember for BCC_B2 (as in this example) is [\"AL\", \"NI\", VA\"] and if there were mixing (as in the next example) it might be [\"AL\", [\"AL\", \"NI\"], \"VA\"]. Mixing also means that the sublattice_occupancies key must be specified, but that is not the case in this example. It is important to note that any mixing configurations must have any ideal mixing contributions removed. Regardless of whether there is mixing or not, the length of this list should always equal the number of sublattices in the phase, though the sub-lists can have mixing up to the number of components in that sublattice. Note that the sublattice_configurations is a list of these lists. That is, there can be multiple sublattice configurations in a single dataset. See the second example in this section for such an example.\n\nThe conditions describe temperatures (T) and pressures (P) as either scalars or one-dimensional lists.\nThe type of quantity is expressed using the output key. This can in principle be any thermodynamic quantity, but currently only CPM*, SM*, and HM* (where * is either nothing, _MIX or _FORM) are supported. Support for changing reference states is planned but not yet implemented, so all thermodynamic quantities must be formation quantities (e.g. HM_FORM or HM_MIX, etc.). This is tracked by ESPEI issue #85 on GitHub.\nvalues is a 3-dimensional array where each value is the output for a specific condition of pressure, temperature, and sublattice configurations from outside to inside. Alternatively, the size of the array must be (len(P), len(T), len(subl_config)). In the example below, the shape of the values array is (1, 12, 1) as there is one pressure scalar, one sublattice configuration, and 12 temperatures.\nThere is also a key, excluded_model_contributions, which will make those contributions of pycalphad's Model not be fit to when doing parameter selection or MCMC. This is useful for cases where the type of data used does not include some specific Model contributions that parameters may already exist for. For example, DFT formation energies do not include ideal mixing or (Calphad-type) magnetic model contributions, but formation energies from experiments would include these contributions so experimental formation energies should not be excluded.\n\n{\n  \"reference\": \"Yi Wang et al 2009\",\n  \"components\": [\"AL\", \"NI\", \"VA\"],\n  \"phases\": [\"BCC_B2\"],\n  \"solver\": {\n    \"mode\": \"manual\",\n      \"sublattice_site_ratios\": [0.5, 0.5, 1],\n      \"sublattice_configurations\": [[\"AL\", \"NI\", \"VA\"]],\n      \"comment\": \"NiAl sublattice configuration (2SL)\"\n  },\n  \"conditions\": {\n      \"P\": 101325,\n      \"T\": [  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100, 110]\n  },\n  \"excluded_model_contributions\": [\"idmix\", \"mag\"],\n  \"output\": \"CPM_FORM\",\n  \"values\":   [[[ 0      ],\n                [-0.0173 ],\n                [-0.01205],\n                [ 0.12915],\n                [ 0.24355],\n                [ 0.13305],\n                [-0.1617 ],\n                [-0.51625],\n                [-0.841  ],\n                [-1.0975 ],\n                [-1.28045],\n                [-1.3997 ]]]\n}\nIn the second example below, there is formation enthalpy data for multiple sublattice configurations. All of the keys and values are conceptually similar. Here, instead of describing how the output quantity changes with temperature or pressure, we are instead only comparing HM_FORM values for different sublattice configurations. The key differences from the previous example are that there are 9 different sublattice configurations described by sublattice_configurations and sublattice_occupancies. Note that the sublattice_configurations and sublattice_occupancies should have exactly the same shape. Sublattices without mixing should have single strings and occupancies of one. Sublattices that do have mixing should have a site ratio for each active component in that sublattice. If the sublattice of a phase is [\"AL\", \"NI\", \"VA\"], it should only have two occupancies if only [\"AL\", \"NI\"] are active in the sublattice configuration.\nThe last difference to note is the shape of the values array. Here there is one pressure, one temperature, and 9 sublattice configurations to give a shape of (1, 1, 9).\n{\n  \"reference\": \"C. Jiang 2009 (constrained SQS)\",\n  \"components\": [\"AL\", \"NI\", \"VA\"],\n  \"phases\": [\"BCC_B2\"],\n  \"solver\": {\n      \"sublattice_occupancies\": [\n                     [1, [0.5, 0.5], 1],\n                     [1, [0.75, 0.25], 1],\n                     [1, [0.75, 0.25], 1],\n                     [1, [0.5, 0.5], 1],\n                     [1, [0.5, 0.5], 1],\n                     [1, [0.25, 0.75], 1],\n                     [1, [0.75, 0.25], 1],\n                     [1, [0.5, 0.5], 1],\n                     [1, [0.5, 0.5], 1]\n                    ],\n      \"sublattice_site_ratios\": [0.5, 0.5, 1],\n      \"sublattice_configurations\": [\n                        [\"AL\", [\"NI\", \"VA\"], \"VA\"],\n                        [\"AL\", [\"NI\", \"VA\"], \"VA\"],\n                        [\"NI\", [\"AL\", \"NI\"], \"VA\"],\n                        [\"NI\", [\"AL\", \"NI\"], \"VA\"],\n                        [\"AL\", [\"AL\", \"NI\"], \"VA\"],\n                        [\"AL\", [\"AL\", \"NI\"], \"VA\"],\n                        [\"NI\", [\"AL\", \"VA\"], \"VA\"],\n                        [\"NI\", [\"AL\", \"VA\"], \"VA\"],\n                        [\"VA\", [\"AL\", \"NI\"], \"VA\"]\n                       ],\n      \"comment\": \"BCC_B2 sublattice configuration (2SL)\"\n  },\n  \"conditions\": {\n      \"P\": 101325,\n      \"T\": 300\n  },\n  \"output\": \"HM_FORM\",\n  \"values\":   [[[-40316.61077, -56361.58554,\n             -49636.39281, -32471.25149, -10890.09929,\n             -35190.49282, -38147.99217, -2463.55684,\n             -15183.13371]]]\n}",
    "crumbs": [
      "Reference",
      "ESPEI Dataset Schema"
    ]
  },
  {
    "objectID": "reference/dataset_schema.html#equilibrium-thermochemical-data",
    "href": "reference/dataset_schema.html#equilibrium-thermochemical-data",
    "title": "ESPEI Dataset Schema",
    "section": "Equilibrium Thermochemical Data",
    "text": "Equilibrium Thermochemical Data\nEquilibrium thermochemical data is used when the internal degrees of freedom are not known. This is typically true for experimental thermochemical data. Some cases where this type of data is useful, compared to non-equilibrium thermochemical data are:\n\nActivity data\nEnthalpy of formation data in region with two or more phases in equilibrium\nEnthalpy of formation for a phase with multiple sublattice, e.g. the σ phase\n\nThis type of data can not be used in parameter selection, because a core assumption of parameter selection is that the site fractions are known.\nActivity data is similar to non-equilibrium thermochemical data, except we must enter a reference state and the solver key is no longer required, since we do not know the internal degrees of freedom. A key detail is that the phases key must specify all phases that are possible to form.\nAn example for Mg activties in Cu-Mg follows, with data digitized from S.P. Garg, Y.J. Bhatt, C. V. Sundaram, Thermodynamic study of liquid Cu-Mg alloys by vapor pressure measurements, Metall. Trans. 4 (1973) 283–289. doi:10.1007/BF02649628.\n{\n  \"components\": [\"CU\", \"MG\", \"VA\"],\n  \"phases\": [\"LIQUID\", \"FCC_A1\", \"HCP_A3\"],\n  \"reference_state\": {\n    \"phases\": [\"LIQUID\"],\n    \"conditions\": {\n      \"P\": 101325,\n      \"T\": 1200,\n      \"X_MG\": 1.0\n    }\n  },\n  \"conditions\": {\n    \"P\": 101325,\n    \"T\": 1200,\n    \"X_CU\": [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]\n  },\n\n  \"output\": \"ACR_MG\",\n    \"values\":   [[[0.0057,0.0264,0.0825,0.1812,0.2645,0.4374,0.5852,0.7296,0.882,1.0]]],\n  \"reference\": \"garg1973thermodynamic\",\n  \"comment\": \"Digitized Figure 3 and converted from activity coefficients.\"\n}",
    "crumbs": [
      "Reference",
      "ESPEI Dataset Schema"
    ]
  },
  {
    "objectID": "reference/dataset_schema.html#phase_boundary_data",
    "href": "reference/dataset_schema.html#phase_boundary_data",
    "title": "ESPEI Dataset Schema",
    "section": "Phase Diagram Data",
    "text": "Phase Diagram Data\nESPEI can consider multi-component phase diagram data with an arbitrary number of phases in equilibrium. Phase diagram data JSON datasets are distingished by using \"output\": \"ZPF\"1. Each entry in the JSON values corresponds to a phase region where one or more phases are participating in equilibrium under the given temperature and pressure conditions.\nEach phase in the phase region must give its phase composition, i.e. the internal composition of that phase (not the overall composition). The \"phase composition\" is the same as a \"tie-line composition\" in a two-phase region of a binary phase diagram, but is a more general term for cases where the meaning of a tie-line is ambiguous like a single phase equilibrum or an equilibrium with three or more phases.\nSometimes there may be a phase equilibrium where one or more of the phase compositions are unknown. This is especially common for phase diagram data determined by equilibrated alloys or by scanning calorimetry in binary systems, where one phase composition is determined, but the phase composition of the other phase(s) in equilibrium are not. In these cases, phase compositions can be given as null and ESPEI will estimate the phase composition.\n\nImportant\nEach phase region must have at least one phase with a prescribed phase composition. If all phases in a phase region have null phase compositions, the target hyperplane (described by Figure 1 in (Bocklund et al. 2019)) will be undefined and no driving forces will be computed.\n\n\nImportant\nFor a dataset with c components, each phase composition must be specified by c-1 components. There is an implicit N=1 condition.\n\n\nExample\n{\n  \"components\": [\"AL\", \"NI\"],\n  \"phases\": [\"AL3NI2\", \"BCC_B2\", \"LIQUID\"],\n  \"conditions\": {\n    \"P\": 101325,\n     \"T\": [2500, 1348, 1176, 977]\n  },\n  \"output\": \"ZPF\",\n  \"values\": [\n    [[\"LIQUID\", [\"NI\"], [0.5]]],\n    [[\"AL3NI2\", [\"NI\"], [0.4083]], [\"BCC_B2\", [\"NI\"], [0.4340]]],\n    [[\"AL3NI2\", [\"NI\"], [0.4114]], [\"BCC_B2\", [\"NI\"], [null]]],\n    [[\"BCC_B2\", [\"NI\"], [0.71]], [\"LIQUID\", [\"NI\"], [0.752]], [\"FCC_L12\", [\"NI\"], [0.76]]]\n  ],\n  \"reference\": \"37ALE\"\n}\nEach entry in the values list is a list of all phases in equilibrium in a phase region. There are four phase regions:\n\n[[\"LIQUID\", [\"NI\"], [0.5]]]\n\nSingle phase equilibrium with LIQUID having a phase composition of X(NI,LIQUID)=0.5.\n\n[[\"AL3NI2\", [\"NI\"], [0.4083]], [\"BCC_B2\", [\"NI\"], [0.4340]]]\n\nTwo phase equilibrium between AL3NI2 and BCC_B2, which have phase compositions of X(NI,AL3NI2)=0.4083 and X(NI,BCC_B2)=0.4340, respectively.\n\n[[\"AL3NI2\", [\"NI\"], [0.4114]], [\"BCC_B2\", [\"NI\"], [null]]]\n\nTwo phase equilibrium between AL3NI2 and BCC_B2 where the phase composition of BCC_B2 is unknown.\n\n[[\"BCC_B2\", [\"NI\"], [0.71]], [\"LIQUID\", [\"NI\"], [0.752]], [\"FCC_L12\", [\"NI\"], [0.76]]]\n\nEutectic reaction between LIQUID, BCC_B2 and FCC_L12.\n\n\n\nTip: Multi-component phase regions\nTo describe multi-component phase regions, simply include more components and compositions in each phase composition. For example, a two-phase equilibrium in a three component system could be described by [[\"ALPHA\", [\"CR\", \"NI\"], [0.1, 0.25]], [\"BETA\", [\"CR\", \"NI\"], [null, null]]]",
    "crumbs": [
      "Reference",
      "ESPEI Dataset Schema"
    ]
  },
  {
    "objectID": "reference/dataset_schema.html#tags",
    "href": "reference/dataset_schema.html#tags",
    "title": "ESPEI Dataset Schema",
    "section": "Tags",
    "text": "Tags\nTags are a flexible method to adjust many ESPEI datasets simultaneously and drive them via the ESPEI's input YAML file. Each dataset can have a \"tags\" key, with a corresponding value of a list of tags, e.g. [\"dft\"]. Any tag modifications present in the input YAML file are applied to the datasets before ESPEI is run.\nThey can be used in many creative ways, but some suggested ways include to add weights or to exclude model contributions, e.g. for DFT data that should not have contributions for a Calphad magnetic model or ideal mixing energy. An example of using the tags in an input file looks like:\n{\n  \"components\": [\"CR\", \"FE\", \"VA\"],\"phases\": [\"BCC_A2\"],\n  \"solver\": {\"mode\": \"manual\", \"sublattice_site_ratios\": [1, 3],\n             \"sublattice_configurations\": [[[\"CR\", \"FE\"], \"VA\"]],\n  \"sublattice_occupancies\": [[[0.5, 0.5], 1.0]]},\n  \"conditions\": {\"P\": 101325, \"T\": 300},\n  \"output\": \"HM_MIX\",\n  \"values\": [[[10000]]],\n  \"tags\": [\"dft\"]\n}\nAn example input YAML looks like\nsystem:\n  phase_models: CR-FE.json\n  datasets: FE-NI-datasets-sep\n  tags:\n    dft:\n      excluded_model_contributions: [\"idmix\", \"mag\"]\n\ngenerate_parameters:\n  excess_model: linear\n  ref_state: SGTE91\n  ridge_alpha: 1.0e-20\noutput:\n  verbosity: 2\n  output_db: out.tdb\nThis will add the key \"excluded_model_contributions\" to all datasets that have the \"dft\" tag:\n{\n  \"components\": [\"CR\", \"FE\", \"VA\"],\"phases\": [\"BCC_A2\"],\n  \"solver\": {\"mode\": \"manual\", \"sublattice_site_ratios\": [1, 3],\n             \"sublattice_configurations\": [[[\"CR\", \"FE\"], \"VA\"]],\n  \"sublattice_occupancies\": [[[0.5, 0.5], 1.0]]},\n  \"conditions\": {\"P\": 101325, \"T\": 300},\n  \"output\": \"HM_MIX\",\n  \"values\": [[[10000]]],\n  \"excluded_model_contributions\": [\"idmix\", \"mag\"]\n}",
    "crumbs": [
      "Reference",
      "ESPEI Dataset Schema"
    ]
  },
  {
    "objectID": "reference/dataset_schema.html#common-mistakes-and-notes",
    "href": "reference/dataset_schema.html#common-mistakes-and-notes",
    "title": "ESPEI Dataset Schema",
    "section": "Common Mistakes and Notes",
    "text": "Common Mistakes and Notes\n\nA single element sublattice is different in a phase model ([[\"A\", \"B\"], [\"A\"]]]) than a sublattice configuration ([[\"A\", \"B\"], \"A\"]).\nMake sure you use the right units (J/mole-atom, mole fractions, Kelvin, Pascal)\nMixing configurations should not have ideal mixing contributions.\nAll types of data can have a weight key at the top level that will weight the standard deviation parameter in MCMC runs for that dataset. If a single dataset should have different weights applied, multiple datasets should be created.",
    "crumbs": [
      "Reference",
      "ESPEI Dataset Schema"
    ]
  },
  {
    "objectID": "reference/dataset_schema.html#footnotes",
    "href": "reference/dataset_schema.html#footnotes",
    "title": "ESPEI Dataset Schema",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nZPF after the \"Zero Phase Fraction\" method (Bocklund et al. 2019) used to compute the likelihood. \"Zero phase fraction\" is a little misleading as a name, since the prescribed phase compositions in the datasets actually correspond to the overall composition where the phase fraction of the desired phase should be one.↩︎",
    "crumbs": [
      "Reference",
      "ESPEI Dataset Schema"
    ]
  },
  {
    "objectID": "reference/api/datasets.load_datasets.html",
    "href": "reference/api/datasets.load_datasets.html",
    "title": "datasets.load_datasets",
    "section": "",
    "text": "datasets.load_datasets(dataset_filenames, include_disabled=False)\nCreate a PickelableTinyDB with the data from a list of filenames.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataset_filenames\n[str]\nList of filenames to load as datasets\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nPickleableTinyDB",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.datasets",
      "datasets.load_datasets"
    ]
  },
  {
    "objectID": "reference/api/datasets.load_datasets.html#parameters",
    "href": "reference/api/datasets.load_datasets.html#parameters",
    "title": "datasets.load_datasets",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndataset_filenames\n[str]\nList of filenames to load as datasets\nrequired",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.datasets",
      "datasets.load_datasets"
    ]
  },
  {
    "objectID": "reference/api/datasets.load_datasets.html#returns",
    "href": "reference/api/datasets.load_datasets.html#returns",
    "title": "datasets.load_datasets",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nPickleableTinyDB",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.datasets",
      "datasets.load_datasets"
    ]
  },
  {
    "objectID": "reference/api/plot.plot_endmember.html",
    "href": "reference/api/plot.plot_endmember.html",
    "title": "plot.plot_endmember",
    "section": "",
    "text": "plot.plot_endmember(dbf, comps, phase_name, configuration, output, datasets=None, symmetry=None, x='T', ax=None, plot_kwargs=None, dataplot_kwargs=None)\nReturn one set of plotted Axes with data compared to calculated parameters\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndbf\nDatabase\npycalphad thermodynamic database containing the relevant parameters.\nrequired\n\n\ncomps\nSequence[str]\nNames of components to consider in the calculation.\nrequired\n\n\nphase_name\nstr\nName of the considered phase phase\nrequired\n\n\nconfiguration\nTuple[Tuple[str]]\nESPEI-style configuration\nrequired\n\n\noutput\nstr\nModel property to plot on the y-axis e.g. 'HM_MIX', or 'SM_MIX'. Must be a '_MIX' property.\nrequired\n\n\ndatasets\ntinydb.TinyDB\n\nNone\n\n\nsymmetry\nlist\nList of lists containing indices of symmetric sublattices e.g. [[0, 1], [2, 3]]\nNone\n\n\nax\nplt.Axes\nDefault axes used if not specified.\nNone\n\n\nplot_kwargs\nOptional[Dict[str, Any]]\nKeyword arguments to ax.plot for the predicted data.\nNone\n\n\ndataplot_kwargs\nOptional[Dict[str, Any]]\nKeyword arguments to ax.plot the observed data.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nplt.Axes",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.plot",
      "plot.plot_endmember"
    ]
  },
  {
    "objectID": "reference/api/plot.plot_endmember.html#parameters",
    "href": "reference/api/plot.plot_endmember.html#parameters",
    "title": "plot.plot_endmember",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndbf\nDatabase\npycalphad thermodynamic database containing the relevant parameters.\nrequired\n\n\ncomps\nSequence[str]\nNames of components to consider in the calculation.\nrequired\n\n\nphase_name\nstr\nName of the considered phase phase\nrequired\n\n\nconfiguration\nTuple[Tuple[str]]\nESPEI-style configuration\nrequired\n\n\noutput\nstr\nModel property to plot on the y-axis e.g. 'HM_MIX', or 'SM_MIX'. Must be a '_MIX' property.\nrequired\n\n\ndatasets\ntinydb.TinyDB\n\nNone\n\n\nsymmetry\nlist\nList of lists containing indices of symmetric sublattices e.g. [[0, 1], [2, 3]]\nNone\n\n\nax\nplt.Axes\nDefault axes used if not specified.\nNone\n\n\nplot_kwargs\nOptional[Dict[str, Any]]\nKeyword arguments to ax.plot for the predicted data.\nNone\n\n\ndataplot_kwargs\nOptional[Dict[str, Any]]\nKeyword arguments to ax.plot the observed data.\nNone",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.plot",
      "plot.plot_endmember"
    ]
  },
  {
    "objectID": "reference/api/plot.plot_endmember.html#returns",
    "href": "reference/api/plot.plot_endmember.html#returns",
    "title": "plot.plot_endmember",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nplt.Axes",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.plot",
      "plot.plot_endmember"
    ]
  },
  {
    "objectID": "reference/api/run_espei.html",
    "href": "reference/api/run_espei.html",
    "title": "run_espei",
    "section": "",
    "text": "run_espei(run_settings)\nWrapper around the ESPEI fitting procedure, taking only a settings dictionary.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrun_settings\ndict\nDictionary of input settings\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nEither a Database (for generate parameters only) or a tuple of (Database, sampler)",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei",
      "run_espei"
    ]
  },
  {
    "objectID": "reference/api/run_espei.html#parameters",
    "href": "reference/api/run_espei.html#parameters",
    "title": "run_espei",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nrun_settings\ndict\nDictionary of input settings\nrequired",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei",
      "run_espei"
    ]
  },
  {
    "objectID": "reference/api/run_espei.html#returns",
    "href": "reference/api/run_espei.html#returns",
    "title": "run_espei",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nEither a Database (for generate parameters only) or a tuple of (Database, sampler)",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei",
      "run_espei"
    ]
  },
  {
    "objectID": "reference/api/error_functions.zpf_error.ZPFResidual.html",
    "href": "reference/api/error_functions.zpf_error.ZPFResidual.html",
    "title": "error_functions.zpf_error.ZPFResidual",
    "section": "",
    "text": "error_functions.zpf_error.ZPFResidual\nerror_functions.zpf_error.ZPFResidual(self, database, datasets, phase_models, symbols_to_fit=None, weight=None)\n\n\n\n\n Back to top",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.error_functions",
      "error_functions.zpf_error.ZPFResidual"
    ]
  },
  {
    "objectID": "reference/api/error_functions.equilibrium_thermochemical_error.EquilibriumPropertyResidual.html",
    "href": "reference/api/error_functions.equilibrium_thermochemical_error.EquilibriumPropertyResidual.html",
    "title": "error_functions.equilibrium_thermochemical_error.EquilibriumPropertyResidual",
    "section": "",
    "text": "error_functions.equilibrium_thermochemical_error.EquilibriumPropertyResidual\nerror_functions.equilibrium_thermochemical_error.EquilibriumPropertyResidual(self, database, datasets, phase_models, symbols_to_fit=None, weight=None)\n\n\n\n\n Back to top",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.error_functions",
      "error_functions.equilibrium_thermochemical_error.EquilibriumPropertyResidual"
    ]
  },
  {
    "objectID": "reference/api/error_functions.non_equilibrium_thermochemical_error.FixedConfigurationPropertyResidual.html",
    "href": "reference/api/error_functions.non_equilibrium_thermochemical_error.FixedConfigurationPropertyResidual.html",
    "title": "error_functions.non_equilibrium_thermochemical_error.FixedConfigurationPropertyResidual",
    "section": "",
    "text": "error_functions.non_equilibrium_thermochemical_error.FixedConfigurationPropertyResidual\nerror_functions.non_equilibrium_thermochemical_error.FixedConfigurationPropertyResidual(self, database, datasets, phase_models, symbols_to_fit=None, weight=None)\n\n\n\n\n Back to top",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.error_functions",
      "error_functions.non_equilibrium_thermochemical_error.FixedConfigurationPropertyResidual"
    ]
  },
  {
    "objectID": "reference/api/analysis.truncate_arrays.html",
    "href": "reference/api/analysis.truncate_arrays.html",
    "title": "analysis.truncate_arrays",
    "section": "",
    "text": "analysis.truncate_arrays(trace_array, prob_array=None)\nReturn slides of ESPEI output arrays with any empty remaining iterations (zeros) removed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrace_array\nnp.ndarray\nArray of the trace from an ESPEI run. Should have shape (chains, iterations, parameters)\nrequired\n\n\nprob_array\nnp.ndarray\nArray of the lnprob output from an ESPEI run. Should have shape (chains, iterations)\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnp.ndarry or (np.ndarray, np.ndarray)\nA slide of the zeros-removed trace array is returned if only the trace is passed. Otherwise a tuple of both the trace and lnprob are returned.\n\n\n\n\n\n\n&gt;&gt;&gt; from espei.analysis import truncate_arrays\n&gt;&gt;&gt; trace = np.array([[[1, 0], [2, 0], [3, 0], [0, 0]], [[0, 2], [0, 4], [0, 6], [0, 0]]])  # 3 iterations of 4 allocated\n&gt;&gt;&gt; truncate_arrays(trace).shape\n(2, 3, 2)",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.analsys",
      "analysis.truncate_arrays"
    ]
  },
  {
    "objectID": "reference/api/analysis.truncate_arrays.html#parameters",
    "href": "reference/api/analysis.truncate_arrays.html#parameters",
    "title": "analysis.truncate_arrays",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntrace_array\nnp.ndarray\nArray of the trace from an ESPEI run. Should have shape (chains, iterations, parameters)\nrequired\n\n\nprob_array\nnp.ndarray\nArray of the lnprob output from an ESPEI run. Should have shape (chains, iterations)\nNone",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.analsys",
      "analysis.truncate_arrays"
    ]
  },
  {
    "objectID": "reference/api/analysis.truncate_arrays.html#returns",
    "href": "reference/api/analysis.truncate_arrays.html#returns",
    "title": "analysis.truncate_arrays",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nnp.ndarry or (np.ndarray, np.ndarray)\nA slide of the zeros-removed trace array is returned if only the trace is passed. Otherwise a tuple of both the trace and lnprob are returned.",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.analsys",
      "analysis.truncate_arrays"
    ]
  },
  {
    "objectID": "reference/api/analysis.truncate_arrays.html#examples",
    "href": "reference/api/analysis.truncate_arrays.html#examples",
    "title": "analysis.truncate_arrays",
    "section": "",
    "text": "&gt;&gt;&gt; from espei.analysis import truncate_arrays\n&gt;&gt;&gt; trace = np.array([[[1, 0], [2, 0], [3, 0], [0, 0]], [[0, 2], [0, 4], [0, 6], [0, 0]]])  # 3 iterations of 4 allocated\n&gt;&gt;&gt; truncate_arrays(trace).shape\n(2, 3, 2)",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.analsys",
      "analysis.truncate_arrays"
    ]
  },
  {
    "objectID": "reference/api/plot.dataplot.html",
    "href": "reference/api/plot.dataplot.html",
    "title": "plot.dataplot",
    "section": "",
    "text": "plot.dataplot(comps, phases, conds, datasets, tielines=True, ax=None, plot_kwargs=None, tieline_plot_kwargs=None)\nPlot datapoints corresponding to the components, phases, and conditions.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncomps\nlist\nNames of components to consider in the calculation.\nrequired\n\n\nphases\n[]\nNames of phases to consider in the calculation.\nrequired\n\n\nconds\ndict\nMaps StateVariables to values and/or iterables of values.\nrequired\n\n\ndatasets\nPickleableTinyDB\n\nrequired\n\n\ntielines\nbool\nIf True (default), plot the tie-lines from the data\nTrue\n\n\nax\nmatplotlib.Axes\nDefault axes used if not specified.\nNone\n\n\nplot_kwargs\ndict\nAdditional keyword arguments to pass to the matplotlib plot function for points\nNone\n\n\ntieline_plot_kwargs\ndict\nAdditional keyword arguments to pass to the matplotlib plot function for tielines\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nmatplotlib.Axes\nA plot of phase equilibria points as a figure\n\n\n\n\n\n\n&gt;&gt;&gt; from espei.datasets import load_datasets, recursive_glob\n&gt;&gt;&gt; from espei.plot import dataplot\n&gt;&gt;&gt; datasets = load_datasets(recursive_glob('.', '*.json'))\n&gt;&gt;&gt; my_phases = ['BCC_A2', 'CUMG2', 'FCC_A1', 'LAVES_C15', 'LIQUID']\n&gt;&gt;&gt; my_components = ['CU', 'MG' 'VA']\n&gt;&gt;&gt; conditions = {v.P: 101325, v.T: (500, 1000, 10), v.X('MG'): (0, 1, 0.01)}\n&gt;&gt;&gt; dataplot(my_components, my_phases, conditions, datasets)",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.plot",
      "plot.dataplot"
    ]
  },
  {
    "objectID": "reference/api/plot.dataplot.html#parameters",
    "href": "reference/api/plot.dataplot.html#parameters",
    "title": "plot.dataplot",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ncomps\nlist\nNames of components to consider in the calculation.\nrequired\n\n\nphases\n[]\nNames of phases to consider in the calculation.\nrequired\n\n\nconds\ndict\nMaps StateVariables to values and/or iterables of values.\nrequired\n\n\ndatasets\nPickleableTinyDB\n\nrequired\n\n\ntielines\nbool\nIf True (default), plot the tie-lines from the data\nTrue\n\n\nax\nmatplotlib.Axes\nDefault axes used if not specified.\nNone\n\n\nplot_kwargs\ndict\nAdditional keyword arguments to pass to the matplotlib plot function for points\nNone\n\n\ntieline_plot_kwargs\ndict\nAdditional keyword arguments to pass to the matplotlib plot function for tielines\nNone",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.plot",
      "plot.dataplot"
    ]
  },
  {
    "objectID": "reference/api/plot.dataplot.html#returns",
    "href": "reference/api/plot.dataplot.html#returns",
    "title": "plot.dataplot",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nmatplotlib.Axes\nA plot of phase equilibria points as a figure",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.plot",
      "plot.dataplot"
    ]
  },
  {
    "objectID": "reference/api/plot.dataplot.html#examples",
    "href": "reference/api/plot.dataplot.html#examples",
    "title": "plot.dataplot",
    "section": "",
    "text": "&gt;&gt;&gt; from espei.datasets import load_datasets, recursive_glob\n&gt;&gt;&gt; from espei.plot import dataplot\n&gt;&gt;&gt; datasets = load_datasets(recursive_glob('.', '*.json'))\n&gt;&gt;&gt; my_phases = ['BCC_A2', 'CUMG2', 'FCC_A1', 'LAVES_C15', 'LIQUID']\n&gt;&gt;&gt; my_components = ['CU', 'MG' 'VA']\n&gt;&gt;&gt; conditions = {v.P: 101325, v.T: (500, 1000, 10), v.X('MG'): (0, 1, 0.01)}\n&gt;&gt;&gt; dataplot(my_components, my_phases, conditions, datasets)",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.plot",
      "plot.dataplot"
    ]
  },
  {
    "objectID": "reference/api/parameter_selection.fitting_descriptions.ModelFittingDescription.html",
    "href": "reference/api/parameter_selection.fitting_descriptions.ModelFittingDescription.html",
    "title": "parameter_selection.fitting_descriptions.ModelFittingDescription",
    "section": "",
    "text": "parameter_selection.fitting_descriptions.ModelFittingDescription(self, fitting_steps, model=Model)\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nfitting_steps\n[FittingStep]\n\n\n\nmodel\nType[Model]",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.parameter_selection.fitting_descriptions",
      "parameter_selection.fitting_descriptions.ModelFittingDescription"
    ]
  },
  {
    "objectID": "reference/api/parameter_selection.fitting_descriptions.ModelFittingDescription.html#attributes",
    "href": "reference/api/parameter_selection.fitting_descriptions.ModelFittingDescription.html#attributes",
    "title": "parameter_selection.fitting_descriptions.ModelFittingDescription",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nfitting_steps\n[FittingStep]\n\n\n\nmodel\nType[Model]",
    "crumbs": [
      "Reference",
      "API Reference",
      "espei.parameter_selection.fitting_descriptions",
      "parameter_selection.fitting_descriptions.ModelFittingDescription"
    ]
  },
  {
    "objectID": "reference/yaml_input_schema.html",
    "href": "reference/yaml_input_schema.html",
    "title": "YAML Input Schema",
    "section": "",
    "text": "This page aims to completely describe the ESPEI input file in the YAML format. Possibly useful links are the YAML refcard and the (possibly less useful) Full YAML specification. These are all key value pairs in the format\nThey are nested for purely organizational purposes.\nAs long as keys are nested under the correct heading, they have no required order. All of the possible keys are\nThe next sections describe each of the keys individually. If a setting has a default of required it must be set explicitly.",
    "crumbs": [
      "Reference",
      "YAML Input Schema"
    ]
  },
  {
    "objectID": "reference/yaml_input_schema.html#system",
    "href": "reference/yaml_input_schema.html#system",
    "title": "YAML Input Schema",
    "section": "system",
    "text": "system\nThe system key is intended to describe the specific system you are fitting, including the components, phases, and the data to fit to.\n\nphase_models\n\ntype\n\nstring\n\ndefault\n\nrequired\n\n\nThe JSON file describing the Calphad models for each phase. See the phase models description for an example of how to write this file.\n\n\ndatasets\n\ntype\n\nstring\n\ndefault\n\nrequired\n\n\nThe path to a directory containing JSON files of input datasets. The file extension to each of the datasets must be named as .json, but they can otherwise be named freely.\nFor an examples of writing these input JSON files, see the dataset schema page.\n\n\ntags\n\ntype\n\ndict\n\ndefault\n\nrequired\n\n\nMapping of keys to values to add to datasets with matching tags. These can be used to dynamically drive values in datasets without adjusting the datasets themselves. Useful for adjusting weights or other values in datasets in bulk. For an examples of using tags in input JSON files, see the Tags section in the dataset schema.",
    "crumbs": [
      "Reference",
      "YAML Input Schema"
    ]
  },
  {
    "objectID": "reference/yaml_input_schema.html#output",
    "href": "reference/yaml_input_schema.html#output",
    "title": "YAML Input Schema",
    "section": "output",
    "text": "output\n\nverbosity\n\ntype\n\nint\n\ndefault\n\n0\n\n\nControls the logging level. Most users will probably want to use Info or Trace.\nWarning logs should almost never occur and this log level will be relatively quiet. Debug is a fire hose of information, but may be useful in fixing calculation errors or adjusting weights.\n\n\n\nValue\nLog Level\n\n\n\n\n0\nWarning\n\n\n1\nInfo\n\n\n2\nTrace\n\n\n3\nDebug\n\n\n\n\n\nlogfile\n\ntype\n\nstring\n\ndefault\n\nnull\n\n\nName of the file that the logs (controlled by verbosity) will be output to. The default is None (in Python, null in YAML), meaning the logging will be output to stdout and stderr.\n\n\noutput_db\n\ntype\n\nstring\n\ndefault\n\nout.tdb\n\n\nThe database to write out. Can be any file format that can be written by a pycalphad Database.\n\n\ntracefile\n\ntype\n\nstring\n\ndefault\n\ntrace.npy\n\n\nName of the file that the MCMC trace is written to. The array has shape (number of chains, iterations, number of parameters).\nThe array is preallocated and padded with zeros, so if you selected to take 2000 MCMC iterations, but only got through 1500, the last 500 values would be all 0.\nYou must choose a unique file name. An error will be raised if file specified by tracefile already exists. If you don't want a file to be output (e.g. for debugging), you can enter null.\n\n\nprobfile\n\ntype\n\nstring\n\ndefault\n\nlnprob.npy\n\n\nName of the file that the MCMC ln probabilities are written to. The array has shape (number of chains, iterations).\nThe array is preallocated and padded with zeros, so if you selected to take 2000 MCMC iterations, but only got through 1500, the last 500 values would be all 0.\nYou must choose a unique file name. An error will be raised if file specified by probfile already exists. If you don't want a file to be output (e.g. for debugging), you can enter null.",
    "crumbs": [
      "Reference",
      "YAML Input Schema"
    ]
  },
  {
    "objectID": "reference/yaml_input_schema.html#generate_parameters",
    "href": "reference/yaml_input_schema.html#generate_parameters",
    "title": "YAML Input Schema",
    "section": "generate_parameters",
    "text": "generate_parameters\nThe options in generate_parameters are used to control parameter selection and fitting to single phase data. This should be used if you have input thermochemical data, such as heat capacities and mixing energies.\nGenerate parameters will use the Akaike information criterion to select model parameters and fit them, creating a database.\n\nexcess_model\n\ntype\n\nstring\n\ndefault\n\nrequired\n\noptions\n\nlinear\n\n\nWhich type of model to use for excess mixing parameters. Currently only linear is supported.\nThe exponential model is planned, as well as support for custom models.\n\n\nref_state\n\ntype\n\nstring\n\ndefault\n\nrequired\n\noptions\n\nSGTE91 | SR2016\n\n\nThe reference state to use for the pure elements and lattice stabilities. Currently only SGTE91 and SR2016 (for certain elements) is supported.\nThere are plans to extend to support custom reference states.\n\n\nridge_alpha\n\ntype\n\nfloat\n\ndefault\n\n1.0e-100\n\n\nControls the ridge regression hyperparameter, \\(\\alpha\\), as given in the following equation for the ridge regression problem\n\\[\\min_w || Xw - y||^2_2 + \\alpha ||w||^2_2\\]\nridge_alpha should be a positive floating point number which scales the relative contribution of parameter magnitudes to the residuals.\nIf an exponential form is used, the floating point value must have a decimal place before the e, that is 1e-4 is invalid while 1.e-4 is valid. More generally, the floating point must match the following regular expression per the YAML 1.1 spec: [-+]?([0-9][0-9_]*)?\\.[0-9.]*([eE][-+][0-9]+)?.\n\n\naicc_penalty_factor\n\ntype\n\ndict\n\ndefault\n\nnull\n\n\nThis parameter is a mapping from a phase name and property to a penalty factor to apply to the AICc number of parameters. The default is null, which means that all the penalty factors are one (1) for all phases, which means no bias for more or fewer parameters compared to the textbook definition of AICc. If phases or data are not included, the penalty factors remain one.\nIncreasing the penalty factor will increase the penalty for more parameters, so it will bias parameter selection to choose fewer parameters. This can be especially useful when there is not many data points and an exact fit is possible (e.g. 4 points and 4 parameters), but modeling intutition would suggest that fewer parameters are required. A negative penalty factor will bias ESPEI's parameter selection to choose more parameters, which can be useful for generating more degrees of freedom for MCMC.\naicc_penalty_factor:\n  BCC_A2:\n    HM: 5.0\n    SM: 5.0\n  LIQUID:\n    HM: 2.0\n    SM: 2.0\n\n\ninput_db\n\ntype\n\nstring\n\ndefault\n\nnull\n\n\nA file path that can be read as a pycalphad Database, which can provide existing parameters to add as a starting point for parameter generation, for example magnetic parameters.\nIf you have single phase data, ESPEI will try to fit parameters to that data regardless of whether or not parameters were passed in for that phase. You must be careful to only add initial parameters that do not have data that ESPEI will try to fit. For example, do not include liquid enthalpy of mixing data for ESPEI to fit if you are providing an initial set of parameters.\n\n\nfitting_description\n\ntype\n\nstring\n\ndefault\n\nespei.parameter_selection.fitting_descriptions.gibbs_energy_fitting_description\n\n\nThis is a string for the fully qualified import path for a ModelFittingDescription instance. ESPEI currently provides three built-in fitting descriptions that are available to use:\n# the default, fit Gibbs energy (G/L) parameters to HM/SM/CPM data:\nfitting-description: espei.parameter_selection.fitting_descriptions.gibbs_energy_fitting_description\n# fit molar volume (V0 and VA) parameters to V0/VM data:\nfitting-description: espei.parameter_selection.fitting_descriptions.molar_volume_fitting_description\n# a combination that fits volume and Gibbs energy parameters\nfitting-description: espei.parameter_selection.fitting_descriptions.molar_volume_gibbs_energy_fitting_description\nThe ModelFittingDescription instance can be from any module that is importable from the Python environment where ESPEI is installed. For example, the Gibbs energy fitting description is importable as:\nfrom espei.parameter_selection.fitting_descriptions import gibbs_energy_fitting_description\nBecause the fitting description given can be in any importable module, users can provide their own fitting descriptions that use ESPEI's built-in FittingStep objects, custom fitting steps, or a combination ESPEI's fitting steps and custom fitting steps. The Generating Custom Model Parameters tutorial gives an example of how to fit a custom PyCalphad model that uses custom parameters for BCC elastic constants.",
    "crumbs": [
      "Reference",
      "YAML Input Schema"
    ]
  },
  {
    "objectID": "reference/yaml_input_schema.html#mcmc",
    "href": "reference/yaml_input_schema.html#mcmc",
    "title": "YAML Input Schema",
    "section": "mcmc",
    "text": "mcmc\nThe options in mcmc control how Markov Chain Monte Carlo is performed using the emcee package.\nIn order to run an MCMC fitting, you need to specify one and only one source of parameters somewhere in the input file. The parameters can come from including a generate_parameters step, or by specifying the mcmc.input_db key with a file to load as pycalphad Database.\nIf you choose to use the parameters from a database, you can then further control settings based on whether it is the first MCMC run for a system (you are starting fresh) or whether you are continuing from a previous run (a 'restart').\n\niterations\n\ntype\n\nint\n\ndefault\n\nrequired\n\n\nNumber of iterations to perform in emcee. Each iteration consists of accepting one step for each chain in the ensemble.\n\n\nprior\n\ntype\n\nlist or dict\n\ndefault\n\n{'name': 'zero'}\n\n\nEither a single prior dictionary or a list of prior dictionaries corresponding to the number of parameters. See Specifying Priors for examples and details on writing priors.\n\n\nsave_interval\n\ntype\n\nint\n\ndefault\n\n1\n\n\nControls the interval (in number of iterations) for saving the MCMC chain and probability files. By default, new files will be written out every iteration. For large files (many mcmc iterations and chains per parameter), these might become expensive to write out to disk.\n\n\ncores\n\ntype\n\nint\n\nmin\n\n1\n\n\nHow many cores from available cores to use during parallelization with dask or emcee. If the chosen number of cores is larger than available, then this value is ignored and espei defaults to using the number available.\nCores does not take affect for MPIPool scheduler option. MPIPool requires the number of processors be set directly with MPI.\n\n\nscheduler\n\ntype\n\nstring\n\ndefault\n\ndask\n\noptions\n\ndask | null | JSON file\n\n\nWhich scheduler to use for parallelization. You can choose from either dask, null, or pass the path to a JSON scheduler file created by dask-distributed.\nChoosing dask allows for the choice of cores used through the cores key.\nChoosing null will result in no parallel scheduler being used. This is useful for debugging.\nPassing the path to a JSON scheduler file will use the resources set up by the scheduler. JSON file schedulers are most useful because schedulers can be started on MPI clusters using dask-mpi command. See the alternative schedulers page for more information.\n\n\ninput_db\n\ntype\n\nstring\n\ndefault\n\nnull\n\n\nA file path that can be read as a pycalphad Database. The parameters to fit will be taken from this database.\nFor a parameter to be fit, it must be a symbol where the name starts with VV, e.g. VV0001. For a TDB formatted database, this means that the free parameters must be functions of a single value that are used in your parameters. For example, the following is a valid symbol to fit:\nFUNCTION VV0000  298.15  10000; 6000 N !\n\n\nrestart_trace\n\ntype\n\nstring\n\ndefault\n\nnull\n\n\nIf you have run a previous MCMC calculation, then you will have a trace file that describes the position and history of all of the chains from the run. You can use these chains to start the emcee run and pick up from where you left off in the MCMC run by passing the trace file (e.g. chain.npy) to this key.\nIf you are restarting from a previous calculation, you must also specify the same database file (with input_db) as you used to run that calculation.\n\n\nchains_per_parameter\n\ntype\n\nint\n\ndefault\n\n2\n\n\nThis controls the number of chains to run in the MCMC calculation as an integer multiple of the number of parameters.\nThis parameter can only be used when initializing the first MCMC run. If you are restarting a calculation, the number of chains per parameter is fixed by the number you chose previously.\nEnsemble samplers require at least 2*p chains for p fitting parameters to be able to make proposals. If chains_per_parameter = 2, then the number of chains if there are 10 parameters to fit is 20.\nThe value of chains_per_parameter must be an EVEN integer.\n\n\nchain_std_deviation\n\ntype\n\nfloat\n\ndefault\n\n0.1\n\n\nThe standard deviation to use when initializing chains in a Gaussian distribution from a set of parameters as a fraction of the parameter.\nA value of 0.1 means that for parameters with values (-1.5, 2000, 50000) the chains will be initialized using those values as the mean and (0.15, 200, 5000) as standard deviations for each parameter, respectively.\nThis parameter can only be used when initializing the first MCMC run. If you are restarting a calculation, the standard deviation for your chains are fixed by the value you chose previously.\nYou may technically set this to any positive value, you would like. Be warned that too small of a standard deviation may cause convergence to a local minimum in parameter space and slow convergence, while a standard deviation that is too large may cause convergence to meaningless thermodynamic descriptions.\n\n\napproximate_equilibrium\n\ntype\n\nbool\n\ndefault\n\nFalse\n\n\nIf True, an approximate version of pycalphad's equilibrium() function will be used to calculate the driving force for phase boundary data. It uses pycalphad's starting_point to construct a approximate equilibrium hyperplanes of the lowest energy solution from a numerical sampling of each active phases's internal degrees of freedom. This can give speedups of up to 10x for calculating the ZPF likelihood, but may miss low energy solutions that are not sampled well numerically, especially for phases with many sublattices, which have low energy solutions far from the endmembers.\n\n\ndeterministic\n\ntype\n\nbool\n\ndefault\n\nTrue\n\n\nToggles whether ESPEI runs are deterministic. If this is True, running ESPEI with the same Database and initial settings (either the same chains_per_parameter and chain_std_deviation or the same restart_trace) will result in exactly the same results.\nStarting two runs with the same TDB or with parameter generation (which is deterministic) will result in the chains being at exactly the same position after 100 iterations. If these are both restarted after 100 iterations for another 50 iterations, then the final chain after 150 iterations will be the same.\nIt is important to note that this is only explictly True when starting at the same point. If Run 1 and Run 2 are started with the same initial parameters and Run 1 proceeds 50 iterations while Run 2 proceeds 100 iterations, restarting Run 1 for 100 iterations and Run 2 for 50 iterations (so they are both at 150 total iterations) will NOT give the same result.\n\n\ndata_weights\n\ntype\n\ndict\n\ndefault\n\n{'ZPF': 1.0, 'ACR': 1.0, 'HM': 1.0, 'SM': 1.0, 'CPM': 1.0}\n\n\nEach type of data can be weighted: zero phase fraction (ZPF), activity (ACR) and the different types of thermochemical error. These weights are used to modify the initial standard deviation of each data type by\n\\[\\sigma = \\frac{\\sigma_{\\mathrm{initial}}} {w}\\]\n\n\nsymbols\n\ntype\n\nlist[str]\n\ndefault\n\nnull\n\n\nBy default, any symbol in the database following the naming pattern VV#### where #### is any number is optimized by ESPEI. If this option is set, this can be used to manually fit a subset of the degrees of freedom in the system, or fit degrees of freedom that do not folow the naming convention of 'VV####':\nsymbols: ['VV0000', 'FF0000', ...]",
    "crumbs": [
      "Reference",
      "YAML Input Schema"
    ]
  },
  {
    "objectID": "how-to/specifying_priors.html",
    "href": "how-to/specifying_priors.html",
    "title": "MCMC Priors",
    "section": "",
    "text": "In Bayesian statistics, data are used to update prior distributions for all parameters to calculate posterior distributions. A basic introduction to priors and Bayesian statistics can be found in \"Kruschke, J. (2014). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press.\". A more advanced treatment is given in \"Gelman, A., Stern, H. S., Carlin, J. B., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis. Chapman and Hall/CRC.\".\nESPEI provides a flexible interface to specify priors your want to use for a variety of parameters of different sign and magnitude through the espei.priors.PriorSpec class. This section will cover how to",
    "crumbs": [
      "How-To",
      "MCMC Priors"
    ]
  },
  {
    "objectID": "how-to/specifying_priors.html#built-in-priors",
    "href": "how-to/specifying_priors.html#built-in-priors",
    "title": "MCMC Priors",
    "section": "Built-in Priors",
    "text": "Built-in Priors\nESPEI has several built-in priors that correspond to functions in scipy.stats: uniform, normal, and triangular. There is also a special (improper) zero prior that always gives \\(\\ln p = 0\\), which is the default.\nEach scipy.stats prior is typically specified using several keyword argument parameters, e.g. loc and scale, which have special meaning for the different distribution functions. In order to be flexible to specifying these arguments when the Calphad parameters they will be used for are not known beforehand, ESPEI uses a small language to specify how the distribution hyperparameters can be set relative to the Calphad parameters.\nBasically, the PriorSpec objects are created with the name of the distribution and the hyperparameters that are modified with one of the modifier types: absolute, relative, shift_absolute, or shift_relative. For example, the loc parameter might become loc_relative and scale might become scale_shift_relative.\nHere are some examples of how the modifier parameters of value v modify the hyperparameters when given a Calphad parameter of value p:\n\n_absolute=v always take the exact value passed in, v; loc_absolute=-20 gives a value of loc=-20.\n_relative=v gives , v*p; scale_absolute=0.1 with p=10000 gives a value of scale=10000*0.1=1000.\n_shift_absolute=v gives , p + v; scale_shift_absolute=-10 with p=10000 gives a value of scale=10000+(-10)=9990.\n_shift_relative=v gives , p + v*abs(p); loc_shift_relative=-0.5 with p=-1000 gives a value of loc=-1000+abs(-1000)*0.5=-1500.\n\nNote that the hyperparameter prefixes (loc or scale) are arbitrary and any hyperparameters used in the statistical distributions (e.g. c for the triangular distribution) could be used.\n\nYAML\nPrior can be specified in the YAML input as a list of dictionaries for different parameters. Since Python objects cannot be instantiated in the YAML files, the PriorSpec can be described a dictionary of {'name': &lt;&lt;NAME&gt;&gt;, 'loc_relative': &lt;&lt;VALUE&gt;&gt;, 'scale_relative': &lt;&lt;VALUE&gt;&gt;, ...}.\nSome common examples in YAML format are as follows:\n# normal prior, centered on parameter, standard deviation of 0.25*parameter\nmcmc:\n  prior:\n    name: normal\n    loc_relative: 1.0\n    scale_relative: 0.25\n\n# uniform prior from 0 to 2*parameter (or negative(2*parameter) to 0)\nmcmc:\n  prior:\n    name: uniform\n    loc_shift_relative: -1.0\n    scale_relative: 2.0\n\n# triangular prior, centered on parameter, from -0.5*parameter to 0.5*parameter\nmcmc:\n  prior:\n    name: triangular\n    loc_shift_relative: -0.5\n    scale_relative: 1.0\nGraphically, these are shown below:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom espei.priors import PriorSpec\n\ninitial_parameters = PriorSpec(name=\"normal\", loc_relative=1.0, scale_relative=0.10)\nnormal_prior = PriorSpec(name=\"normal\", loc_relative=1.0, scale_relative=0.25)\nuniform_prior = PriorSpec(name=\"uniform\", loc_shift_relative=-1.0, scale_relative=2.0)\ntriangular_prior = PriorSpec(name=\"triangular\", loc_shift_relative=-0.5, scale_relative=1.0)\n\ninitial_param_value = 10000\nparam_values = np.linspace(-5000, 25000, 1001)\n\nfig, ax = plt.subplots()\nax.plot(param_values, initial_parameters.get_prior(initial_param_value).pdf(param_values), label=\"initialization, chain $\\\\sigma$=0.1\")\nax.plot(param_values, normal_prior.get_prior(initial_param_value).pdf(param_values), label=\"normal prior\")\nax.plot(param_values, uniform_prior.get_prior(initial_param_value).pdf(param_values), label=\"uniform prior\")\nax.plot(param_values, triangular_prior.get_prior(initial_param_value).pdf(param_values), label=\"triangular prior\")\nax.set_xlabel(\"Parmeter value\")\nax.set_ylabel(\"Probability density\")\nax.set_title(f\"Initial value: {initial_param_value}\")\nax.legend()\nfig.show()\n\n\n\n\n\n\n\n\n\nAs a side note: the priors in YAML files can be passed as Python dictionaries, e.g.:\n# normal prior, centered on parameter, standard deviation of 0.25*parameter\nprior: {'name': 'normal', 'loc_relative': 1.0, 'scale_relative': 0.5}\nAdditionally, different priors can be specified using a list of prior specs that match the total degrees of freedom (VV-parameters) in the system. For example, a two parameter system could use a normal and a triangular prior simultaneously:\n# two priors:\n# first a normal prior, centered on parameter, standard deviation of 0.25*parameter\n# second a triangular prior, centered on parameter, from -0.5*parameter to 0.5*parameter\nprior: [{'name': 'normal', 'loc_relative': 1.0, 'scale_relative': 0.5}, {'name': 'triangular', 'loc_shift_relative': -0.5, 'scale_relative': 1.0}]",
    "crumbs": [
      "How-To",
      "MCMC Priors"
    ]
  },
  {
    "objectID": "how-to/specifying_priors.html#custom-priors",
    "href": "how-to/specifying_priors.html#custom-priors",
    "title": "MCMC Priors",
    "section": "Custom Priors",
    "text": "Custom Priors\nGenerally speaking, a custom prior in ESPEI is any Python object that has a logpdf method that takes a parameter and returns the natural log of the probability density function for that parameter. Any distribution you can create using the functions in scipy.stats, such as norm, is valid.\nA list of these custom priors can be passed to ESPEI similar to using built-in priors, but only from the Python interface (not YAML). The number of priors must match the number of parameters, but you can also mix these with the PriorSpec objects as desired.\nAn example of fitting two parameters using a custom gamma distributions with minima at 10 and 100, respectively.\nfrom scipy.stats import gamma\n\nmy_priors = [gamma(a=1, loc=10), gamma(a=1, loc=100)]\n\nfrom espei.espei_script import get_run_settings, run_espei\n\n\ninput_dict = {\n    'system': {\n        'phase_models': 'phases.json',\n        'datasets': 'input-data',\n    },\n    'mcmc': {\n        'iterations': '1000',\n        'input_db': 'param_gen.tdb',  # must have two parameters to fit\n        'prior': my_priors,\n    },\n}\n\nrun_espei(get_run_settings(input_dict))",
    "crumbs": [
      "How-To",
      "MCMC Priors"
    ]
  },
  {
    "objectID": "how-to/schedulers.html",
    "href": "how-to/schedulers.html",
    "title": "Alternative Schedulers",
    "section": "",
    "text": "ESPEI uses dask-distributed for parallelization and provides an easy way to deploy clusters locally via TCP with the mcmc.scheduler: dask setting.\nSometimes ESPEI's dask scheduling options are not sufficiently flexible for different environments.\nAs an alternative to setting the cores with the mcmc.scheduler: dask setting, you can provide ESPEI with a scheduler file from dask that has information about how to connect to a dask parallel scheduler.\nThis is generally a two step process of\nIn order to let the system manage the memory and prevent dask from pausing or killing workers, the memory limit should be set to zero.",
    "crumbs": [
      "How-To",
      "Alternative Schedulers"
    ]
  },
  {
    "objectID": "how-to/schedulers.html#starting-a-scheduler",
    "href": "how-to/schedulers.html#starting-a-scheduler",
    "title": "Alternative Schedulers",
    "section": "Starting a scheduler",
    "text": "Starting a scheduler\n\nMPI-based dask scheduler\nDask provides a dask-mpi package that sets this up for you and creates a scheduler file to pass to ESPEI. The scheduler information will be serialized as a JSON file that you set in your ESPEI input file.\nThe dask-mpi package (version 2.0.0 or greater) must be installed before you can use it:\nconda install -c conda-forge --yes \"dask-mpi&gt;=2\"\nNote that you may also need a particular MPI implementation, conda-forge provides packages for OpenMPI or MPICH. You can pick a particular one by installing dask-mpi using either:\nconda install -c conda-forge --yes \"dask-mpi&gt;=2\" \"mpi=*=openmpi\"\nor\nconda install -c conda-forge --yes \"dask-mpi&gt;=2\" \"mpi=*=mpich\"\nor let conda pick one for you by not including any.\nTo start the scheduler and workers in the background, you can run the dask-mpi command (use dask-mpi --help to check the arguments). The following command will start a scheduler on the main MPI task, then a worker for each remaining MPI task that mpirun sees.\nmpirun dask-mpi --scheduler-file my_scheduler.json --nthreads 1 --memory-limit 0 &\n\n\nGeneric scheduler\nIf you need further customization of dask schedulers, you can start a distributed Client any way you like, then write out the scheduler file for ESPEI to use.\nFor example, if you name the following file start_scheduler.py, you can run this Python script in the background, which will contain the scheduler and workers, then ESPEI will connect to it.\n# start_scheduler.py\nfrom distributed import Client, LocalCluster\nfrom tornado.ioloop import IOLoop\n\nif __name__ == '__main__':\n    loop = IOLoop()\n    cluster = LocalCluster(n_workers=4, threads_per_worker=1, memory_limit=0)\n    client = Client(cluster)\n    client.write_scheduler_file('my-scheduler.json')\n    loop.start()  # keeps the scheduler running\n    loop.close()\nRunning start_scheduler.py &, will run this process in the background with 4 processes.",
    "crumbs": [
      "How-To",
      "Alternative Schedulers"
    ]
  },
  {
    "objectID": "how-to/schedulers.html#espei-input",
    "href": "how-to/schedulers.html#espei-input",
    "title": "Alternative Schedulers",
    "section": "ESPEI Input",
    "text": "ESPEI Input\nAfter starting the scheduler on the cluster, you run ESPEI like normal.\nFor the most part, this ESPEI input file is the same as you use locally, except the scheduler parameter is set to the name of your scheduler file.\nHere is an example for Bayesian parameter estimation using MCMC starting from a generated TDB with a scheduler file named my-scheduler.json:\nsystem:\n  phase_models: my-phases.json\n  datasets: my-input-data\nmcmc:\n  iterations: 1000\n  input_db: my-tdb.tdb\n  scheduler: my-scheduler.json",
    "crumbs": [
      "How-To",
      "Alternative Schedulers"
    ]
  },
  {
    "objectID": "how-to/schedulers.html#example-queue-script---mpi",
    "href": "how-to/schedulers.html#example-queue-script---mpi",
    "title": "Alternative Schedulers",
    "section": "Example Queue Script - MPI",
    "text": "Example Queue Script - MPI\nTo run on through a queueing system, you'll often use queue scripts that start batch jobs.\nThis example will create an MPI scheduler using dask-mpi via mpirun (or other MPI executable). Since many MPI jobs are run through batch schedulers, an example script for a PBS job looks like:\n#!/bin/bash\n\n#PBS -l nodes=1:ppn=20\n#PBS -l walltime=48:00:00\n#PBS -A open\n#PBS -N espei-mpi\n#PBS -o espei-mpi.out\n#PBS -e espei-mpi.error\n\n# starts the scheduler on MPI and creates the scheduler file called 'my_scheduler.json'\n# you can replace this line with any script that starts a scheduler\n# e.g. a `start_scheduler.py` file\n# make sure it ends with `&` to run the process in the background\nmpirun dask-mpi --scheduler-file my_scheduler.json --nthreads 1 --memory-limit 0 &\n\n# runs ESPEI as normal\nespei --in espei-mpi-input.yaml\n\nSee also\nSee https://docs.dask.org/en/latest/setup/hpc.html for more details on using dask on HPC machines.",
    "crumbs": [
      "How-To",
      "Alternative Schedulers"
    ]
  },
  {
    "objectID": "how-to/recipes/zpf-driving-forces/index.html",
    "href": "how-to/recipes/zpf-driving-forces/index.html",
    "title": "Plot ZPF Driving Forces",
    "section": "",
    "text": "This visualization can be used as a diagnostic for understanding which ZPF data are contributing the most driving force towards the likelihood. Note that these driving forces are unweighted, since the weight is applied when computing the log-likelihood of each driving force.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pycalphad import Database, binplot, variables as v\nfrom pycalphad.core.utils import extract_parameters\nfrom espei.datasets import load_datasets, recursive_glob\nfrom espei.error_functions.zpf_error import get_zpf_data, calculate_zpf_driving_forces\n\ndbf = Database(\"Cr-Ni_mcmc.tdb\")\ncomps = [\"CR\", \"NI\", \"VA\"]\nphases = list(dbf.phases.keys())\nindep_comp_cond = v.X(\"NI\")  # binary assumed\nconditions = {v.N: 1, v.P: 101325, v.T: (500, 2200, 20), indep_comp_cond: (0, 1, 0.01)}\nparameters = {}  # e.g. {\"VV0001\": 10000.0}, if empty, will use the current database parameters\n\n# Get the datasets, construct ZPF data and compute driving forces\n# Driving forces and weights are ragged 2D arrays of shape (len(zpf_data), len(vertices in each zpf_data))\ndatasets = load_datasets(recursive_glob(\"input-data\"))\nzpf_data = get_zpf_data(dbf, comps, phases, datasets, parameters=parameters)\nparam_vec = extract_parameters(parameters)[1]\ndriving_forces, weights = calculate_zpf_driving_forces(zpf_data, param_vec)\n\n# Construct the plotting compositions, temperatures and driving forces\n# Each should have len() == (number of vertices)\n# Driving forces already have the vertices unrolled so we can concatenate directly\nXs = []\nTs = []\ndfs = []\nfor data, data_driving_forces in zip(zpf_data, driving_forces):\n    for phase_region in data[\"phase_regions\"]:\n        for vertex, df in zip(phase_region.vertices, data_driving_forces):\n            comp_cond = vertex.comp_conds\n            if vertex.has_missing_comp_cond:\n                # No composition to plot\n                continue\n            dfs.append(df)\n            Ts.append(phase_region.potential_conds[v.T])\n            # Binary assumptions here\n            assert len(comp_cond) == 1\n            if indep_comp_cond in comp_cond:\n                Xs.append(comp_cond[indep_comp_cond])\n            else:\n                # Switch the dependent and independent component\n                Xs.append(1.0 - tuple(comp_cond.values())[0])\n\n# Plot the phase diagram with driving forces\nfig, ax = plt.subplots(dpi=100, figsize=(8,4))\nbinplot(dbf, comps, phases, conditions, plot_kwargs=dict(ax=ax), eq_kwargs={\"parameters\": parameters})\nsm = plt.cm.ScalarMappable(cmap=\"hot\")\nsm.set_array(dfs)\nax.scatter(Xs, Ts, c=dfs, cmap=\"hot\", edgecolors=\"k\")\nfig.colorbar(sm, ax=ax, pad=0.25, label=\"Driving Force\")\nfig.show()\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "How-To",
      "Recipes",
      "Plot ZPF Driving Forces"
    ]
  },
  {
    "objectID": "how-to/recipes/mcmc-probability-convergence/index.html",
    "href": "how-to/recipes/mcmc-probability-convergence/index.html",
    "title": "MCMC Probability Convergence Plots",
    "section": "",
    "text": "Convergence can be qualitatively estimated by looking at how the log-probability changes for all of the chains as a function of iterations.\nSome metrics exist to help estimate convergence, but they should be used with caution, especialy ones that depend on multiple independent chains, as ensemble MCMC methods like the one used by ESPEI do not have independent chains.\nPlotting these can also be helpful to estimate how many iterations to discard as burn-in, e.g. in the corner plot example.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nlnprob = np.load(\"lnprob.npy\")\n# the following lines are optional, but useful if your traces are not full\n# (i.e. your MCMC runs didn't run all their steps)\n# trace = np.load(\"trace.npy\")\n# from espei.analysis import truncate_arrays\n# trace, lnprob = truncate_arrays(trace, lnprob)\n\nfig, ax = plt.subplots()\nax.plot(lnprob.T)\nax.set_title(\"log-probability convergence\")\nax.set_xlabel(\"iterations\")\nax.set_ylabel(\"lnprob\")\nax.set_yscale(\"symlog\")  # log-probabilties are often negative, symlog gives log scale for negative numbers\nfig.show()\n\n\n\n\n\n\n\n\nwe can zoom in using a linear scale to inspect more closely:\n\nfig, ax = plt.subplots()\nax.plot(lnprob.T)\nax.set_title(\"log-probability convergence\")\nax.set_xlabel(\"iterations\")\nax.set_ylabel(\"lnprob\")\nax.set_ylim(-3000, -2700)\nfig.show()\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "How-To",
      "Recipes",
      "MCMC Probability Convergence Plots"
    ]
  },
  {
    "objectID": "how-to/recipes/optimal_parameters/index.html",
    "href": "how-to/recipes/optimal_parameters/index.html",
    "title": "Get optimal MCMC parameters",
    "section": "",
    "text": "Load an existing database and optimal parameters from a tracefile and probfile:\n\nimport numpy as np\nfrom pycalphad import Database\nfrom espei.analysis import truncate_arrays\nfrom espei.utils import database_symbols_to_fit, optimal_parameters\n\ntrace = np.load(\"trace.npy\")\nlnprob = np.load(\"lnprob.npy\")\ntrace, lnprob = truncate_arrays(trace, lnprob)\n\ndbf = Database(\"Cu-Mg-generated.tdb\")\nopt_params = dict(zip(database_symbols_to_fit(dbf), optimal_parameters(trace, lnprob)))\n# Update the symbols in the database, this database can now be used in PyCalphad calculations\ndbf.symbols.update(opt_params)\n# dbf.to_file(\"Cu-Mg-opt.tdb\")  # if desired, write out the database\n\n\n\n\n Back to top",
    "crumbs": [
      "How-To",
      "Recipes",
      "Get optimal MCMC parameters"
    ]
  },
  {
    "objectID": "developer/contributing.html",
    "href": "developer/contributing.html",
    "title": "Contributing Guide",
    "section": "",
    "text": "This is the place to start as a new ESPEI contributor. This guide assumes you have installed a development version of ESPEI.\nThe next sections lay out the basics of getting an ESPEI development set up and the development standards. Then the Software design sections walk through the key parts of the codebase.",
    "crumbs": [
      "Developer",
      "Contributing Guide"
    ]
  },
  {
    "objectID": "developer/contributing.html#tests",
    "href": "developer/contributing.html#tests",
    "title": "Contributing Guide",
    "section": "Tests",
    "text": "Tests\nEven though much of ESPEI is devoted to being a multi-core, stochastic user tool, we strive to test all logic and functionality. We are continuously maintaining tests and writing tests for previously untested code. As a general rule, any time you write a new function or modify an existing function you should write or maintain a test for that function.\nSome tips for testing:\n\nIdeally you would practicing test driven development by writing tests of your intended results before you write the function.\nIf possible, keep the tests small and fast.\nSee the NumPy/SciPy testing guidelines for more tips.\n\n\nRunning Tests\nESPEI uses pytest as a test runner. The tests can be run from the root directory of the cloned repository:\npytest",
    "crumbs": [
      "Developer",
      "Contributing Guide"
    ]
  },
  {
    "objectID": "developer/contributing.html#style",
    "href": "developer/contributing.html#style",
    "title": "Contributing Guide",
    "section": "Style",
    "text": "Style\n\nCode style\nFor most naming and style, follow PEP8. One exception to PEP8 is regarding the line length, which we suggest a 120 character maximum, but may be longer within reason.\n\n\nCode documentation\nESPEI uses the NumPy documentation style. All functions and classes should be documented with at least a description, parameters, and return values, if applicable.\nUsing Examples in the documentation is especially encouraged for utilities that are likely to be run by users. See espei.analysis.truncate_arrays for an example.\n\n\nWeb documention\nDocumentation on ESPEI is split into user tutorials, reference and developer documentation.\n\nTutorials are resources for users new to ESPEI or new to certain features of ESPEI to be guided through typical actions.\nReference pages should be concise articles that explain how to complete specific goals for users who know what they want to accomplish.\nDeveloper documentation should describe what should be considered when contributing source code back to ESPEI.\n\nYou can check changes you make to the documentation by going to the documentation folder in the root repository cd docs/. Running the command make html && cd build/html && python3 -m http.server && cd ../.. && make clean from that folder will build the docs and run them on a local HTTP server. You can see the documentation when the server is running by visting the URL at the end of the output, usually localhost port 8000 &lt;http://0.0.0.0:8000&gt;_. When you are finished, type Ctrl-C to stop the server and the command will clean up the build for you.\nMake sure to fix any warnings that come up if you are adding documentation.\n\nBuilding Documentation\nThe docs can be built by running the docs/Makefile (or docs/make.bat on Windows). Then Python can be used to serve the html files in the _build directory and you can visit http://localhost:8000 in your broswer to see the built documentation.\nFor Unix systems:\ncd docs\nmake html\ncd _build/html\npython -m http.server\nWindows:\ncd docs\nmake.bat html\ncd _build\\html\npython -m http.server",
    "crumbs": [
      "Developer",
      "Contributing Guide"
    ]
  },
  {
    "objectID": "developer/contributing.html#logging",
    "href": "developer/contributing.html#logging",
    "title": "Contributing Guide",
    "section": "Logging",
    "text": "Logging\nSince ESPEI is intended to be run by users, we must provide useful feedback on how their runs are progressing. ESPEI uses the logging module to allow control over verbosity of the output.\nThere are 5 different logging levels provided by Python. They should be used as follows:\n\nCritical or Error (logging.critical or logging.error)\n\nNever use these. These log levels would only be used when there is an unrecoverable error that requires the run to be stopped. In that case, it is better to raise an appropriate error instead.\n\nWarning (logging.warning)\n\nWarnings are best used when we are able to recover from something bad that has happened. The warning should inform the user about potentially incorrect results or let them know about something they have the potential to fix. Again, anything unrecoverable should not be logged and should instead be raised with a good error message.\n\nInfo (logging.info)\n\nInfo logging should report on the progress of the program. Usually info should give feedback on milestones of a run or on actions that were taken as a result of a user setting. An example of a milestone is starting and finishing parameter generation. An example of an action taken as a result of a user setting is the logging of the number of chains in an mcmc run.\n\nDebug (logging.debug)\n\nDebugging is the lowest level of logging we provide in ESPEI. Debug messages should consist of possibly useful information that is beyond the user's direct control. Examples are the values of initial parameters, progress of checking datasets and building phase models, and the acceptance ratios of MCMC iterations.",
    "crumbs": [
      "Developer",
      "Contributing Guide"
    ]
  },
  {
    "objectID": "developer/design.html",
    "href": "developer/design.html",
    "title": "Design Guidelines",
    "section": "",
    "text": "The following sections elaborate on the design principles on the software side. The goal is to make it clear how different modules in ESPEI fit together and where to find specific functionality to override or improve.\nESPEI provides tools to",
    "crumbs": [
      "Developer",
      "Design Guidelines"
    ]
  },
  {
    "objectID": "developer/design.html#api",
    "href": "developer/design.html#api",
    "title": "Design Guidelines",
    "section": "API",
    "text": "API\nESPEI has two levels of API that users should expect to interact with:\n\nInput from YAML files on the command line (via espei --input &lt;input_file&gt; or by Python via the espei.espei_script.run_espei function\nWork directly with the Python functions for parameter selection espei.paramselect.generate_parameters and MCMC espei.mcmc.mcmc_fit\n\nYAML files are the recommended way to use ESPEI and should have a way to express most if not all of the options that the Python functions support. The schema for YAML files is located in the root of the ESPEI directory as input-schema.yaml and is validated in the espei_script.py module by the Cerberus package.",
    "crumbs": [
      "Developer",
      "Design Guidelines"
    ]
  },
  {
    "objectID": "developer/design.html#module-hierarchy",
    "href": "developer/design.html#module-hierarchy",
    "title": "Design Guidelines",
    "section": "Module Hierarchy",
    "text": "Module Hierarchy\n\nespei_script.py is the main entry point for the YAML input API.\noptimzers is a package that defines an OptimizerBase class for writing optimizers. EmceeOptimzer and ScipyOptimizer subclasses this.\nerror_functions is a package with modules for each type of likelihood function.\npriors.py defines priors to be used in MCMC, see Specifying Priors.\nparamselect.py is where parameter generation happens.\nmcmc.py creates the likelihood function and runs MCMC. Deprecated. In the future, users should use EmceeOptimizer.\nparameter_selection is a package with core pieces of parameter selection.\nutils.py are utilities with reuse potential across several parts of ESPEI.\nplot.py holds plotting functions.\ndatasets.py manages validating and loading datasets into a TinyDB in memory database.\ncore_utils.py are legacy utility functions that should be refactored out to be closer to individual modules and packages where they are used.\nshadow_functions.py are core internals that are designed to be fast, minimal versions of pycalphad's calculate and equilibrium functions.",
    "crumbs": [
      "Developer",
      "Design Guidelines"
    ]
  },
  {
    "objectID": "developer/design.html#parameter-selection",
    "href": "developer/design.html#parameter-selection",
    "title": "Design Guidelines",
    "section": "Parameter selection",
    "text": "Parameter selection\nParameter selection goes through the generate_parameters function in the espei.paramselect module. The goal of parameter selection is go through each phase (one at a time) and fit a Calphad model to the data.\nFor each phase, the endmembers are fit first, followed by binary and ternary interactions. For each individual endmember or interaction to fit, a series of candidate models are generated that have increasing complexity in both temperature and interaction order (an L0 excess parameter, L0 and L1, ...).\nEach model is then fit by espei.parameter_selection.selection.fit_model, which currently uses a simple pseudo-inverse linear model from scikit-learn. Then the tradeoff between the goodness of fit and the model complexity is scored by the AICc in espei.parameter_selection.selection.score_model. The optimal scoring model is accepted as the model with the fit model parameters set as degrees of freedom for the MCMC step.\nThe main principle is that ESPEI transforms the data and candidate models to vectors and matricies that fit a typical machine learning type problem of \\(Ax = b\\). Extending ESPEI to use different or custom models in the current scheme basically comes down to formulating candidate models in terms of this type of problem. The main ways to improve on the fitting or scoring methods used in parameter selection is to override the fit and score functions.\nCurrently the capabilities for providing custom models or contributions (e.g. magnetic data) in the form of generic pycalphad Model objects are limited. This is also true for custom types of data that one would use in fitting a custom model.",
    "crumbs": [
      "Developer",
      "Design Guidelines"
    ]
  },
  {
    "objectID": "developer/design.html#mcmc-optimization-and-uncertainty-quantification",
    "href": "developer/design.html#mcmc-optimization-and-uncertainty-quantification",
    "title": "Design Guidelines",
    "section": "MCMC optimization and uncertainty quantification",
    "text": "MCMC optimization and uncertainty quantification\nMost of the Markov chain Monte Carlo optimization and uncertainty quantification happen in the espei.optimizers.opt_mcmc.py module through the EmceeOptimizer class.\nEmceeOptimizer is a subclass of OptimizerBase, which defines an interface for performing opitmizations of parameters. It defines several methods:\nfit takes a list of symbol names and datasets to fit to. It calls an _fit method that returns an OptNode representing the parameters that result from the fit to the datasets. fit evaluates the parameters by calling the objective function on some parameters (an array of values) and a context in the predict method, which is overridden by OptimizerBase subclasses. There is also an interface for storing a history of successive fits to different parameter sets, using the commit method, which will store the history of the calls to fit in a graph of fitting steps. The idea is that users can generate a graph of fitting results and go back to specific points on the graph and test fitting different sets of parameters or different datasets, creating a unique history of committed parameter sets and optimization paths, similar to a history in version control software like git.\nThe main reason ESPEI's parameter selection and MCMC routines are split up is that custom Models or existing TDB files can be provided and fit. In other words, if you are using a model that doesn't need parameter selection or is for a property that is not Gibbs energy, MCMC can fit it with uncertainty quantification.\nThe general process is\n\nTake a database with degrees of freedom as database symbols named VV####, where #### is a number, e.g. 0001. The symbols correspond to FUNCTION in the TDB files.\nInitialize those degrees of freedom to a starting distribution for ensemble MCMC. The starting distribution is controlled by the EmceeOptimizer.initialize_new_chains function, which currently supports initializing the parameters to a Gaussian ball.\nUse the emcee package to run ensemble MCMC\n\nESPEI's MCMC is quite flexible for customization. To fit a custom model, it just needs to be read by pycalphad and have correctly named degrees of freedom (VV####).\nTo fit an existing or custom model to new types of data, just write a function that takes in datasets and the parameters that are required to calculate the values (e.g. pycalphad Database, components, phases, ...) and returns the error. Then override the EmceeOptimizer.predict function to include your custom error contribution. There are examples of these functions espei.error_functions that ESPEI uses by default.\nModifications to how parameters are initialized can be made by subclassing EmceeOptimizer.initialize_new_chains. Many other modifications can be made by subclassing EmceeOptimizer.",
    "crumbs": [
      "Developer",
      "Design Guidelines"
    ]
  },
  {
    "objectID": "tutorials/custom_model_parameter_selection.html",
    "href": "tutorials/custom_model_parameter_selection.html",
    "title": "Generating Custom Model Parameters",
    "section": "",
    "text": "ESPEI's default parameter selection capabilities support fitting Gibbs energy parameters (G parameters for endmembers, and L parameters for binary and ternary interations) from enthalpy, entropy, and heat capacity data. Using the parameter_generation.fitting_description parameter, ESPEI can support fitting different types of models and data too. For example, the built-in molar volume fitting description can be used to fit molar volume parameters (V0 and VA) to V0 and VM data (see fitting_description in YAML input schema for more details).\nThrough these fitting descrptions, ESPEI can be extended to fit custom PyCalphad models and model parameters without changing any of the source code for PyCalphad or ESPEI. Here we will implement a PyCalphad model for BCC elastic stiffness coefficients, which can be uniquly described by three independent components of the elastic stiffness matrix due to the cubic symmetry of the BCC phase. We'll then use DFT data from (Marker et al. 2018) to fit these endmember and interaction parameters and compare the result to values in the original publication. At the end of this tutorial, you will have used ESPEI to generated parameters for three types of elastic constant parameters using DFT data from the literature.\nTo run ESPEI and generate the output in this tutorial, download the files from GitHub.",
    "crumbs": [
      "Tutorials",
      "Generating Custom Model Parameters"
    ]
  },
  {
    "objectID": "tutorials/custom_model_parameter_selection.html#implementing-the-elastic-constant-model",
    "href": "tutorials/custom_model_parameter_selection.html#implementing-the-elastic-constant-model",
    "title": "Generating Custom Model Parameters",
    "section": "Implementing the elastic constant model",
    "text": "Implementing the elastic constant model\nBefore we can fit parameters to data using ESPEI, we first need a PyCalphad model that can use the parameters. PyCalphad Model objects supports most of the typical Calphad model parameters that are implemented by the commercial Calphad software tools, but here we want to use model parameters that PyCalphad doesn't have built-in support for. The custom_elastic_model.py file contains the implementation of an ElasticModel class that sublasses the PyCalphad Model class:\nimport tinydb\nfrom pycalphad import Model\nclass ElasticModel(Model):\ndef build_phase(self, dbe):\n    phase = dbe.phases[self.phase_name]\n    param_search = dbe.search\n    for prop in ['C11', 'C12', 'C44']:\n        prop_param_query = (\n            (tinydb.where('phase_name') == phase.name) & \\\n            (tinydb.where('parameter_type') == prop) & \\\n            (tinydb.where('constituent_array').test(self._array_validity))\n            )\n        prop_val = self.redlich_kister_sum(phase, param_search, prop_param_query).subs(dbe.symbols)\n        setattr(self, prop, prop_val)\nThis class overides the build_phase method of the Model class to add a property to class, one for C11, C12, and C44, where each property is modeled using a Redlich-Kister polynomial, following (Marker et al. 2018) Eq. (3). Note that redlich_kister_sum() gives us full generality for an arbitrary number sublattices and species in each sublattice.",
    "crumbs": [
      "Tutorials",
      "Generating Custom Model Parameters"
    ]
  },
  {
    "objectID": "tutorials/custom_model_parameter_selection.html#implementing-a-fitting-description",
    "href": "tutorials/custom_model_parameter_selection.html#implementing-a-fitting-description",
    "title": "Generating Custom Model Parameters",
    "section": "Implementing a fitting description",
    "text": "Implementing a fitting description\nThe ElasticModel from the previous section is enough to PyCalphad to use C11, C12, and C44 in a PyCalphad Database object to do calculations and calculate those properties with equilibrium. Now we need to tell ESPEI how to use read datasets for C11, C12, and C44 data, and use this model to fit the corresponding parameters. The two concepts we need are a FittingStep and a ModelFittingDescription.\nA FittingStep is an abstract class provided by ESPEI that defines an API for how to fit a particular type of data and what parameters describe it. There's a 1:1 relationship between a concrete FittingStep and a data type. Fitting steps also have a 1:1 relationship with a parameter type, but note that multiple fitting steps can be used to different data types to a single parameter type. For example, Gibbs energy parameters (G) are fit in three fitting steps: heat capacity data (CPM), then entropy data (SM), then enthalpy data (HM). Below are the implementations for each of the three elastic parameters and data. We are using the AbstractLinearPropertyStep helper class from ESPEI, which is a convience class to make it easier to define FittingStep classes that are 1:1 mappings between a data type and a parameter type and don't require and linearization or data/model transformation steps.\nfrom espei.parameter_selection.fitting_steps import AbstractLinearPropertyStep\n\nclass StepElasticC11(AbstractLinearPropertyStep):\n    parameter_name = \"C11\"\n    data_types_read = \"C11\"\n\nclass StepElasticC12(AbstractLinearPropertyStep):\n    parameter_name = \"C12\"\n    data_types_read = \"C12\"\n\nclass StepElasticC44(AbstractLinearPropertyStep):\n    parameter_name = \"C44\"\n    data_types_read = \"C44\"\nA ModelFittingDescription defines a series of fitting steps that are fit in order, and a PyCalphad model that can use a the parameters to model that data. Here we'll combine the elastic fitting steps to fit them in order of C11, then C12, then C44. The order of fitting doesn't matter in this case since the models are independent, but more complex cases might have dependencies requiring certain contributions to be fit before others (e.g. V0 molar volume parameters need to be fit before VA parameters when fitting VM data). By default, ESPEI will use the base PyCalphad Model class, but since that class doesn't know how to use our custom elastic constant parameters, we need to use the model keyword argument to tell ESPEI to use this model.\nfrom espei.parameter_selection.fitting_descriptions import ModelFittingDescription\nelastic_fitting_description = ModelFittingDescription([StepElasticC11, StepElasticC12, StepElasticC44], model=ElasticModel)\nThe elastic_fitting_description object that we created is the one we will pass to ESPEI via the parameter_generation.fitting_description input parameter.",
    "crumbs": [
      "Tutorials",
      "Generating Custom Model Parameters"
    ]
  },
  {
    "objectID": "tutorials/custom_model_parameter_selection.html#defining-datasets",
    "href": "tutorials/custom_model_parameter_selection.html#defining-datasets",
    "title": "Generating Custom Model Parameters",
    "section": "Defining datasets",
    "text": "Defining datasets\nParameter generation can only fit datasets of the same type of non-equilibrium thermochemical data, where the conditions and site fractions to generate the output value are provided explictly. Other than the \"output\" key that must match one of the data types read by one of our fitting steps, there are no additional steps for ESPEI to be able to read these datasets. See here for a complete description of these type of datasets. Here's an example for pure BCC Ti from the elastic-datasets directory:\n{\n    \"components\": [\"TI\", \"VA\"],\n    \"phases\": [\"BCC_A2\"],\n    \"output\": \"C11\",\n    \"values\": [[[93]]],\n    \"conditions\": {\"T\": 298.15, \"P\": 101325},\n    \"solver\": {\"mode\": \"manual\", \"sublattice_site_ratios\": [1, 3], \"sublattice_configurations\": [[\"TI\", \"VA\"]], \"sublattice_occupancies\": [[1.0, 1.0]]},\n    \"reference\": \"Marker (2018)\",\n    \"bibtex\": \"marker2018binary_elastic\",\n    \"comment\": \"Values pulled from Table 4 (DFT calculations).\",\n    \"tags\": []\n}",
    "crumbs": [
      "Tutorials",
      "Generating Custom Model Parameters"
    ]
  },
  {
    "objectID": "tutorials/custom_model_parameter_selection.html#running-espei",
    "href": "tutorials/custom_model_parameter_selection.html#running-espei",
    "title": "Generating Custom Model Parameters",
    "section": "Running ESPEI",
    "text": "Running ESPEI\nThe phase models description defined in phase_model.json are the same as for regular runs of ESPEI:\n{\n    \"components\": [\"MO\", \"TI\", \"ZR\", \"VA\"],\n    \"phases\": {\n        \"BCC_A2\": {\n            \"sublattice_model\": [[\"MO\", \"TI\", \"ZR\"], [\"VA\"]],\n            \"sublattice_site_ratios\": [1, 3]\n        }\n    }\n}\nThe YAML file that generated parameters for the system described above with the elastic model is in generate_parameters.yaml:\nsystem:\n  phase_models: phase_models.json\n  datasets: elastic-datasets\ngenerate_parameters:\n  excess_model: linear\n  ref_state: SGTE91\n  fitting_description: custom_elastic_model.elastic_fitting_description\noutput:\n  output_db: Ti-elastic.tdb\nThe key difference compared to typical runs of ESPEI that fit Gibbs energy parameters is the generate_parameters.fitting_description entry, which is a fully qualified import string to our fitting description object. This is the import path for the object and can be anything that can be imported by the Python interpreter that ESPEI is installed under. In this case, we are importing the object elastic_fitting_description from the custom_elastic_model.py file in the local directory. Note that this can be any local module or an object from any package installed, for example if you have pip installed a package called my_fitting_descriptions that provides a ModelFittingDescrption object for your custom model called my_fit_desc, you would set fitting_description: my_fitting_descriptions.my_fit_desc. If you start a Python interpreter from the same place that you'll run ESPEI, you can see if ESPEI can detect the fitting description by running the code\nfrom custom_elastic_model import elastic_fitting_description\nFinally, we can run this input file using ESPEI by running the command:\nespei --in generate_parameters.yaml\nThat's it! You should now have a TDB Ti-elastic.tdb that has C11, C22, C44 parameters for pure Ti, Mo, and Zr, and interaction paramters for Ti-Mo and Ti-Zr.",
    "crumbs": [
      "Tutorials",
      "Generating Custom Model Parameters"
    ]
  },
  {
    "objectID": "tutorials/custom_model_parameter_selection.html#comparing-the-results",
    "href": "tutorials/custom_model_parameter_selection.html#comparing-the-results",
    "title": "Generating Custom Model Parameters",
    "section": "Comparing the results",
    "text": "Comparing the results\nThe following table compares the parameters fit by (Marker et al. 2018) and those generated by ESPEI. In all cases except for Ti-Zr C11, ESPEI generated the same number of parameters as assessed by Marker et al., where the C11 model is simpler in the version fit by ESPEI. Where the parameters generated are the same, they are close aside from some small differences that could be due to the weighting by Marker et al. (the data used in this tutorial are not weighted). Plots comparing the generated parameters (plotted as lines) in to the data (points) are generated using the plot_model_comparisons.py script and appear to show good agreement.\n\n\n\n\n\n\n\n\n\n\n\nParam\nInteraction\nTi-Mo\n\nTi-Zr\n\n\n\n\n\n\n\nMarker (2018)\nESPEI\nMarker (2018)\nESPEI\n\n\nC11\nL0\n-22.16\n-27.7702\n246.97\n249.001\n\n\n\nL1\n0\n0\n-135.95\n0\n\n\nC12\nL0\n-36.40\n-27.9687\n-110.53\n-110.725\n\n\n\nL1\n0\n0\n-78.00\n-70.2764\n\n\nC44\nL0\n-142.9\n-137.882\n70.06\n57.7446\n\n\n\nL1\n0\n0\n0\n0\n\n\n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\n\n\nimage",
    "crumbs": [
      "Tutorials",
      "Generating Custom Model Parameters"
    ]
  }
]